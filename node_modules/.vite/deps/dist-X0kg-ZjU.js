import { __commonJS, __export, __toESM } from "./chunk-DbKvDyjX.js";

//#region node_modules/@tensorflow/tfjs-core/dist/backends/backend.js
/**
* @license
* Copyright 2020 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
const EPSILON_FLOAT32 = 1e-7;
const EPSILON_FLOAT16 = 1e-4;
/** Convenient class for storing tensor-related data. */
var DataStorage = class {
	constructor(backend$1, dataMover) {
		this.backend = backend$1;
		this.dataMover = dataMover;
		this.data = /* @__PURE__ */ new WeakMap();
		this.dataIdsCount = 0;
	}
	get(dataId) {
		if (!this.data.has(dataId)) this.dataMover.moveData(this.backend, dataId);
		return this.data.get(dataId);
	}
	set(dataId, value) {
		this.dataIdsCount++;
		this.data.set(dataId, value);
	}
	has(dataId) {
		return this.data.has(dataId);
	}
	delete(dataId) {
		this.dataIdsCount--;
		return this.data.delete(dataId);
	}
	numDataIds() {
		return this.dataIdsCount;
	}
};
/**
* The interface that defines the kernels that should be implemented when
* adding a new backend. New backends don't need to implement every one of the
* methods, this can be done gradually (throw an error for unimplemented
* methods).
*/
var KernelBackend = class {
	refCount(dataId) {
		return notYetImplemented("refCount");
	}
	incRef(dataId) {
		return notYetImplemented("incRef");
	}
	timerAvailable() {
		return true;
	}
	time(f) {
		return notYetImplemented("time");
	}
	read(dataId) {
		return notYetImplemented("read");
	}
	readSync(dataId) {
		return notYetImplemented("readSync");
	}
	numDataIds() {
		return notYetImplemented("numDataIds");
	}
	disposeData(dataId, force) {
		return notYetImplemented("disposeData");
	}
	write(values, shape, dtype) {
		return notYetImplemented("write");
	}
	move(dataId, values, shape, dtype, refCount) {
		return notYetImplemented("move");
	}
	memory() {
		return notYetImplemented("memory");
	}
	/** Returns the highest precision for floats in bits (e.g. 16 or 32) */
	floatPrecision() {
		return notYetImplemented("floatPrecision");
	}
	/** Returns the smallest representable number.  */
	epsilon() {
		return this.floatPrecision() === 32 ? EPSILON_FLOAT32 : EPSILON_FLOAT16;
	}
	dispose() {
		return notYetImplemented("dispose");
	}
};
function notYetImplemented(kernelName) {
	throw new Error(`'${kernelName}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/util_base.js
/**
* @license
* Copyright 2020 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
* Shuffles the array in-place using Fisher-Yates algorithm.
*
* ```js
* const a = [1, 2, 3, 4, 5];
* tf.util.shuffle(a);
* console.log(a);
* ```
*
* @param array The array to shuffle in-place.
*
* @doc {heading: 'Util', namespace: 'util'}
*/
function shuffle(array) {
	let counter = array.length;
	let index = 0;
	while (counter > 0) {
		index = Math.random() * counter | 0;
		counter--;
		swap(array, counter, index);
	}
}
/**
* Shuffles two arrays in-place the same way using Fisher-Yates algorithm.
*
* ```js
* const a = [1,2,3,4,5];
* const b = [11,22,33,44,55];
* tf.util.shuffleCombo(a, b);
* console.log(a, b);
* ```
*
* @param array The first array to shuffle in-place.
* @param array2 The second array to shuffle in-place with the same permutation
*     as the first array.
*
* @doc {heading: 'Util', namespace: 'util'}
*/
function shuffleCombo(array, array2) {
	if (array.length !== array2.length) throw new Error(`Array sizes must match to be shuffled together First array length was ${array.length}Second array length was ${array2.length}`);
	let counter = array.length;
	let index = 0;
	while (counter > 0) {
		index = Math.random() * counter | 0;
		counter--;
		swap(array, counter, index);
		swap(array2, counter, index);
	}
}
/** Clamps a value to a specified range. */
function clamp(min$1, x, max$1) {
	return Math.max(min$1, Math.min(x, max$1));
}
function nearestLargerEven(val) {
	return val % 2 === 0 ? val : val + 1;
}
function swap(object, left, right) {
	const temp = object[left];
	object[left] = object[right];
	object[right] = temp;
}
function sum$1(arr) {
	let sum$2 = 0;
	for (let i = 0; i < arr.length; i++) sum$2 += arr[i];
	return sum$2;
}
/**
* Returns a sample from a uniform [a, b) distribution.
*
* @param a The minimum support (inclusive).
* @param b The maximum support (exclusive).
* @return A pseudorandom number on the half-open interval [a,b).
*/
function randUniform(a, b) {
	const r = Math.random();
	return b * r + (1 - r) * a;
}
/** Returns the squared Euclidean distance between two vectors. */
function distSquared(a, b) {
	let result = 0;
	for (let i = 0; i < a.length; i++) {
		const diff = Number(a[i]) - Number(b[i]);
		result += diff * diff;
	}
	return result;
}
/**
* Asserts that the expression is true. Otherwise throws an error with the
* provided message.
*
* ```js
* const x = 2;
* tf.util.assert(x === 2, 'x is not 2');
* ```
*
* @param expr The expression to assert (as a boolean).
* @param msg A function that returns the message to report when throwing an
*     error. We use a function for performance reasons.
*
* @doc {heading: 'Util', namespace: 'util'}
*/
function assert(expr, msg) {
	if (!expr) throw new Error(typeof msg === "string" ? msg : msg());
}
function assertShapesMatch(shapeA, shapeB, errorMessagePrefix = "") {
	assert(arraysEqual(shapeA, shapeB), () => errorMessagePrefix + ` Shapes ${shapeA} and ${shapeB} must match`);
}
function assertNonNull(a) {
	assert(a != null, () => `The input to the tensor constructor must be a non-null value.`);
}
/**
*  Flattens an arbitrarily nested array.
*
* ```js
* const a = [[1, 2], [3, 4], [5, [6, [7]]]];
* const flat = tf.util.flatten(a);
* console.log(flat);
* ```
*
*  @param arr The nested array to flatten.
*  @param result The destination array which holds the elements.
*  @param skipTypedArray If true, avoids flattening the typed arrays. Defaults
*      to false.
*
* @doc {heading: 'Util', namespace: 'util'}
*/
function flatten(arr, result = [], skipTypedArray = false) {
	if (result == null) result = [];
	if (Array.isArray(arr) || isTypedArray(arr) && !skipTypedArray) for (let i = 0; i < arr.length; ++i) flatten(arr[i], result, skipTypedArray);
	else result.push(arr);
	return result;
}
/**
* Returns the size (number of elements) of the tensor given its shape.
*
* ```js
* const shape = [3, 4, 2];
* const size = tf.util.sizeFromShape(shape);
* console.log(size);
* ```
*
* @doc {heading: 'Util', namespace: 'util'}
*/
function sizeFromShape(shape) {
	if (shape.length === 0) return 1;
	let size = shape[0];
	for (let i = 1; i < shape.length; i++) size *= shape[i];
	return size;
}
function isScalarShape(shape) {
	return shape.length === 0;
}
function arraysEqual(n1, n2) {
	if (n1 === n2) return true;
	if (n1 == null || n2 == null) return false;
	if (n1.length !== n2.length) return false;
	for (let i = 0; i < n1.length; i++) if (n1[i] !== n2[i]) return false;
	return true;
}
function isInt(a) {
	return a % 1 === 0;
}
function tanh$1(x) {
	if (Math.tanh != null) return Math.tanh(x);
	if (x === Infinity) return 1;
	else if (x === -Infinity) return -1;
	else {
		const e2x = Math.exp(2 * x);
		return (e2x - 1) / (e2x + 1);
	}
}
function sizeToSquarishShape(size) {
	const width = Math.ceil(Math.sqrt(size));
	return [width, Math.ceil(size / width)];
}
/**
* Creates a new array with randomized indicies to a given quantity.
*
* ```js
* const randomTen = tf.util.createShuffledIndices(10);
* console.log(randomTen);
* ```
*
* @param number Quantity of how many shuffled indicies to create.
*
* @doc {heading: 'Util', namespace: 'util'}
*/
function createShuffledIndices(n) {
	const shuffledIndices = new Uint32Array(n);
	for (let i = 0; i < n; ++i) shuffledIndices[i] = i;
	shuffle(shuffledIndices);
	return shuffledIndices;
}
function rightPad(a, size) {
	if (size <= a.length) return a;
	return a + " ".repeat(size - a.length);
}
function repeatedTry(checkFn, delayFn = (counter) => 0, maxCounter) {
	return new Promise((resolve, reject) => {
		let tryCount = 0;
		const tryFn = () => {
			if (checkFn()) {
				resolve();
				return;
			}
			tryCount++;
			const nextBackoff = delayFn(tryCount);
			if (maxCounter != null && tryCount >= maxCounter) {
				reject();
				return;
			}
			setTimeout(tryFn, nextBackoff);
		};
		tryFn();
	});
}
/**
* Given the full size of the array and a shape that may contain -1 as the
* implicit dimension, returns the inferred shape where -1 is replaced.
* E.g. For shape=[2, -1, 3] and size=24, it will return [2, 4, 3].
*
* @param shape The shape, which may contain -1 in some dimension.
* @param size The full size (number of elements) of the array.
* @return The inferred shape where -1 is replaced with the inferred size.
*/
function inferFromImplicitShape(shape, size) {
	let shapeProd = 1;
	let implicitIdx = -1;
	for (let i = 0; i < shape.length; ++i) if (shape[i] >= 0) shapeProd *= shape[i];
	else if (shape[i] === -1) {
		if (implicitIdx !== -1) throw Error(`Shapes can only have 1 implicit size. Found -1 at dim ${implicitIdx} and dim ${i}`);
		implicitIdx = i;
	} else if (shape[i] < 0) throw Error(`Shapes can not be < 0. Found ${shape[i]} at dim ${i}`);
	if (implicitIdx === -1) {
		if (size > 0 && size !== shapeProd) throw Error(`Size(${size}) must match the product of shape ${shape}`);
		return shape;
	}
	if (shapeProd === 0) throw Error(`Cannot infer the missing size in [${shape}] when there are 0 elements`);
	if (size % shapeProd !== 0) throw Error(`The implicit shape can't be a fractional number. Got ${size} / ${shapeProd}`);
	const newShape = shape.slice();
	newShape[implicitIdx] = size / shapeProd;
	return newShape;
}
function parseAxisParam(axis, shape) {
	const rank = shape.length;
	axis = axis == null ? shape.map((s, i) => i) : [].concat(axis);
	assert(axis.every((ax) => ax >= -rank && ax < rank), () => `All values in axis param must be in range [-${rank}, ${rank}) but got axis ${axis}`);
	assert(axis.every((ax) => isInt(ax)), () => `All values in axis param must be integers but got axis ${axis}`);
	return axis.map((a) => a < 0 ? rank + a : a);
}
/** Reduces the shape by removing all dimensions of shape 1. */
function squeezeShape(shape, axis) {
	const newShape = [];
	const keptDims = [];
	const isEmptyArray = axis != null && Array.isArray(axis) && axis.length === 0;
	const axes = axis == null || isEmptyArray ? null : parseAxisParam(axis, shape).sort();
	let j = 0;
	for (let i = 0; i < shape.length; ++i) {
		if (axes != null) {
			if (axes[j] === i && shape[i] !== 1) throw new Error(`Can't squeeze axis ${i} since its dim '${shape[i]}' is not 1`);
			if ((axes[j] == null || axes[j] > i) && shape[i] === 1) {
				newShape.push(shape[i]);
				keptDims.push(i);
			}
			if (axes[j] <= i) j++;
		}
		if (shape[i] !== 1) {
			newShape.push(shape[i]);
			keptDims.push(i);
		}
	}
	return {
		newShape,
		keptDims
	};
}
function getTypedArrayFromDType(dtype, size) {
	let values = null;
	if (dtype == null || dtype === "float32") values = new Float32Array(size);
	else if (dtype === "int32") values = new Int32Array(size);
	else if (dtype === "bool") values = new Uint8Array(size);
	else throw new Error(`Unknown data type ${dtype}`);
	return values;
}
function getArrayFromDType(dtype, size) {
	let values = null;
	if (dtype == null || dtype === "float32") values = new Float32Array(size);
	else if (dtype === "int32") values = new Int32Array(size);
	else if (dtype === "bool") values = new Uint8Array(size);
	else if (dtype === "string") values = new Array(size);
	else throw new Error(`Unknown data type ${dtype}`);
	return values;
}
function checkConversionForErrors(vals, dtype) {
	for (let i = 0; i < vals.length; i++) {
		const num = vals[i];
		if (isNaN(num) || !isFinite(num)) throw Error(`A tensor of type ${dtype} being uploaded contains ${num}.`);
	}
}
/** Returns true if the dtype is valid. */
function isValidDtype(dtype) {
	return dtype === "bool" || dtype === "complex64" || dtype === "float32" || dtype === "int32" || dtype === "string";
}
/**
* Returns true if the new type can't encode the old type without loss of
* precision.
*/
function hasEncodingLoss(oldType, newType) {
	if (newType === "complex64") return false;
	if (newType === "float32" && oldType !== "complex64") return false;
	if (newType === "int32" && oldType !== "float32" && oldType !== "complex64") return false;
	if (newType === "bool" && oldType === "bool") return false;
	return true;
}
function isTypedArray(a) {
	return a instanceof Float32Array || a instanceof Int32Array || a instanceof Uint8Array || a instanceof Uint8ClampedArray;
}
function bytesPerElement(dtype) {
	if (dtype === "float32" || dtype === "int32") return 4;
	else if (dtype === "complex64") return 8;
	else if (dtype === "bool") return 1;
	else throw new Error(`Unknown dtype ${dtype}`);
}
/**
* Returns the approximate number of bytes allocated in the string array - 2
* bytes per character. Computing the exact bytes for a native string in JS is
* not possible since it depends on the encoding of the html page that serves
* the website.
*/
function bytesFromStringArray(arr) {
	if (arr == null) return 0;
	let bytes = 0;
	arr.forEach((x) => bytes += x.length);
	return bytes;
}
/** Returns true if the value is a string. */
function isString(value) {
	return typeof value === "string" || value instanceof String;
}
function isBoolean(value) {
	return typeof value === "boolean";
}
function isNumber(value) {
	return typeof value === "number";
}
function inferDtype(values) {
	if (Array.isArray(values)) return inferDtype(values[0]);
	if (values instanceof Float32Array) return "float32";
	else if (values instanceof Int32Array || values instanceof Uint8Array || values instanceof Uint8ClampedArray) return "int32";
	else if (isNumber(values)) return "float32";
	else if (isString(values)) return "string";
	else if (isBoolean(values)) return "bool";
	return "float32";
}
function isFunction(f) {
	return !!(f && f.constructor && f.call && f.apply);
}
function nearestDivisor(size, start) {
	for (let i = start; i < size; ++i) if (size % i === 0) return i;
	return size;
}
function computeStrides(shape) {
	const rank = shape.length;
	if (rank < 2) return [];
	const strides = new Array(rank - 1);
	strides[rank - 2] = shape[rank - 1];
	for (let i = rank - 3; i >= 0; --i) strides[i] = strides[i + 1] * shape[i + 1];
	return strides;
}
function createNestedArray(offset, shape, a, isComplex = false) {
	const ret = new Array();
	if (shape.length === 1) {
		const d = shape[0] * (isComplex ? 2 : 1);
		for (let i = 0; i < d; i++) ret[i] = a[offset + i];
	} else {
		const d = shape[0];
		const rest = shape.slice(1);
		const len = rest.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);
		for (let i = 0; i < d; i++) ret[i] = createNestedArray(offset + i * len, rest, a, isComplex);
	}
	return ret;
}
function toNestedArray(shape, a, isComplex = false) {
	if (shape.length === 0) return a[0];
	const size = shape.reduce((acc, c) => acc * c) * (isComplex ? 2 : 1);
	if (size === 0) return [];
	if (size !== a.length) throw new Error(`[${shape}] does not match the input size ${a.length}${isComplex ? " for a complex tensor" : ""}.`);
	return createNestedArray(0, shape, a, isComplex);
}
function makeOnesTypedArray(size, dtype) {
	const array = makeZerosTypedArray(size, dtype);
	for (let i = 0; i < array.length; i++) array[i] = 1;
	return array;
}
function makeZerosTypedArray(size, dtype) {
	if (dtype == null || dtype === "float32" || dtype === "complex64") return new Float32Array(size);
	else if (dtype === "int32") return new Int32Array(size);
	else if (dtype === "bool") return new Uint8Array(size);
	else throw new Error(`Unknown data type ${dtype}`);
}
/**
* Make nested `TypedArray` filled with zeros.
* @param shape The shape information for the nested array.
* @param dtype dtype of the array element.
*/
function makeZerosNestedTypedArray(shape, dtype) {
	const size = shape.reduce((prev, curr) => prev * curr, 1);
	if (dtype == null || dtype === "float32") return toNestedArray(shape, new Float32Array(size));
	else if (dtype === "int32") return toNestedArray(shape, new Int32Array(size));
	else if (dtype === "bool") return toNestedArray(shape, new Uint8Array(size));
	else throw new Error(`Unknown data type ${dtype}`);
}
function assertNonNegativeIntegerDimensions(shape) {
	shape.forEach((dimSize) => {
		assert(Number.isInteger(dimSize) && dimSize >= 0, () => `Tensor must have a shape comprised of positive integers but got shape [${shape}].`);
	});
}
/**
* Computes flat index for a given location (multidimentionsal index) in a
* Tensor/multidimensional array.
*
* @param locs Location in the tensor.
* @param rank Rank of the tensor.
* @param strides Tensor strides.
*/
function locToIndex(locs, rank, strides) {
	if (rank === 0) return 0;
	else if (rank === 1) return locs[0];
	let index = locs[locs.length - 1];
	for (let i = 0; i < locs.length - 1; ++i) index += strides[i] * locs[i];
	return index;
}
/**
* Computes the location (multidimensional index) in a tensor/multidimentional
* array for a given flat index.
*
* @param index Index in flat array.
* @param rank Rank of tensor.
* @param strides Strides of tensor.
*/
function indexToLoc(index, rank, strides) {
	if (rank === 0) return [];
	else if (rank === 1) return [index];
	const locs = new Array(rank);
	for (let i = 0; i < locs.length - 1; ++i) {
		locs[i] = Math.floor(index / strides[i]);
		index -= locs[i] * strides[i];
	}
	locs[locs.length - 1] = index;
	return locs;
}
/**
* This method asserts whether an object is a Promise instance.
* @param object
*/
function isPromise(object) {
	return object && object.then && typeof object.then === "function";
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/log.js
function warn(...msg) {
	if (!(env().getBool("IS_TEST") || env().getBool("PROD"))) console.warn(...msg);
}
function log$1(...msg) {
	if (!(env().getBool("IS_TEST") || env().getBool("PROD"))) console.log(...msg);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/environment.js
var TENSORFLOWJS_FLAGS_PREFIX = "tfjsflags";
/**
* The environment contains evaluated flags as well as the registered platform.
* This is always used as a global singleton and can be retrieved with
* `tf.env()`.
*
* @doc {heading: 'Environment'}
*/
var Environment = class {
	constructor(global$1) {
		this.global = global$1;
		this.flags = {};
		this.flagRegistry = {};
		this.urlFlags = {};
		this.getQueryParams = getQueryParams;
		this.populateURLFlags();
	}
	setPlatform(platformName, platform) {
		if (this.platform != null) warn(`Platform ${this.platformName} has already been set. Overwriting the platform with ${platform}.`);
		this.platformName = platformName;
		this.platform = platform;
	}
	registerFlag(flagName, evaluationFn, setHook) {
		this.flagRegistry[flagName] = {
			evaluationFn,
			setHook
		};
		if (this.urlFlags[flagName] != null) {
			const flagValue = this.urlFlags[flagName];
			warn(`Setting feature override from URL ${flagName}: ${flagValue}.`);
			this.set(flagName, flagValue);
		}
	}
	async getAsync(flagName) {
		if (flagName in this.flags) return this.flags[flagName];
		this.flags[flagName] = await this.evaluateFlag(flagName);
		return this.flags[flagName];
	}
	get(flagName) {
		if (flagName in this.flags) return this.flags[flagName];
		const flagValue = this.evaluateFlag(flagName);
		if (isPromise(flagValue)) throw new Error(`Flag ${flagName} cannot be synchronously evaluated. Please use getAsync() instead.`);
		this.flags[flagName] = flagValue;
		return this.flags[flagName];
	}
	getNumber(flagName) {
		return this.get(flagName);
	}
	getBool(flagName) {
		return this.get(flagName);
	}
	getFlags() {
		return this.flags;
	}
	get features() {
		return this.flags;
	}
	set(flagName, value) {
		if (this.flagRegistry[flagName] == null) throw new Error(`Cannot set flag ${flagName} as it has not been registered.`);
		this.flags[flagName] = value;
		if (this.flagRegistry[flagName].setHook != null) this.flagRegistry[flagName].setHook(value);
	}
	evaluateFlag(flagName) {
		if (this.flagRegistry[flagName] == null) throw new Error(`Cannot evaluate flag '${flagName}': no evaluation function found.`);
		return this.flagRegistry[flagName].evaluationFn();
	}
	setFlags(flags) {
		this.flags = Object.assign({}, flags);
	}
	reset() {
		this.flags = {};
		this.urlFlags = {};
		this.populateURLFlags();
	}
	populateURLFlags() {
		if (typeof this.global === "undefined" || typeof this.global.location === "undefined" || typeof this.global.location.search === "undefined") return;
		const urlParams = this.getQueryParams(this.global.location.search);
		if (TENSORFLOWJS_FLAGS_PREFIX in urlParams) urlParams[TENSORFLOWJS_FLAGS_PREFIX].split(",").forEach((keyValue) => {
			const [key, value] = keyValue.split(":");
			this.urlFlags[key] = parseValue(key, value);
		});
	}
};
function getQueryParams(queryString) {
	const params = {};
	queryString.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g, (s, ...t) => {
		decodeParam(params, t[0], t[1]);
		return t.join("=");
	});
	return params;
}
function decodeParam(params, name, value) {
	params[decodeURIComponent(name)] = decodeURIComponent(value || "");
}
function parseValue(flagName, value) {
	value = value.toLowerCase();
	if (value === "true" || value === "false") return value === "true";
	else if (`${+value}` === value) return +value;
	throw new Error(`Could not parse value flag value ${value} for flag ${flagName}.`);
}
/**
* Returns the current environment (a global singleton).
*
* The environment object contains the evaluated feature values as well as the
* active platform.
*
* @doc {heading: 'Environment'}
*/
function env() {
	return ENV$1;
}
let ENV$1 = null;
function setEnvironmentGlobal(environment) {
	ENV$1 = environment;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/global_util.js
/**
* @license
* Copyright 2020 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
var globalNameSpace;
function getGlobalNamespace() {
	if (globalNameSpace == null) {
		let ns;
		if (typeof window !== "undefined") ns = window;
		else if (typeof global !== "undefined") ns = global;
		else if (typeof process !== "undefined") ns = process;
		else if (typeof self !== "undefined") ns = self;
		else throw new Error("Could not find a global object");
		globalNameSpace = ns;
	}
	return globalNameSpace;
}
function getGlobalMap() {
	const ns = getGlobalNamespace();
	if (ns._tfGlobals == null) ns._tfGlobals = /* @__PURE__ */ new Map();
	return ns._tfGlobals;
}
/**
* Returns a globally accessible 'singleton' object.
*
* @param key the name of the object
* @param init a function to initialize to initialize this object
*             the first time it is fetched.
*/
function getGlobal(key, init) {
	const globalMap = getGlobalMap();
	if (globalMap.has(key)) return globalMap.get(key);
	else {
		const singleton = init();
		globalMap.set(key, singleton);
		return globalMap.get(key);
	}
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/kernel_names.js
const Abs = "Abs";
const Acos = "Acos";
const Acosh = "Acosh";
const Add = "Add";
const AddN = "AddN";
const All = "All";
const Any = "Any";
const ArgMax = "ArgMax";
const ArgMin = "ArgMin";
const Asin = "Asin";
const Asinh = "Asinh";
const Atan = "Atan";
const Atanh = "Atanh";
const Atan2 = "Atan2";
const AvgPool = "AvgPool";
const AvgPoolGrad = "AvgPoolGrad";
const AvgPool3D = "AvgPool3D";
const AvgPool3DGrad = "AvgPool3DGrad";
const BatchMatMul = "BatchMatMul";
const BatchToSpaceND = "BatchToSpaceND";
const Bincount = "Bincount";
const BroadcastTo = "BroadcastTo";
const BroadcastArgs = "BroadcastArgs";
const Cast = "Cast";
const Ceil = "Ceil";
const ClipByValue = "ClipByValue";
const Complex = "Complex";
const ComplexAbs = "ComplexAbs";
const Concat = "Concat";
const Conv2D = "Conv2D";
const Conv2DBackpropFilter = "Conv2DBackpropFilter";
const Conv2DBackpropInput = "Conv2DBackpropInput";
const Conv3D = "Conv3D";
const Conv3DBackpropFilterV2 = "Conv3DBackpropFilterV2";
const Conv3DBackpropInputV2 = "Conv3DBackpropInputV2";
const Cos = "Cos";
const Cosh = "Cosh";
const Cumsum = "Cumsum";
const CropAndResize = "CropAndResize";
const DenseBincount = "DenseBincount";
const DepthToSpace = "DepthToSpace";
const DepthwiseConv2dNative = "DepthwiseConv2dNative";
const DepthwiseConv2dNativeBackpropFilter = "DepthwiseConv2dNativeBackpropFilter";
const DepthwiseConv2dNativeBackpropInput = "DepthwiseConv2dNativeBackpropInput";
const Diag = "Diag";
const Dilation2D = "Dilation2D";
const Dilation2DBackpropInput = "Dilation2DBackpropInput";
const Dilation2DBackpropFilter = "Dilation2DBackpropFilter";
const RealDiv = "RealDiv";
const Einsum = "Einsum";
const Elu = "Elu";
const EluGrad = "EluGrad";
const Erf = "Erf";
const Equal = "Equal";
const Exp = "Exp";
const ExpandDims = "ExpandDims";
const Expm1 = "Expm1";
const FFT = "FFT";
const Fill = "Fill";
const FlipLeftRight = "FlipLeftRight";
const Floor = "Floor";
const FloorDiv = "FloorDiv";
const FusedBatchNorm = "FusedBatchNorm";
const GatherV2 = "GatherV2";
const GatherNd = "GatherNd";
const Greater = "Greater";
const GreaterEqual = "GreaterEqual";
const Identity = "Identity";
const IFFT = "IFFT";
const Imag = "Imag";
const IsFinite = "IsFinite";
const IsInf = "IsInf";
const IsNan = "IsNan";
const LeakyRelu = "LeakyRelu";
const Less = "Less";
const LessEqual = "LessEqual";
const LinSpace = "LinSpace";
const Log = "Log";
const Log1p = "Log1p";
const LogicalAnd = "LogicalAnd";
const LogicalNot = "LogicalNot";
const LogicalOr = "LogicalOr";
const LogSoftmax = "LogSoftmax";
const LRN = "LRN";
const LRNGrad = "LRNGrad";
const Max = "Max";
const Maximum = "Maximum";
const MaxPool = "MaxPool";
const MaxPoolGrad = "MaxPoolGrad";
const MaxPool3D = "MaxPool3D";
const MaxPool3DGrad = "MaxPool3DGrad";
const MaxPoolWithArgmax = "MaxPoolWithArgmax";
const Mean = "Mean";
const Min = "Min";
const Minimum = "Minimum";
const MirrorPad = "MirrorPad";
const Mod = "Mod";
const Multinomial = "Multinomial";
const Multiply = "Multiply";
const Neg = "Neg";
const NotEqual = "NotEqual";
const NonMaxSuppressionV3 = "NonMaxSuppressionV3";
const NonMaxSuppressionV4 = "NonMaxSuppressionV4";
const NonMaxSuppressionV5 = "NonMaxSuppressionV5";
const OnesLike = "OnesLike";
const OneHot = "OneHot";
const Pack = "Pack";
const PadV2 = "PadV2";
const Pool = "Pool";
const Pow = "Pow";
const Prelu = "Prelu";
const Prod = "Prod";
const Range = "Range";
const Real = "Real";
const Reciprocal = "Reciprocal";
const Relu = "Relu";
const Reshape = "Reshape";
const ResizeNearestNeighbor = "ResizeNearestNeighbor";
const ResizeNearestNeighborGrad = "ResizeNearestNeighborGrad";
const ResizeBilinear = "ResizeBilinear";
const ResizeBilinearGrad = "ResizeBilinearGrad";
const Relu6 = "Relu6";
const Reverse = "Reverse";
const Round = "Round";
const Rsqrt = "Rsqrt";
const ScatterNd = "ScatterNd";
const Select = "Select";
const Selu = "Selu";
const Slice = "Slice";
const Sin = "Sin";
const Sinh = "Sinh";
const Sign = "Sign";
const Sigmoid = "Sigmoid";
const Softplus = "Softplus";
const Sqrt = "Sqrt";
const Sum = "Sum";
const SpaceToBatchND = "SpaceToBatchND";
const SplitV = "SplitV";
const Softmax = "Softmax";
const SparseFillEmptyRows = "SparseFillEmptyRows";
const SparseReshape = "SparseReshape";
const SparseSegmentMean = "SparseSegmentMean";
const SparseSegmentSum = "SparseSegmentSum";
const SparseToDense = "SparseToDense";
const SquaredDifference = "SquaredDifference";
const Square = "Square";
const StridedSlice = "StridedSlice";
const StringNGrams = "StringNGrams";
const StringSplit = "StringSplit";
const StringToHashBucketFast = "StringToHashBucketFast";
const Sub = "Sub";
const Tan = "Tan";
const Tanh = "Tanh";
const Tile = "Tile";
const TopK = "TopK";
const Transform = "Transform";
const Transpose = "Transpose";
const Unique = "Unique";
const Unpack = "Unpack";
const UnsortedSegmentSum = "UnsortedSegmentSum";
const ZerosLike = "ZerosLike";
/**
* TensorFlow.js-only kernels
*/
const Step = "Step";
const FromPixels = "FromPixels";
const RotateWithOffset = "RotateWithOffset";
const _FusedMatMul = "_FusedMatMul";
const FusedConv2D = "FusedConv2D";
const FusedDepthwiseConv2D = "FusedDepthwiseConv2D";

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/kernel_registry.js
var kernelRegistry = getGlobal("kernelRegistry", () => /* @__PURE__ */ new Map());
var gradRegistry = getGlobal("gradRegistry", () => /* @__PURE__ */ new Map());
/**
* Returns the kernel function (code) associated with the provided names.
*
* @param kernelName The official name of the kernel.
* @param backendName The official name of the backend.
*/
function getKernel(kernelName, backendName) {
	const key = makeKey(kernelName, backendName);
	return kernelRegistry.get(key);
}
/**
* Returns the registered gradient info associated with the provided kernel.
* @param kernelName The official TF kernel name.
*/
function getGradient(kernelName) {
	return gradRegistry.get(kernelName);
}
function getKernelsForBackend(backendName) {
	const it = kernelRegistry.entries();
	const result = [];
	while (true) {
		const { done, value } = it.next();
		if (done) break;
		const [key, config] = value;
		const [backend$1] = key.split("_");
		if (backend$1 === backendName) result.push(config);
	}
	return result;
}
/**
* Registers the function (forward pass) for the kernel in a global registry.
*
* @param config A config object with the following properties:
* - `kernelName` The official name of the kernel.
* - `backendName` The official name of the backend.
* - `kernelFunc` The function to run during the forward pass of the kernel.
* - `setupFunc` Optional. Gets called once, after the backend initializes.
* - `disposeFunc` Optional. Gets called once, right before the backend is
* disposed.
*/
function registerKernel(config) {
	const { kernelName, backendName } = config;
	const key = makeKey(kernelName, backendName);
	if (kernelRegistry.has(key)) warn(`The kernel '${kernelName}' for backend '${backendName}' is already registered`);
	kernelRegistry.set(key, config);
}
/**
* Registers a gradient function for a given kernel in the global registry,
* to be used during the back-propagation of that kernel.
*
* @param config An object with the following properties:
* - `kernelName` The name of the kernel that the gradient function is for.
* - `gradFunc` The function to run during back-propagation.
*/
function registerGradient(config) {
	const { kernelName } = config;
	if (gradRegistry.has(kernelName)) {
		if (env().getBool("DEBUG")) warn(`Overriding the gradient for '${kernelName}'`);
	}
	gradRegistry.set(kernelName, config);
}
/**
* Removes the kernel function from the registry.
*
* @param kernelName The official name of the kernel.
* @param backendName The official name of the backend.
*
*/
function unregisterKernel(kernelName, backendName) {
	const key = makeKey(kernelName, backendName);
	if (!kernelRegistry.has(key)) throw new Error(`The kernel '${kernelName}' for backend '${backendName}' is not registered`);
	kernelRegistry.delete(key);
}
/** Removes the registered gradient from the global registry. */
function unregisterGradient(kernelName) {
	if (!gradRegistry.has(kernelName)) throw new Error(`The gradient '${kernelName}' for backend is not registered`);
	gradRegistry.delete(kernelName);
}
/**
* Finds kernels that have already been registered to a backend and re-registers
* them for a new backend. Useful for registering custom backends.
* @param registeredBackendName Already registered backend.
* @param newBackendName New backend.
*/
function copyRegisteredKernels(registeredBackendName, newBackendName) {
	getKernelsForBackend(registeredBackendName).forEach((kernelConfig) => {
		const newKernelConfig = Object.assign({}, kernelConfig, { backendName: newBackendName });
		registerKernel(newKernelConfig);
	});
}
function makeKey(kernelName, backendName) {
	return `${backendName}_${kernelName}`;
}

//#endregion
//#region node_modules/long/src/long.js
var require_long = /* @__PURE__ */ __commonJS({ "node_modules/long/src/long.js": ((exports, module) => {
	module.exports = Long$1;
	/**
	* wasm optimizations, to do native i64 multiplication and divide
	*/
	var wasm = null;
	try {
		wasm = new WebAssembly.Instance(new WebAssembly.Module(new Uint8Array([
			0,
			97,
			115,
			109,
			1,
			0,
			0,
			0,
			1,
			13,
			2,
			96,
			0,
			1,
			127,
			96,
			4,
			127,
			127,
			127,
			127,
			1,
			127,
			3,
			7,
			6,
			0,
			1,
			1,
			1,
			1,
			1,
			6,
			6,
			1,
			127,
			1,
			65,
			0,
			11,
			7,
			50,
			6,
			3,
			109,
			117,
			108,
			0,
			1,
			5,
			100,
			105,
			118,
			95,
			115,
			0,
			2,
			5,
			100,
			105,
			118,
			95,
			117,
			0,
			3,
			5,
			114,
			101,
			109,
			95,
			115,
			0,
			4,
			5,
			114,
			101,
			109,
			95,
			117,
			0,
			5,
			8,
			103,
			101,
			116,
			95,
			104,
			105,
			103,
			104,
			0,
			0,
			10,
			191,
			1,
			6,
			4,
			0,
			35,
			0,
			11,
			36,
			1,
			1,
			126,
			32,
			0,
			173,
			32,
			1,
			173,
			66,
			32,
			134,
			132,
			32,
			2,
			173,
			32,
			3,
			173,
			66,
			32,
			134,
			132,
			126,
			34,
			4,
			66,
			32,
			135,
			167,
			36,
			0,
			32,
			4,
			167,
			11,
			36,
			1,
			1,
			126,
			32,
			0,
			173,
			32,
			1,
			173,
			66,
			32,
			134,
			132,
			32,
			2,
			173,
			32,
			3,
			173,
			66,
			32,
			134,
			132,
			127,
			34,
			4,
			66,
			32,
			135,
			167,
			36,
			0,
			32,
			4,
			167,
			11,
			36,
			1,
			1,
			126,
			32,
			0,
			173,
			32,
			1,
			173,
			66,
			32,
			134,
			132,
			32,
			2,
			173,
			32,
			3,
			173,
			66,
			32,
			134,
			132,
			128,
			34,
			4,
			66,
			32,
			135,
			167,
			36,
			0,
			32,
			4,
			167,
			11,
			36,
			1,
			1,
			126,
			32,
			0,
			173,
			32,
			1,
			173,
			66,
			32,
			134,
			132,
			32,
			2,
			173,
			32,
			3,
			173,
			66,
			32,
			134,
			132,
			129,
			34,
			4,
			66,
			32,
			135,
			167,
			36,
			0,
			32,
			4,
			167,
			11,
			36,
			1,
			1,
			126,
			32,
			0,
			173,
			32,
			1,
			173,
			66,
			32,
			134,
			132,
			32,
			2,
			173,
			32,
			3,
			173,
			66,
			32,
			134,
			132,
			130,
			34,
			4,
			66,
			32,
			135,
			167,
			36,
			0,
			32,
			4,
			167,
			11
		])), {}).exports;
	} catch (e) {}
	/**
	* Constructs a 64 bit two's-complement integer, given its low and high 32 bit values as *signed* integers.
	*  See the from* functions below for more convenient ways of constructing Longs.
	* @exports Long
	* @class A Long class for representing a 64 bit two's-complement integer value.
	* @param {number} low The low (signed) 32 bits of the long
	* @param {number} high The high (signed) 32 bits of the long
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @constructor
	*/
	function Long$1(low, high, unsigned) {
		/**
		* The low 32 bits as a signed value.
		* @type {number}
		*/
		this.low = low | 0;
		/**
		* The high 32 bits as a signed value.
		* @type {number}
		*/
		this.high = high | 0;
		/**
		* Whether unsigned or not.
		* @type {boolean}
		*/
		this.unsigned = !!unsigned;
	}
	/**
	* An indicator used to reliably determine if an object is a Long or not.
	* @type {boolean}
	* @const
	* @private
	*/
	Long$1.prototype.__isLong__;
	Object.defineProperty(Long$1.prototype, "__isLong__", { value: true });
	/**
	* @function
	* @param {*} obj Object
	* @returns {boolean}
	* @inner
	*/
	function isLong(obj) {
		return (obj && obj["__isLong__"]) === true;
	}
	/**
	* Tests if the specified object is a Long.
	* @function
	* @param {*} obj Object
	* @returns {boolean}
	*/
	Long$1.isLong = isLong;
	/**
	* A cache of the Long representations of small integer values.
	* @type {!Object}
	* @inner
	*/
	var INT_CACHE = {};
	/**
	* A cache of the Long representations of small unsigned integer values.
	* @type {!Object}
	* @inner
	*/
	var UINT_CACHE = {};
	/**
	* @param {number} value
	* @param {boolean=} unsigned
	* @returns {!Long}
	* @inner
	*/
	function fromInt(value, unsigned) {
		var obj, cachedObj, cache;
		if (unsigned) {
			value >>>= 0;
			if (cache = 0 <= value && value < 256) {
				cachedObj = UINT_CACHE[value];
				if (cachedObj) return cachedObj;
			}
			obj = fromBits(value, (value | 0) < 0 ? -1 : 0, true);
			if (cache) UINT_CACHE[value] = obj;
			return obj;
		} else {
			value |= 0;
			if (cache = -128 <= value && value < 128) {
				cachedObj = INT_CACHE[value];
				if (cachedObj) return cachedObj;
			}
			obj = fromBits(value, value < 0 ? -1 : 0, false);
			if (cache) INT_CACHE[value] = obj;
			return obj;
		}
	}
	/**
	* Returns a Long representing the given 32 bit integer value.
	* @function
	* @param {number} value The 32 bit integer in question
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @returns {!Long} The corresponding Long value
	*/
	Long$1.fromInt = fromInt;
	/**
	* @param {number} value
	* @param {boolean=} unsigned
	* @returns {!Long}
	* @inner
	*/
	function fromNumber(value, unsigned) {
		if (isNaN(value)) return unsigned ? UZERO : ZERO;
		if (unsigned) {
			if (value < 0) return UZERO;
			if (value >= TWO_PWR_64_DBL) return MAX_UNSIGNED_VALUE;
		} else {
			if (value <= -TWO_PWR_63_DBL) return MIN_VALUE;
			if (value + 1 >= TWO_PWR_63_DBL) return MAX_VALUE;
		}
		if (value < 0) return fromNumber(-value, unsigned).neg();
		return fromBits(value % TWO_PWR_32_DBL | 0, value / TWO_PWR_32_DBL | 0, unsigned);
	}
	/**
	* Returns a Long representing the given value, provided that it is a finite number. Otherwise, zero is returned.
	* @function
	* @param {number} value The number in question
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @returns {!Long} The corresponding Long value
	*/
	Long$1.fromNumber = fromNumber;
	/**
	* @param {number} lowBits
	* @param {number} highBits
	* @param {boolean=} unsigned
	* @returns {!Long}
	* @inner
	*/
	function fromBits(lowBits, highBits, unsigned) {
		return new Long$1(lowBits, highBits, unsigned);
	}
	/**
	* Returns a Long representing the 64 bit integer that comes by concatenating the given low and high bits. Each is
	*  assumed to use 32 bits.
	* @function
	* @param {number} lowBits The low 32 bits
	* @param {number} highBits The high 32 bits
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @returns {!Long} The corresponding Long value
	*/
	Long$1.fromBits = fromBits;
	/**
	* @function
	* @param {number} base
	* @param {number} exponent
	* @returns {number}
	* @inner
	*/
	var pow_dbl = Math.pow;
	/**
	* @param {string} str
	* @param {(boolean|number)=} unsigned
	* @param {number=} radix
	* @returns {!Long}
	* @inner
	*/
	function fromString(str, unsigned, radix) {
		if (str.length === 0) throw Error("empty string");
		if (str === "NaN" || str === "Infinity" || str === "+Infinity" || str === "-Infinity") return ZERO;
		if (typeof unsigned === "number") radix = unsigned, unsigned = false;
		else unsigned = !!unsigned;
		radix = radix || 10;
		if (radix < 2 || 36 < radix) throw RangeError("radix");
		var p;
		if ((p = str.indexOf("-")) > 0) throw Error("interior hyphen");
		else if (p === 0) return fromString(str.substring(1), unsigned, radix).neg();
		var radixToPower = fromNumber(pow_dbl(radix, 8));
		var result = ZERO;
		for (var i = 0; i < str.length; i += 8) {
			var size = Math.min(8, str.length - i), value = parseInt(str.substring(i, i + size), radix);
			if (size < 8) {
				var power = fromNumber(pow_dbl(radix, size));
				result = result.mul(power).add(fromNumber(value));
			} else {
				result = result.mul(radixToPower);
				result = result.add(fromNumber(value));
			}
		}
		result.unsigned = unsigned;
		return result;
	}
	/**
	* Returns a Long representation of the given string, written using the specified radix.
	* @function
	* @param {string} str The textual representation of the Long
	* @param {(boolean|number)=} unsigned Whether unsigned or not, defaults to signed
	* @param {number=} radix The radix in which the text is written (2-36), defaults to 10
	* @returns {!Long} The corresponding Long value
	*/
	Long$1.fromString = fromString;
	/**
	* @function
	* @param {!Long|number|string|!{low: number, high: number, unsigned: boolean}} val
	* @param {boolean=} unsigned
	* @returns {!Long}
	* @inner
	*/
	function fromValue(val, unsigned) {
		if (typeof val === "number") return fromNumber(val, unsigned);
		if (typeof val === "string") return fromString(val, unsigned);
		return fromBits(val.low, val.high, typeof unsigned === "boolean" ? unsigned : val.unsigned);
	}
	/**
	* Converts the specified value to a Long using the appropriate from* function for its type.
	* @function
	* @param {!Long|number|string|!{low: number, high: number, unsigned: boolean}} val Value
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @returns {!Long}
	*/
	Long$1.fromValue = fromValue;
	/**
	* @type {number}
	* @const
	* @inner
	*/
	var TWO_PWR_16_DBL = 65536;
	/**
	* @type {number}
	* @const
	* @inner
	*/
	var TWO_PWR_24_DBL = 1 << 24;
	/**
	* @type {number}
	* @const
	* @inner
	*/
	var TWO_PWR_32_DBL = TWO_PWR_16_DBL * TWO_PWR_16_DBL;
	/**
	* @type {number}
	* @const
	* @inner
	*/
	var TWO_PWR_64_DBL = TWO_PWR_32_DBL * TWO_PWR_32_DBL;
	/**
	* @type {number}
	* @const
	* @inner
	*/
	var TWO_PWR_63_DBL = TWO_PWR_64_DBL / 2;
	/**
	* @type {!Long}
	* @const
	* @inner
	*/
	var TWO_PWR_24 = fromInt(TWO_PWR_24_DBL);
	/**
	* @type {!Long}
	* @inner
	*/
	var ZERO = fromInt(0);
	/**
	* Signed zero.
	* @type {!Long}
	*/
	Long$1.ZERO = ZERO;
	/**
	* @type {!Long}
	* @inner
	*/
	var UZERO = fromInt(0, true);
	/**
	* Unsigned zero.
	* @type {!Long}
	*/
	Long$1.UZERO = UZERO;
	/**
	* @type {!Long}
	* @inner
	*/
	var ONE = fromInt(1);
	/**
	* Signed one.
	* @type {!Long}
	*/
	Long$1.ONE = ONE;
	/**
	* @type {!Long}
	* @inner
	*/
	var UONE = fromInt(1, true);
	/**
	* Unsigned one.
	* @type {!Long}
	*/
	Long$1.UONE = UONE;
	/**
	* @type {!Long}
	* @inner
	*/
	var NEG_ONE = fromInt(-1);
	/**
	* Signed negative one.
	* @type {!Long}
	*/
	Long$1.NEG_ONE = NEG_ONE;
	/**
	* @type {!Long}
	* @inner
	*/
	var MAX_VALUE = fromBits(-1, 2147483647, false);
	/**
	* Maximum signed value.
	* @type {!Long}
	*/
	Long$1.MAX_VALUE = MAX_VALUE;
	/**
	* @type {!Long}
	* @inner
	*/
	var MAX_UNSIGNED_VALUE = fromBits(-1, -1, true);
	/**
	* Maximum unsigned value.
	* @type {!Long}
	*/
	Long$1.MAX_UNSIGNED_VALUE = MAX_UNSIGNED_VALUE;
	/**
	* @type {!Long}
	* @inner
	*/
	var MIN_VALUE = fromBits(0, -2147483648, false);
	/**
	* Minimum signed value.
	* @type {!Long}
	*/
	Long$1.MIN_VALUE = MIN_VALUE;
	/**
	* @alias Long.prototype
	* @inner
	*/
	var LongPrototype = Long$1.prototype;
	/**
	* Converts the Long to a 32 bit integer, assuming it is a 32 bit integer.
	* @returns {number}
	*/
	LongPrototype.toInt = function toInt() {
		return this.unsigned ? this.low >>> 0 : this.low;
	};
	/**
	* Converts the Long to a the nearest floating-point representation of this value (double, 53 bit mantissa).
	* @returns {number}
	*/
	LongPrototype.toNumber = function toNumber() {
		if (this.unsigned) return (this.high >>> 0) * TWO_PWR_32_DBL + (this.low >>> 0);
		return this.high * TWO_PWR_32_DBL + (this.low >>> 0);
	};
	/**
	* Converts the Long to a string written in the specified radix.
	* @param {number=} radix Radix (2-36), defaults to 10
	* @returns {string}
	* @override
	* @throws {RangeError} If `radix` is out of range
	*/
	LongPrototype.toString = function toString(radix) {
		radix = radix || 10;
		if (radix < 2 || 36 < radix) throw RangeError("radix");
		if (this.isZero()) return "0";
		if (this.isNegative()) if (this.eq(MIN_VALUE)) {
			var radixLong = fromNumber(radix), div$1 = this.div(radixLong), rem1 = div$1.mul(radixLong).sub(this);
			return div$1.toString(radix) + rem1.toInt().toString(radix);
		} else return "-" + this.neg().toString(radix);
		var radixToPower = fromNumber(pow_dbl(radix, 6), this.unsigned), rem = this;
		var result = "";
		while (true) {
			var remDiv = rem.div(radixToPower), intval = rem.sub(remDiv.mul(radixToPower)).toInt() >>> 0, digits = intval.toString(radix);
			rem = remDiv;
			if (rem.isZero()) return digits + result;
			else {
				while (digits.length < 6) digits = "0" + digits;
				result = "" + digits + result;
			}
		}
	};
	/**
	* Gets the high 32 bits as a signed integer.
	* @returns {number} Signed high bits
	*/
	LongPrototype.getHighBits = function getHighBits() {
		return this.high;
	};
	/**
	* Gets the high 32 bits as an unsigned integer.
	* @returns {number} Unsigned high bits
	*/
	LongPrototype.getHighBitsUnsigned = function getHighBitsUnsigned() {
		return this.high >>> 0;
	};
	/**
	* Gets the low 32 bits as a signed integer.
	* @returns {number} Signed low bits
	*/
	LongPrototype.getLowBits = function getLowBits() {
		return this.low;
	};
	/**
	* Gets the low 32 bits as an unsigned integer.
	* @returns {number} Unsigned low bits
	*/
	LongPrototype.getLowBitsUnsigned = function getLowBitsUnsigned() {
		return this.low >>> 0;
	};
	/**
	* Gets the number of bits needed to represent the absolute value of this Long.
	* @returns {number}
	*/
	LongPrototype.getNumBitsAbs = function getNumBitsAbs() {
		if (this.isNegative()) return this.eq(MIN_VALUE) ? 64 : this.neg().getNumBitsAbs();
		var val = this.high != 0 ? this.high : this.low;
		for (var bit = 31; bit > 0; bit--) if ((val & 1 << bit) != 0) break;
		return this.high != 0 ? bit + 33 : bit + 1;
	};
	/**
	* Tests if this Long's value equals zero.
	* @returns {boolean}
	*/
	LongPrototype.isZero = function isZero() {
		return this.high === 0 && this.low === 0;
	};
	/**
	* Tests if this Long's value equals zero. This is an alias of {@link Long#isZero}.
	* @returns {boolean}
	*/
	LongPrototype.eqz = LongPrototype.isZero;
	/**
	* Tests if this Long's value is negative.
	* @returns {boolean}
	*/
	LongPrototype.isNegative = function isNegative() {
		return !this.unsigned && this.high < 0;
	};
	/**
	* Tests if this Long's value is positive.
	* @returns {boolean}
	*/
	LongPrototype.isPositive = function isPositive() {
		return this.unsigned || this.high >= 0;
	};
	/**
	* Tests if this Long's value is odd.
	* @returns {boolean}
	*/
	LongPrototype.isOdd = function isOdd() {
		return (this.low & 1) === 1;
	};
	/**
	* Tests if this Long's value is even.
	* @returns {boolean}
	*/
	LongPrototype.isEven = function isEven() {
		return (this.low & 1) === 0;
	};
	/**
	* Tests if this Long's value equals the specified's.
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.equals = function equals(other) {
		if (!isLong(other)) other = fromValue(other);
		if (this.unsigned !== other.unsigned && this.high >>> 31 === 1 && other.high >>> 31 === 1) return false;
		return this.high === other.high && this.low === other.low;
	};
	/**
	* Tests if this Long's value equals the specified's. This is an alias of {@link Long#equals}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.eq = LongPrototype.equals;
	/**
	* Tests if this Long's value differs from the specified's.
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.notEquals = function notEquals(other) {
		return !this.eq(other);
	};
	/**
	* Tests if this Long's value differs from the specified's. This is an alias of {@link Long#notEquals}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.neq = LongPrototype.notEquals;
	/**
	* Tests if this Long's value differs from the specified's. This is an alias of {@link Long#notEquals}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.ne = LongPrototype.notEquals;
	/**
	* Tests if this Long's value is less than the specified's.
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.lessThan = function lessThan(other) {
		return this.comp(other) < 0;
	};
	/**
	* Tests if this Long's value is less than the specified's. This is an alias of {@link Long#lessThan}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.lt = LongPrototype.lessThan;
	/**
	* Tests if this Long's value is less than or equal the specified's.
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.lessThanOrEqual = function lessThanOrEqual(other) {
		return this.comp(other) <= 0;
	};
	/**
	* Tests if this Long's value is less than or equal the specified's. This is an alias of {@link Long#lessThanOrEqual}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.lte = LongPrototype.lessThanOrEqual;
	/**
	* Tests if this Long's value is less than or equal the specified's. This is an alias of {@link Long#lessThanOrEqual}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.le = LongPrototype.lessThanOrEqual;
	/**
	* Tests if this Long's value is greater than the specified's.
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.greaterThan = function greaterThan(other) {
		return this.comp(other) > 0;
	};
	/**
	* Tests if this Long's value is greater than the specified's. This is an alias of {@link Long#greaterThan}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.gt = LongPrototype.greaterThan;
	/**
	* Tests if this Long's value is greater than or equal the specified's.
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.greaterThanOrEqual = function greaterThanOrEqual(other) {
		return this.comp(other) >= 0;
	};
	/**
	* Tests if this Long's value is greater than or equal the specified's. This is an alias of {@link Long#greaterThanOrEqual}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.gte = LongPrototype.greaterThanOrEqual;
	/**
	* Tests if this Long's value is greater than or equal the specified's. This is an alias of {@link Long#greaterThanOrEqual}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {boolean}
	*/
	LongPrototype.ge = LongPrototype.greaterThanOrEqual;
	/**
	* Compares this Long's value with the specified's.
	* @param {!Long|number|string} other Other value
	* @returns {number} 0 if they are the same, 1 if the this is greater and -1
	*  if the given one is greater
	*/
	LongPrototype.compare = function compare(other) {
		if (!isLong(other)) other = fromValue(other);
		if (this.eq(other)) return 0;
		var thisNeg = this.isNegative(), otherNeg = other.isNegative();
		if (thisNeg && !otherNeg) return -1;
		if (!thisNeg && otherNeg) return 1;
		if (!this.unsigned) return this.sub(other).isNegative() ? -1 : 1;
		return other.high >>> 0 > this.high >>> 0 || other.high === this.high && other.low >>> 0 > this.low >>> 0 ? -1 : 1;
	};
	/**
	* Compares this Long's value with the specified's. This is an alias of {@link Long#compare}.
	* @function
	* @param {!Long|number|string} other Other value
	* @returns {number} 0 if they are the same, 1 if the this is greater and -1
	*  if the given one is greater
	*/
	LongPrototype.comp = LongPrototype.compare;
	/**
	* Negates this Long's value.
	* @returns {!Long} Negated Long
	*/
	LongPrototype.negate = function negate() {
		if (!this.unsigned && this.eq(MIN_VALUE)) return MIN_VALUE;
		return this.not().add(ONE);
	};
	/**
	* Negates this Long's value. This is an alias of {@link Long#negate}.
	* @function
	* @returns {!Long} Negated Long
	*/
	LongPrototype.neg = LongPrototype.negate;
	/**
	* Returns the sum of this and the specified Long.
	* @param {!Long|number|string} addend Addend
	* @returns {!Long} Sum
	*/
	LongPrototype.add = function add$2(addend) {
		if (!isLong(addend)) addend = fromValue(addend);
		var a48 = this.high >>> 16;
		var a32 = this.high & 65535;
		var a16 = this.low >>> 16;
		var a00 = this.low & 65535;
		var b48 = addend.high >>> 16;
		var b32 = addend.high & 65535;
		var b16 = addend.low >>> 16;
		var b00 = addend.low & 65535;
		var c48 = 0, c32 = 0, c16 = 0, c00 = 0;
		c00 += a00 + b00;
		c16 += c00 >>> 16;
		c00 &= 65535;
		c16 += a16 + b16;
		c32 += c16 >>> 16;
		c16 &= 65535;
		c32 += a32 + b32;
		c48 += c32 >>> 16;
		c32 &= 65535;
		c48 += a48 + b48;
		c48 &= 65535;
		return fromBits(c16 << 16 | c00, c48 << 16 | c32, this.unsigned);
	};
	/**
	* Returns the difference of this and the specified Long.
	* @param {!Long|number|string} subtrahend Subtrahend
	* @returns {!Long} Difference
	*/
	LongPrototype.subtract = function subtract(subtrahend) {
		if (!isLong(subtrahend)) subtrahend = fromValue(subtrahend);
		return this.add(subtrahend.neg());
	};
	/**
	* Returns the difference of this and the specified Long. This is an alias of {@link Long#subtract}.
	* @function
	* @param {!Long|number|string} subtrahend Subtrahend
	* @returns {!Long} Difference
	*/
	LongPrototype.sub = LongPrototype.subtract;
	/**
	* Returns the product of this and the specified Long.
	* @param {!Long|number|string} multiplier Multiplier
	* @returns {!Long} Product
	*/
	LongPrototype.multiply = function multiply(multiplier) {
		if (this.isZero()) return ZERO;
		if (!isLong(multiplier)) multiplier = fromValue(multiplier);
		if (wasm) {
			var low = wasm.mul(this.low, this.high, multiplier.low, multiplier.high);
			return fromBits(low, wasm.get_high(), this.unsigned);
		}
		if (multiplier.isZero()) return ZERO;
		if (this.eq(MIN_VALUE)) return multiplier.isOdd() ? MIN_VALUE : ZERO;
		if (multiplier.eq(MIN_VALUE)) return this.isOdd() ? MIN_VALUE : ZERO;
		if (this.isNegative()) if (multiplier.isNegative()) return this.neg().mul(multiplier.neg());
		else return this.neg().mul(multiplier).neg();
		else if (multiplier.isNegative()) return this.mul(multiplier.neg()).neg();
		if (this.lt(TWO_PWR_24) && multiplier.lt(TWO_PWR_24)) return fromNumber(this.toNumber() * multiplier.toNumber(), this.unsigned);
		var a48 = this.high >>> 16;
		var a32 = this.high & 65535;
		var a16 = this.low >>> 16;
		var a00 = this.low & 65535;
		var b48 = multiplier.high >>> 16;
		var b32 = multiplier.high & 65535;
		var b16 = multiplier.low >>> 16;
		var b00 = multiplier.low & 65535;
		var c48 = 0, c32 = 0, c16 = 0, c00 = 0;
		c00 += a00 * b00;
		c16 += c00 >>> 16;
		c00 &= 65535;
		c16 += a16 * b00;
		c32 += c16 >>> 16;
		c16 &= 65535;
		c16 += a00 * b16;
		c32 += c16 >>> 16;
		c16 &= 65535;
		c32 += a32 * b00;
		c48 += c32 >>> 16;
		c32 &= 65535;
		c32 += a16 * b16;
		c48 += c32 >>> 16;
		c32 &= 65535;
		c32 += a00 * b32;
		c48 += c32 >>> 16;
		c32 &= 65535;
		c48 += a48 * b00 + a32 * b16 + a16 * b32 + a00 * b48;
		c48 &= 65535;
		return fromBits(c16 << 16 | c00, c48 << 16 | c32, this.unsigned);
	};
	/**
	* Returns the product of this and the specified Long. This is an alias of {@link Long#multiply}.
	* @function
	* @param {!Long|number|string} multiplier Multiplier
	* @returns {!Long} Product
	*/
	LongPrototype.mul = LongPrototype.multiply;
	/**
	* Returns this Long divided by the specified. The result is signed if this Long is signed or
	*  unsigned if this Long is unsigned.
	* @param {!Long|number|string} divisor Divisor
	* @returns {!Long} Quotient
	*/
	LongPrototype.divide = function divide(divisor) {
		if (!isLong(divisor)) divisor = fromValue(divisor);
		if (divisor.isZero()) throw Error("division by zero");
		if (wasm) {
			if (!this.unsigned && this.high === -2147483648 && divisor.low === -1 && divisor.high === -1) return this;
			var low = (this.unsigned ? wasm.div_u : wasm.div_s)(this.low, this.high, divisor.low, divisor.high);
			return fromBits(low, wasm.get_high(), this.unsigned);
		}
		if (this.isZero()) return this.unsigned ? UZERO : ZERO;
		var approx, rem, res;
		if (!this.unsigned) {
			if (this.eq(MIN_VALUE)) if (divisor.eq(ONE) || divisor.eq(NEG_ONE)) return MIN_VALUE;
			else if (divisor.eq(MIN_VALUE)) return ONE;
			else {
				approx = this.shr(1).div(divisor).shl(1);
				if (approx.eq(ZERO)) return divisor.isNegative() ? ONE : NEG_ONE;
				else {
					rem = this.sub(divisor.mul(approx));
					res = approx.add(rem.div(divisor));
					return res;
				}
			}
			else if (divisor.eq(MIN_VALUE)) return this.unsigned ? UZERO : ZERO;
			if (this.isNegative()) {
				if (divisor.isNegative()) return this.neg().div(divisor.neg());
				return this.neg().div(divisor).neg();
			} else if (divisor.isNegative()) return this.div(divisor.neg()).neg();
			res = ZERO;
		} else {
			if (!divisor.unsigned) divisor = divisor.toUnsigned();
			if (divisor.gt(this)) return UZERO;
			if (divisor.gt(this.shru(1))) return UONE;
			res = UZERO;
		}
		rem = this;
		while (rem.gte(divisor)) {
			approx = Math.max(1, Math.floor(rem.toNumber() / divisor.toNumber()));
			var log2 = Math.ceil(Math.log(approx) / Math.LN2), delta = log2 <= 48 ? 1 : pow_dbl(2, log2 - 48), approxRes = fromNumber(approx), approxRem = approxRes.mul(divisor);
			while (approxRem.isNegative() || approxRem.gt(rem)) {
				approx -= delta;
				approxRes = fromNumber(approx, this.unsigned);
				approxRem = approxRes.mul(divisor);
			}
			if (approxRes.isZero()) approxRes = ONE;
			res = res.add(approxRes);
			rem = rem.sub(approxRem);
		}
		return res;
	};
	/**
	* Returns this Long divided by the specified. This is an alias of {@link Long#divide}.
	* @function
	* @param {!Long|number|string} divisor Divisor
	* @returns {!Long} Quotient
	*/
	LongPrototype.div = LongPrototype.divide;
	/**
	* Returns this Long modulo the specified.
	* @param {!Long|number|string} divisor Divisor
	* @returns {!Long} Remainder
	*/
	LongPrototype.modulo = function modulo(divisor) {
		if (!isLong(divisor)) divisor = fromValue(divisor);
		if (wasm) {
			var low = (this.unsigned ? wasm.rem_u : wasm.rem_s)(this.low, this.high, divisor.low, divisor.high);
			return fromBits(low, wasm.get_high(), this.unsigned);
		}
		return this.sub(this.div(divisor).mul(divisor));
	};
	/**
	* Returns this Long modulo the specified. This is an alias of {@link Long#modulo}.
	* @function
	* @param {!Long|number|string} divisor Divisor
	* @returns {!Long} Remainder
	*/
	LongPrototype.mod = LongPrototype.modulo;
	/**
	* Returns this Long modulo the specified. This is an alias of {@link Long#modulo}.
	* @function
	* @param {!Long|number|string} divisor Divisor
	* @returns {!Long} Remainder
	*/
	LongPrototype.rem = LongPrototype.modulo;
	/**
	* Returns the bitwise NOT of this Long.
	* @returns {!Long}
	*/
	LongPrototype.not = function not() {
		return fromBits(~this.low, ~this.high, this.unsigned);
	};
	/**
	* Returns the bitwise AND of this Long and the specified.
	* @param {!Long|number|string} other Other Long
	* @returns {!Long}
	*/
	LongPrototype.and = function and(other) {
		if (!isLong(other)) other = fromValue(other);
		return fromBits(this.low & other.low, this.high & other.high, this.unsigned);
	};
	/**
	* Returns the bitwise OR of this Long and the specified.
	* @param {!Long|number|string} other Other Long
	* @returns {!Long}
	*/
	LongPrototype.or = function or(other) {
		if (!isLong(other)) other = fromValue(other);
		return fromBits(this.low | other.low, this.high | other.high, this.unsigned);
	};
	/**
	* Returns the bitwise XOR of this Long and the given one.
	* @param {!Long|number|string} other Other Long
	* @returns {!Long}
	*/
	LongPrototype.xor = function xor(other) {
		if (!isLong(other)) other = fromValue(other);
		return fromBits(this.low ^ other.low, this.high ^ other.high, this.unsigned);
	};
	/**
	* Returns this Long with bits shifted to the left by the given amount.
	* @param {number|!Long} numBits Number of bits
	* @returns {!Long} Shifted Long
	*/
	LongPrototype.shiftLeft = function shiftLeft(numBits) {
		if (isLong(numBits)) numBits = numBits.toInt();
		if ((numBits &= 63) === 0) return this;
		else if (numBits < 32) return fromBits(this.low << numBits, this.high << numBits | this.low >>> 32 - numBits, this.unsigned);
		else return fromBits(0, this.low << numBits - 32, this.unsigned);
	};
	/**
	* Returns this Long with bits shifted to the left by the given amount. This is an alias of {@link Long#shiftLeft}.
	* @function
	* @param {number|!Long} numBits Number of bits
	* @returns {!Long} Shifted Long
	*/
	LongPrototype.shl = LongPrototype.shiftLeft;
	/**
	* Returns this Long with bits arithmetically shifted to the right by the given amount.
	* @param {number|!Long} numBits Number of bits
	* @returns {!Long} Shifted Long
	*/
	LongPrototype.shiftRight = function shiftRight(numBits) {
		if (isLong(numBits)) numBits = numBits.toInt();
		if ((numBits &= 63) === 0) return this;
		else if (numBits < 32) return fromBits(this.low >>> numBits | this.high << 32 - numBits, this.high >> numBits, this.unsigned);
		else return fromBits(this.high >> numBits - 32, this.high >= 0 ? 0 : -1, this.unsigned);
	};
	/**
	* Returns this Long with bits arithmetically shifted to the right by the given amount. This is an alias of {@link Long#shiftRight}.
	* @function
	* @param {number|!Long} numBits Number of bits
	* @returns {!Long} Shifted Long
	*/
	LongPrototype.shr = LongPrototype.shiftRight;
	/**
	* Returns this Long with bits logically shifted to the right by the given amount.
	* @param {number|!Long} numBits Number of bits
	* @returns {!Long} Shifted Long
	*/
	LongPrototype.shiftRightUnsigned = function shiftRightUnsigned(numBits) {
		if (isLong(numBits)) numBits = numBits.toInt();
		numBits &= 63;
		if (numBits === 0) return this;
		else {
			var high = this.high;
			if (numBits < 32) {
				var low = this.low;
				return fromBits(low >>> numBits | high << 32 - numBits, high >>> numBits, this.unsigned);
			} else if (numBits === 32) return fromBits(high, 0, this.unsigned);
			else return fromBits(high >>> numBits - 32, 0, this.unsigned);
		}
	};
	/**
	* Returns this Long with bits logically shifted to the right by the given amount. This is an alias of {@link Long#shiftRightUnsigned}.
	* @function
	* @param {number|!Long} numBits Number of bits
	* @returns {!Long} Shifted Long
	*/
	LongPrototype.shru = LongPrototype.shiftRightUnsigned;
	/**
	* Returns this Long with bits logically shifted to the right by the given amount. This is an alias of {@link Long#shiftRightUnsigned}.
	* @function
	* @param {number|!Long} numBits Number of bits
	* @returns {!Long} Shifted Long
	*/
	LongPrototype.shr_u = LongPrototype.shiftRightUnsigned;
	/**
	* Converts this Long to signed.
	* @returns {!Long} Signed long
	*/
	LongPrototype.toSigned = function toSigned() {
		if (!this.unsigned) return this;
		return fromBits(this.low, this.high, false);
	};
	/**
	* Converts this Long to unsigned.
	* @returns {!Long} Unsigned long
	*/
	LongPrototype.toUnsigned = function toUnsigned() {
		if (this.unsigned) return this;
		return fromBits(this.low, this.high, true);
	};
	/**
	* Converts this Long to its byte representation.
	* @param {boolean=} le Whether little or big endian, defaults to big endian
	* @returns {!Array.<number>} Byte representation
	*/
	LongPrototype.toBytes = function toBytes(le) {
		return le ? this.toBytesLE() : this.toBytesBE();
	};
	/**
	* Converts this Long to its little endian byte representation.
	* @returns {!Array.<number>} Little endian byte representation
	*/
	LongPrototype.toBytesLE = function toBytesLE() {
		var hi = this.high, lo = this.low;
		return [
			lo & 255,
			lo >>> 8 & 255,
			lo >>> 16 & 255,
			lo >>> 24,
			hi & 255,
			hi >>> 8 & 255,
			hi >>> 16 & 255,
			hi >>> 24
		];
	};
	/**
	* Converts this Long to its big endian byte representation.
	* @returns {!Array.<number>} Big endian byte representation
	*/
	LongPrototype.toBytesBE = function toBytesBE() {
		var hi = this.high, lo = this.low;
		return [
			hi >>> 24,
			hi >>> 16 & 255,
			hi >>> 8 & 255,
			hi & 255,
			lo >>> 24,
			lo >>> 16 & 255,
			lo >>> 8 & 255,
			lo & 255
		];
	};
	/**
	* Creates a Long from its byte representation.
	* @param {!Array.<number>} bytes Byte representation
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @param {boolean=} le Whether little or big endian, defaults to big endian
	* @returns {Long} The corresponding Long value
	*/
	Long$1.fromBytes = function fromBytes(bytes, unsigned, le) {
		return le ? Long$1.fromBytesLE(bytes, unsigned) : Long$1.fromBytesBE(bytes, unsigned);
	};
	/**
	* Creates a Long from its little endian byte representation.
	* @param {!Array.<number>} bytes Little endian byte representation
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @returns {Long} The corresponding Long value
	*/
	Long$1.fromBytesLE = function fromBytesLE(bytes, unsigned) {
		return new Long$1(bytes[0] | bytes[1] << 8 | bytes[2] << 16 | bytes[3] << 24, bytes[4] | bytes[5] << 8 | bytes[6] << 16 | bytes[7] << 24, unsigned);
	};
	/**
	* Creates a Long from its big endian byte representation.
	* @param {!Array.<number>} bytes Big endian byte representation
	* @param {boolean=} unsigned Whether unsigned or not, defaults to signed
	* @returns {Long} The corresponding Long value
	*/
	Long$1.fromBytesBE = function fromBytesBE(bytes, unsigned) {
		return new Long$1(bytes[4] << 24 | bytes[5] << 16 | bytes[6] << 8 | bytes[7], bytes[0] << 24 | bytes[1] << 16 | bytes[2] << 8 | bytes[3], unsigned);
	};
}) });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/hash_util.js
var import_long = /* @__PURE__ */ __toESM(require_long());
var Long = import_long.default || import_long;
function hexToLong(hex) {
	return Long.fromString(hex, true, 16);
}
var k0 = hexToLong("c3a5c85c97cb3127");
var k1 = hexToLong("b492b66fbe98f273");
var k2 = hexToLong("9ae16a3b2f90404f");
function shiftMix(val) {
	return val.xor(val.shru(47));
}
function fetch$2(s, offset, numBytes) {
	const bytes = s.slice(offset, offset + numBytes);
	return Long.fromBytes(Array.from(bytes), true, true);
}
function fetch64(s, offset) {
	return fetch$2(s, offset, 8);
}
function fetch32(s, offset) {
	return fetch$2(s, offset, 4);
}
function rotate64(val, shift) {
	return shift === 0 ? val : val.shru(shift).or(val.shl(64 - shift));
}
function hashLen16(u, v, mul$1 = hexToLong("9ddfea08eb382d69")) {
	let a = u.xor(v).mul(mul$1);
	a = a.xor(a.shru(47));
	let b = v.xor(a).mul(mul$1);
	b = b.xor(b.shru(47));
	b = b.mul(mul$1);
	return b;
}
function weakHashLen32WithSeeds(w, x, y, z, a, b) {
	a = a.add(w);
	b = rotate64(b.add(a).add(z), 21);
	const c = a;
	a = a.add(x);
	a = a.add(y);
	b = b.add(rotate64(a, 44));
	return [a.add(z), b.add(c)];
}
function weakHashLen32WithSeedsStr(s, offset, a, b) {
	return weakHashLen32WithSeeds(fetch64(s, offset), fetch64(s, offset + 8), fetch64(s, offset + 16), fetch64(s, offset + 24), a, b);
}
function hashLen0to16(s, len = s.length) {
	if (len >= 8) {
		const mul$1 = k2.add(len * 2);
		const a = fetch64(s, 0).add(k2);
		const b = fetch64(s, len - 8);
		const c = rotate64(b, 37).mul(mul$1).add(a);
		const d = rotate64(a, 25).add(b).mul(mul$1);
		return hashLen16(c, d, mul$1);
	}
	if (len >= 4) {
		const mul$1 = k2.add(len * 2);
		const a = fetch32(s, 0);
		return hashLen16(a.shl(3).add(len), fetch32(s, len - 4), mul$1);
	}
	if (len > 0) {
		const a = s[0];
		const b = s[len >> 1];
		const c = s[len - 1];
		const y = a + (b << 8);
		const z = len + (c << 2);
		return shiftMix(k2.mul(y).xor(k0.mul(z))).mul(k2);
	}
	return k2;
}
function hashLen17to32(s, len = s.length) {
	const mul$1 = k2.add(len * 2);
	const a = fetch64(s, 0).mul(k1);
	const b = fetch64(s, 8);
	const c = fetch64(s, len - 8).mul(mul$1);
	const d = fetch64(s, len - 16).mul(k2);
	return hashLen16(rotate64(a.add(b), 43).add(rotate64(c, 30)).add(d), a.add(rotate64(b.add(k2), 18)).add(c), mul$1);
}
function hashLen33to64(s, len = s.length) {
	const mul$1 = k2.add(len * 2);
	const a = fetch64(s, 0).mul(k2);
	const b = fetch64(s, 8);
	const c = fetch64(s, len - 8).mul(mul$1);
	const d = fetch64(s, len - 16).mul(k2);
	const y = rotate64(a.add(b), 43).add(rotate64(c, 30)).add(d);
	const z = hashLen16(y, a.add(rotate64(b.add(k2), 18)).add(c), mul$1);
	const e = fetch64(s, 16).mul(mul$1);
	const f = fetch64(s, 24);
	const g = y.add(fetch64(s, len - 32)).mul(mul$1);
	const h = z.add(fetch64(s, len - 24)).mul(mul$1);
	return hashLen16(rotate64(e.add(f), 43).add(rotate64(g, 30)).add(h), e.add(rotate64(f.add(a), 18)).add(g), mul$1);
}
function fingerPrint64(s, len = s.length) {
	const seed = Long.fromNumber(81, true);
	if (len <= 32) if (len <= 16) return hashLen0to16(s, len);
	else return hashLen17to32(s, len);
	else if (len <= 64) return hashLen33to64(s, len);
	let x = seed;
	let y = seed.mul(k1).add(113);
	let z = shiftMix(y.mul(k2).add(113)).mul(k2);
	let v = [Long.UZERO, Long.UZERO];
	let w = [Long.UZERO, Long.UZERO];
	x = x.mul(k2).add(fetch64(s, 0));
	let offset = 0;
	const end = (len - 1 >> 6) * 64;
	const last64 = end + (len - 1 & 63) - 63;
	do {
		x = rotate64(x.add(y).add(v[0]).add(fetch64(s, offset + 8)), 37).mul(k1);
		y = rotate64(y.add(v[1]).add(fetch64(s, offset + 48)), 42).mul(k1);
		x = x.xor(w[1]);
		y = y.add(v[0]).add(fetch64(s, offset + 40));
		z = rotate64(z.add(w[0]), 33).mul(k1);
		v = weakHashLen32WithSeedsStr(s, offset, v[1].mul(k1), x.add(w[0]));
		w = weakHashLen32WithSeedsStr(s, offset + 32, z.add(w[1]), y.add(fetch64(s, offset + 16)));
		[z, x] = [x, z];
		offset += 64;
	} while (offset !== end);
	const mul$1 = k1.add(z.and(255).shl(1));
	offset = last64;
	w[0] = w[0].add(len - 1 & 63);
	v[0] = v[0].add(w[0]);
	w[0] = w[0].add(v[0]);
	x = rotate64(x.add(y).add(v[0]).add(fetch64(s, offset + 8)), 37).mul(mul$1);
	y = rotate64(y.add(v[1]).add(fetch64(s, offset + 48)), 42).mul(mul$1);
	x = x.xor(w[1].mul(9));
	y = y.add(v[0].mul(9).add(fetch64(s, offset + 40)));
	z = rotate64(z.add(w[0]), 33).mul(mul$1);
	v = weakHashLen32WithSeedsStr(s, offset, v[1].mul(mul$1), x.add(w[0]));
	w = weakHashLen32WithSeedsStr(s, offset + 32, z.add(w[1]), y.add(fetch64(s, offset + 16)));
	[z, x] = [x, z];
	return hashLen16(hashLen16(v[0], w[0], mul$1).add(shiftMix(y).mul(k0)).add(z), hashLen16(v[1], w[1], mul$1).add(x), mul$1);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/util.js
var util_exports = /* @__PURE__ */ __export({
	arraysEqual: () => arraysEqual,
	assert: () => assert,
	assertNonNegativeIntegerDimensions: () => assertNonNegativeIntegerDimensions,
	assertNonNull: () => assertNonNull,
	assertShapesMatch: () => assertShapesMatch,
	bytesFromStringArray: () => bytesFromStringArray,
	bytesPerElement: () => bytesPerElement,
	checkConversionForErrors: () => checkConversionForErrors,
	clamp: () => clamp,
	computeStrides: () => computeStrides,
	createScalarValue: () => createScalarValue,
	createShuffledIndices: () => createShuffledIndices,
	decodeString: () => decodeString,
	distSquared: () => distSquared,
	encodeString: () => encodeString,
	fetch: () => fetch$1,
	fingerPrint64: () => fingerPrint64,
	flatten: () => flatten,
	getArrayFromDType: () => getArrayFromDType,
	getTypedArrayFromDType: () => getTypedArrayFromDType,
	hasEncodingLoss: () => hasEncodingLoss,
	hexToLong: () => hexToLong,
	indexToLoc: () => indexToLoc,
	inferDtype: () => inferDtype,
	inferFromImplicitShape: () => inferFromImplicitShape,
	isBoolean: () => isBoolean,
	isFunction: () => isFunction,
	isInt: () => isInt,
	isNumber: () => isNumber,
	isPromise: () => isPromise,
	isScalarShape: () => isScalarShape,
	isString: () => isString,
	isTypedArray: () => isTypedArray,
	isValidDtype: () => isValidDtype,
	locToIndex: () => locToIndex,
	makeOnesTypedArray: () => makeOnesTypedArray,
	makeZerosNestedTypedArray: () => makeZerosNestedTypedArray,
	makeZerosTypedArray: () => makeZerosTypedArray,
	nearestDivisor: () => nearestDivisor,
	nearestLargerEven: () => nearestLargerEven,
	now: () => now,
	parseAxisParam: () => parseAxisParam,
	randUniform: () => randUniform,
	repeatedTry: () => repeatedTry,
	rightPad: () => rightPad,
	shuffle: () => shuffle,
	shuffleCombo: () => shuffleCombo,
	sizeFromShape: () => sizeFromShape,
	sizeToSquarishShape: () => sizeToSquarishShape,
	squeezeShape: () => squeezeShape,
	sum: () => sum$1,
	swap: () => swap,
	tanh: () => tanh$1,
	toNestedArray: () => toNestedArray,
	toTypedArray: () => toTypedArray
});
/**
* Create typed array for scalar value. Used for storing in `DataStorage`.
*/
function createScalarValue(value, dtype) {
	if (dtype === "string") return encodeString(value);
	return toTypedArray([value], dtype);
}
function noConversionNeeded(a, dtype) {
	return a instanceof Float32Array && dtype === "float32" || a instanceof Int32Array && dtype === "int32" || a instanceof Uint8Array && dtype === "bool";
}
function toTypedArray(a, dtype) {
	if (dtype === "string") throw new Error("Cannot convert a string[] to a TypedArray");
	if (Array.isArray(a)) a = flatten(a);
	if (env().getBool("DEBUG")) checkConversionForErrors(a, dtype);
	if (noConversionNeeded(a, dtype)) return a;
	if (dtype == null || dtype === "float32" || dtype === "complex64") return new Float32Array(a);
	else if (dtype === "int32") return new Int32Array(a);
	else if (dtype === "bool") {
		const bool = new Uint8Array(a.length);
		for (let i = 0; i < bool.length; ++i) if (Math.round(a[i]) !== 0) bool[i] = 1;
		return bool;
	} else throw new Error(`Unknown data type ${dtype}`);
}
/**
* Returns the current high-resolution time in milliseconds relative to an
* arbitrary time in the past. It works across different platforms (node.js,
* browsers).
*
* ```js
* console.log(tf.util.now());
* ```
*
* @doc {heading: 'Util', namespace: 'util'}
*/
function now() {
	return env().platform.now();
}
/**
* Returns a platform-specific implementation of
* [`fetch`](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API).
*
* If `fetch` is defined on the global object (`window`, `process`, etc.),
* `tf.util.fetch` returns that function.
*
* If not, `tf.util.fetch` returns a platform-specific solution.
*
* ```js
* const resource = await tf.util.fetch('https://unpkg.com/@tensorflow/tfjs');
* // handle response
* ```
*
* @doc {heading: 'Util'}
*/
function fetch$1(path, requestInits) {
	return env().platform.fetch(path, requestInits);
}
/**
* Encodes the provided string into bytes using the provided encoding scheme.
*
* @param s The string to encode.
* @param encoding The encoding scheme. Defaults to utf-8.
*
* @doc {heading: 'Util'}
*/
function encodeString(s, encoding = "utf-8") {
	encoding = encoding || "utf-8";
	return env().platform.encode(s, encoding);
}
/**
* Decodes the provided bytes into a string using the provided encoding scheme.
* @param bytes The bytes to decode.
*
* @param encoding The encoding scheme. Defaults to utf-8.
*
* @doc {heading: 'Util'}
*/
function decodeString(bytes, encoding = "utf-8") {
	encoding = encoding || "utf-8";
	return env().platform.decode(bytes, encoding);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/profiler.js
var Profiler = class {
	constructor(backendTimer, logger) {
		this.backendTimer = backendTimer;
		this.logger = logger;
		if (logger == null) this.logger = new Logger();
	}
	profileKernel(kernelName, inputs, f) {
		let outputs;
		const holdResultWrapperFn = () => {
			outputs = f();
		};
		let timer;
		const start = now();
		if (this.backendTimer.timerAvailable()) timer = this.backendTimer.time(holdResultWrapperFn);
		else {
			holdResultWrapperFn();
			for (const output of outputs) output.dataSync();
			timer = Promise.resolve({ kernelMs: now() - start });
		}
		if (env().getBool("CHECK_COMPUTATION_FOR_ERRORS")) for (let i = 0; i < outputs.length; i++) {
			const output = outputs[i];
			output.data().then((tensorVals) => {
				checkComputationForErrors(tensorVals, output.dtype, kernelName);
			});
		}
		return {
			kernelName,
			outputs,
			inputs,
			timeMs: timer.then((timing) => timing.kernelMs),
			extraInfo: timer.then((timing) => timing.getExtraProfileInfo != null ? timing.getExtraProfileInfo() : "")
		};
	}
	logKernelProfile(kernelProfile) {
		const { kernelName, outputs, timeMs, inputs, extraInfo } = kernelProfile;
		outputs.forEach((result) => {
			Promise.all([
				result.data(),
				timeMs,
				extraInfo
			]).then((valueContainer) => {
				this.logger.logKernelProfile(kernelName, result, valueContainer[0], valueContainer[1], inputs, valueContainer[2]);
			});
		});
	}
};
function checkComputationForErrors(vals, dtype, kernelName) {
	if (dtype !== "float32") return false;
	for (let i = 0; i < vals.length; i++) {
		const num = vals[i];
		if (isNaN(num) || !isFinite(num)) {
			console.warn(`Found ${num} in the result of '${kernelName}'`);
			return true;
		}
	}
	return false;
}
var Logger = class {
	logKernelProfile(name, result, vals, timeMs, inputs, extraInfo) {
		const time$1 = typeof timeMs === "number" ? rightPad(`${timeMs}ms`, 9) : timeMs["error"];
		const paddedName = rightPad(name, 25);
		const rank = result.rank;
		const size = result.size;
		const shape = rightPad(result.shape.toString(), 14);
		let inputShapesDescription = "";
		for (const name$1 in inputs) {
			const input = inputs[name$1];
			if (input != null) {
				const inputShape = input.shape || result.shape;
				const inputRank = inputShape.length;
				inputShapesDescription += `${name$1}: ${inputRank}D ${inputRank > 0 ? inputShape : ""} `;
			}
		}
		console.log(`%c${paddedName}\t%c${time$1}\t%c${rank}D ${shape}\t%c${size}\t%c${inputShapesDescription}\t%c${extraInfo}`, "font-weight:bold", "color:red", "color:blue", "color: orange", "color: green", "color: steelblue");
	}
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/tape.js
/**
* Computes a list of TapeNodes that connect x to y, filtering everything else
* out and preserving the order of the original tape elements.
*
* @param tape The tape elements to filter.
* @param xs The input Tensors.
* @param y The output Tensor.
*/
function getFilteredNodesXToY(tape, xs, y) {
	const tensorsFromX = {};
	const nodesFromX = {};
	for (let i = 0; i < xs.length; i++) tensorsFromX[xs[i].id] = true;
	for (let i = 0; i < tape.length; i++) {
		const node = tape[i];
		const nodeInputs = node.inputs;
		for (const inputName in nodeInputs) {
			const input = nodeInputs[inputName];
			let anyInputFromX = false;
			for (let j = 0; j < xs.length; j++) if (tensorsFromX[input.id]) {
				node.outputs.forEach((output) => tensorsFromX[output.id] = true);
				anyInputFromX = true;
				nodesFromX[node.id] = true;
				break;
			}
			if (anyInputFromX) break;
		}
	}
	const tensorsLeadToY = {};
	tensorsLeadToY[y.id] = true;
	const nodesToY = {};
	for (let i = tape.length - 1; i >= 0; i--) {
		const node = tape[i];
		const nodeInputs = node.inputs;
		for (let j = 0; j < node.outputs.length; j++) if (tensorsLeadToY[node.outputs[j].id]) {
			for (const inputName in nodeInputs) {
				tensorsLeadToY[nodeInputs[inputName].id] = true;
				nodesToY[node.id] = true;
			}
			break;
		}
	}
	const filteredTape = [];
	for (let i = 0; i < tape.length; i++) {
		const node = tape[i];
		if (nodesFromX[node.id] && nodesToY[node.id]) {
			const prunedInputs = {};
			for (const inputName in node.inputs) {
				const nodeInput = node.inputs[inputName];
				if (tensorsFromX[nodeInput.id]) prunedInputs[inputName] = nodeInput;
			}
			const prunedNode = Object.assign({}, node);
			prunedNode.inputs = prunedInputs;
			prunedNode.outputs = node.outputs;
			filteredTape.push(prunedNode);
		}
	}
	return filteredTape;
}
/**
* Backpropagate gradients through the filtered TapeNodes.
*
* @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map
* is mutated by this method.
* @param filteredTape The filtered TapeNodes to backprop through.
*/
function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy$1, add$2) {
	for (let i = filteredTape.length - 1; i >= 0; i--) {
		const node = filteredTape[i];
		const dys = [];
		node.outputs.forEach((o) => {
			const gradTensor = tensorAccumulatedGradientMap[o.id];
			if (gradTensor != null) dys.push(gradTensor);
			else dys.push(null);
		});
		if (node.gradient == null) throw new Error(`Cannot compute gradient: gradient function not found for ${node.kernelName}.`);
		const inputGradients = node.gradient(dys);
		for (const inputName in node.inputs) {
			if (!(inputName in inputGradients)) throw new Error(`Cannot backprop through input ${inputName}. Available gradients found: ${Object.keys(inputGradients)}.`);
			const dx = tidy$1(() => inputGradients[inputName]());
			if (dx.dtype !== "float32") throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);
			const x = node.inputs[inputName];
			if (!arraysEqual(dx.shape, x.shape)) throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input '${inputName}' has shape '${dx.shape}', which does not match the shape of the input '${x.shape}'`);
			if (tensorAccumulatedGradientMap[x.id] == null) tensorAccumulatedGradientMap[x.id] = dx;
			else {
				const curGradient = tensorAccumulatedGradientMap[x.id];
				tensorAccumulatedGradientMap[x.id] = add$2(curGradient, dx);
				curGradient.dispose();
			}
		}
	}
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/tensor_format.js
var FORMAT_LIMIT_NUM_VALS = 20;
var FORMAT_NUM_FIRST_LAST_VALS = 3;
var FORMAT_NUM_SIG_DIGITS = 7;
function tensorToString(vals, shape, dtype, verbose) {
	const strides = computeStrides(shape);
	const padPerCol = computeMaxSizePerColumn(vals, shape, dtype, strides);
	const rank = shape.length;
	const valsLines = subTensorToString(vals, shape, dtype, strides, padPerCol);
	const lines = ["Tensor"];
	if (verbose) {
		lines.push(`  dtype: ${dtype}`);
		lines.push(`  rank: ${rank}`);
		lines.push(`  shape: [${shape}]`);
		lines.push(`  values:`);
	}
	lines.push(valsLines.map((l) => "    " + l).join("\n"));
	return lines.join("\n");
}
function computeMaxSizePerColumn(vals, shape, dtype, strides) {
	const n = sizeFromShape(shape);
	const numCols = strides[strides.length - 1];
	const padPerCol = new Array(numCols).fill(0);
	const rank = shape.length;
	const valuesOrTuples = dtype === "complex64" ? createComplexTuples(vals) : vals;
	if (rank > 1) for (let row = 0; row < n / numCols; row++) {
		const offset = row * numCols;
		for (let j = 0; j < numCols; j++) padPerCol[j] = Math.max(padPerCol[j], valToString(valuesOrTuples[offset + j], 0, dtype).length);
	}
	return padPerCol;
}
function valToString(val, pad$1, dtype) {
	let valStr;
	if (Array.isArray(val)) valStr = `${parseFloat(val[0].toFixed(FORMAT_NUM_SIG_DIGITS))} + ${parseFloat(val[1].toFixed(FORMAT_NUM_SIG_DIGITS))}j`;
	else if (isString(val)) valStr = `'${val}'`;
	else if (dtype === "bool") valStr = boolNumToString(val);
	else valStr = parseFloat(val.toFixed(FORMAT_NUM_SIG_DIGITS)).toString();
	return rightPad(valStr, pad$1);
}
function boolNumToString(v) {
	return v === 0 ? "false" : "true";
}
function subTensorToString(vals, shape, dtype, strides, padPerCol, isLast = true) {
	const storagePerElement = dtype === "complex64" ? 2 : 1;
	const size = shape[0];
	const rank = shape.length;
	if (rank === 0) {
		if (dtype === "complex64") {
			const complexTuple = createComplexTuples(vals);
			return [valToString(complexTuple[0], 0, dtype)];
		}
		if (dtype === "bool") return [boolNumToString(vals[0])];
		return [vals[0].toString()];
	}
	if (rank === 1) {
		if (size > FORMAT_LIMIT_NUM_VALS) {
			const firstValsSize = FORMAT_NUM_FIRST_LAST_VALS * storagePerElement;
			let firstVals = Array.from(vals.slice(0, firstValsSize));
			let lastVals = Array.from(vals.slice((size - FORMAT_NUM_FIRST_LAST_VALS) * storagePerElement, size * storagePerElement));
			if (dtype === "complex64") {
				firstVals = createComplexTuples(firstVals);
				lastVals = createComplexTuples(lastVals);
			}
			return ["[" + firstVals.map((x, i) => valToString(x, padPerCol[i], dtype)).join(", ") + ", ..., " + lastVals.map((x, i) => valToString(x, padPerCol[size - FORMAT_NUM_FIRST_LAST_VALS + i], dtype)).join(", ") + "]"];
		}
		return ["[" + (dtype === "complex64" ? createComplexTuples(vals) : Array.from(vals)).map((x, i) => valToString(x, padPerCol[i], dtype)).join(", ") + "]"];
	}
	const subshape = shape.slice(1);
	const substrides = strides.slice(1);
	const stride = strides[0] * storagePerElement;
	const lines = [];
	if (size > FORMAT_LIMIT_NUM_VALS) {
		for (let i = 0; i < FORMAT_NUM_FIRST_LAST_VALS; i++) {
			const start = i * stride;
			const end = start + stride;
			lines.push(...subTensorToString(vals.slice(start, end), subshape, dtype, substrides, padPerCol, false));
		}
		lines.push("...");
		for (let i = size - FORMAT_NUM_FIRST_LAST_VALS; i < size; i++) {
			const start = i * stride;
			const end = start + stride;
			lines.push(...subTensorToString(vals.slice(start, end), subshape, dtype, substrides, padPerCol, i === size - 1));
		}
	} else for (let i = 0; i < size; i++) {
		const start = i * stride;
		const end = start + stride;
		lines.push(...subTensorToString(vals.slice(start, end), subshape, dtype, substrides, padPerCol, i === size - 1));
	}
	const sep = rank === 2 ? "," : "";
	lines[0] = "[" + lines[0] + sep;
	for (let i = 1; i < lines.length - 1; i++) lines[i] = " " + lines[i] + sep;
	let newLineSep = ",\n";
	for (let i = 2; i < rank; i++) newLineSep += "\n";
	lines[lines.length - 1] = " " + lines[lines.length - 1] + "]" + (isLast ? "" : newLineSep);
	return lines;
}
function createComplexTuples(vals) {
	const complexTuples = [];
	for (let i = 0; i < vals.length; i += 2) complexTuples.push([vals[i], vals[i + 1]]);
	return complexTuples;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/tensor.js
/**
* A mutable object, similar to `tf.Tensor`, that allows users to set values
* at locations before converting to an immutable `tf.Tensor`.
*
* See `tf.buffer` for creating a tensor buffer.
*
* @doc {heading: 'Tensors', subheading: 'Classes'}
*/
var TensorBuffer = class {
	constructor(shape, dtype, values) {
		this.dtype = dtype;
		this.shape = shape.slice();
		this.size = sizeFromShape(shape);
		if (values != null) {
			const n = values.length;
			assert(n === this.size, () => `Length of values '${n}' does not match the size inferred by the shape '${this.size}'.`);
		}
		if (dtype === "complex64") throw new Error("complex64 dtype TensorBuffers are not supported. Please create a TensorBuffer for the real and imaginary parts separately and call tf.complex(real, imag).");
		this.values = values || getArrayFromDType(dtype, this.size);
		this.strides = computeStrides(shape);
	}
	/**
	* Sets a value in the buffer at a given location.
	*
	* @param value The value to set.
	* @param locs  The location indices.
	*
	* @doc {heading: 'Tensors', subheading: 'Creation'}
	*/
	set(value, ...locs) {
		if (locs.length === 0) locs = [0];
		assert(locs.length === this.rank, () => `The number of provided coordinates (${locs.length}) must match the rank (${this.rank})`);
		const index = this.locToIndex(locs);
		this.values[index] = value;
	}
	/**
	* Returns the value in the buffer at the provided location.
	*
	* @param locs The location indices.
	*
	* @doc {heading: 'Tensors', subheading: 'Creation'}
	*/
	get(...locs) {
		if (locs.length === 0) locs = [0];
		let i = 0;
		for (const loc of locs) {
			if (loc < 0 || loc >= this.shape[i]) {
				const msg = `Requested out of range element at ${locs}.   Buffer shape=${this.shape}`;
				throw new Error(msg);
			}
			i++;
		}
		let index = locs[locs.length - 1];
		for (let i$1 = 0; i$1 < locs.length - 1; ++i$1) index += this.strides[i$1] * locs[i$1];
		return this.values[index];
	}
	locToIndex(locs) {
		if (this.rank === 0) return 0;
		else if (this.rank === 1) return locs[0];
		let index = locs[locs.length - 1];
		for (let i = 0; i < locs.length - 1; ++i) index += this.strides[i] * locs[i];
		return index;
	}
	indexToLoc(index) {
		if (this.rank === 0) return [];
		else if (this.rank === 1) return [index];
		const locs = new Array(this.shape.length);
		for (let i = 0; i < locs.length - 1; ++i) {
			locs[i] = Math.floor(index / this.strides[i]);
			index -= locs[i] * this.strides[i];
		}
		locs[locs.length - 1] = index;
		return locs;
	}
	get rank() {
		return this.shape.length;
	}
	/**
	* Creates an immutable `tf.Tensor` object from the buffer.
	*
	* @doc {heading: 'Tensors', subheading: 'Creation'}
	*/
	toTensor() {
		return trackerFn().makeTensor(this.values, this.shape, this.dtype);
	}
};
var trackerFn = null;
var opHandler = null;
/**
* An external consumer can register itself as the tensor tracker. This way
* the Tensor class can notify the tracker for every tensor created and
* disposed.
*/
function setTensorTracker(fn) {
	trackerFn = fn;
}
/**
* An external consumer can register itself as the op handler. This way the
* Tensor class can have chaining methods that call into ops via the op
* handler.
*/
function setOpHandler(handler) {
	opHandler = handler;
}
/**
* A `tf.Tensor` object represents an immutable, multidimensional array of
* numbers that has a shape and a data type.
*
* For performance reasons, functions that create tensors do not necessarily
* perform a copy of the data passed to them (e.g. if the data is passed as a
* `Float32Array`), and changes to the data will change the tensor. This is not
* a feature and is not supported. To avoid this behavior, use the tensor before
* changing the input data or create a copy with `copy = tf.add(yourTensor, 0)`.
*
* See `tf.tensor` for details on how to create a `tf.Tensor`.
*
* @doc {heading: 'Tensors', subheading: 'Classes'}
*/
var Tensor = class {
	constructor(shape, dtype, dataId, id) {
		/** Whether this tensor has been globally kept. */
		this.kept = false;
		this.isDisposedInternal = false;
		this.shape = shape.slice();
		this.dtype = dtype || "float32";
		this.size = sizeFromShape(shape);
		this.strides = computeStrides(shape);
		this.dataId = dataId;
		this.id = id;
		this.rankType = this.rank < 5 ? this.rank.toString() : "higher";
	}
	get rank() {
		return this.shape.length;
	}
	/**
	* Returns a promise of `tf.TensorBuffer` that holds the underlying data.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	async buffer() {
		const vals = await this.data();
		return opHandler.buffer(this.shape, this.dtype, vals);
	}
	/**
	* Returns a `tf.TensorBuffer` that holds the underlying data.
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	bufferSync() {
		return opHandler.buffer(this.shape, this.dtype, this.dataSync());
	}
	/**
	* Returns the tensor data as a nested array. The transfer of data is done
	* asynchronously.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	async array() {
		const vals = await this.data();
		return toNestedArray(this.shape, vals, this.dtype === "complex64");
	}
	/**
	* Returns the tensor data as a nested array. The transfer of data is done
	* synchronously.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	arraySync() {
		return toNestedArray(this.shape, this.dataSync(), this.dtype === "complex64");
	}
	/**
	* Asynchronously downloads the values from the `tf.Tensor`. Returns a
	* promise of `TypedArray` that resolves when the computation has finished.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	async data() {
		this.throwIfDisposed();
		const data = trackerFn().read(this.dataId);
		if (this.dtype === "string") {
			const bytes = await data;
			try {
				return bytes.map((b) => decodeString(b));
			} catch (_a) {
				throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().");
			}
		}
		return data;
	}
	/**
	* Synchronously downloads the values from the `tf.Tensor`. This blocks the
	* UI thread until the values are ready, which can cause performance issues.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	dataSync() {
		this.throwIfDisposed();
		const data = trackerFn().readSync(this.dataId);
		if (this.dtype === "string") try {
			return data.map((b) => decodeString(b));
		} catch (_a) {
			throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().");
		}
		return data;
	}
	/** Returns the underlying bytes of the tensor's data. */
	async bytes() {
		this.throwIfDisposed();
		const data = await trackerFn().read(this.dataId);
		if (this.dtype === "string") return data;
		else return new Uint8Array(data.buffer);
	}
	/**
	* Disposes `tf.Tensor` from memory.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	dispose() {
		if (this.isDisposed) return;
		trackerFn().disposeTensor(this);
		this.isDisposedInternal = true;
	}
	get isDisposed() {
		return this.isDisposedInternal;
	}
	throwIfDisposed() {
		if (this.isDisposed) throw new Error(`Tensor is disposed.`);
	}
	/**
	* Prints the `tf.Tensor`. See `tf.print` for details.
	*
	* @param verbose Whether to print verbose information about the tensor,
	*    including dtype and size.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	print(verbose = false) {
		return opHandler.print(this, verbose);
	}
	/**
	* Returns a copy of the tensor. See `tf.clone` for details.
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	clone() {
		this.throwIfDisposed();
		return opHandler.clone(this);
	}
	/**
	* Returns a human-readable description of the tensor. Useful for logging.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	toString(verbose = false) {
		const vals = this.dataSync();
		return tensorToString(vals, this.shape, this.dtype, verbose);
	}
	cast(dtype) {
		this.throwIfDisposed();
		return opHandler.cast(this, dtype);
	}
	variable(trainable = true, name, dtype) {
		this.throwIfDisposed();
		return trackerFn().makeVariable(this, trainable, name, dtype);
	}
};
Object.defineProperty(Tensor, Symbol.hasInstance, { value: (instance) => {
	return !!instance && instance.data != null && instance.dataSync != null && instance.throwIfDisposed != null;
} });
function getGlobalTensorClass() {
	return getGlobal("Tensor", () => {
		return Tensor;
	});
}
getGlobalTensorClass();
/**
* A mutable `tf.Tensor`, useful for persisting state, e.g. for training.
*
* @doc {heading: 'Tensors', subheading: 'Classes'}
*/
var Variable = class extends Tensor {
	constructor(initialValue, trainable, name, tensorId) {
		super(initialValue.shape, initialValue.dtype, initialValue.dataId, tensorId);
		this.trainable = trainable;
		this.name = name;
	}
	/**
	* Assign a new `tf.Tensor` to this variable. The new `tf.Tensor` must have
	* the same shape and dtype as the old `tf.Tensor`.
	*
	* @param newValue New tensor to be assigned to this variable.
	*
	* @doc {heading: 'Tensors', subheading: 'Classes'}
	*/
	assign(newValue) {
		if (newValue.dtype !== this.dtype) throw new Error(`dtype of the new value (${newValue.dtype}) and previous value (${this.dtype}) must match`);
		if (!arraysEqual(newValue.shape, this.shape)) throw new Error(`shape of the new value (${newValue.shape}) and previous value (${this.shape}) must match`);
		trackerFn().disposeTensor(this);
		this.dataId = newValue.dataId;
		trackerFn().incRef(this, null);
	}
	dispose() {
		trackerFn().disposeVariable(this);
		this.isDisposedInternal = true;
	}
};
Object.defineProperty(Variable, Symbol.hasInstance, { value: (instance) => {
	return instance instanceof Tensor && instance.assign != null && instance.assign instanceof Function;
} });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/types.js
/**
* @license
* Copyright 2017 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
var Rank;
(function(Rank$1) {
	Rank$1["R0"] = "R0";
	Rank$1["R1"] = "R1";
	Rank$1["R2"] = "R2";
	Rank$1["R3"] = "R3";
	Rank$1["R4"] = "R4";
	Rank$1["R5"] = "R5";
	Rank$1["R6"] = "R6";
})(Rank || (Rank = {}));
var UpcastInt32AndMap;
(function(UpcastInt32AndMap$1) {
	UpcastInt32AndMap$1["float32"] = "float32";
	UpcastInt32AndMap$1["int32"] = "int32";
	UpcastInt32AndMap$1["bool"] = "int32";
	UpcastInt32AndMap$1["complex64"] = "complex64";
})(UpcastInt32AndMap || (UpcastInt32AndMap = {}));
var UpcastBoolAndMap;
(function(UpcastBoolAndMap$1) {
	UpcastBoolAndMap$1["float32"] = "float32";
	UpcastBoolAndMap$1["int32"] = "int32";
	UpcastBoolAndMap$1["bool"] = "bool";
	UpcastBoolAndMap$1["complex64"] = "complex64";
})(UpcastBoolAndMap || (UpcastBoolAndMap = {}));
var UpcastFloat32AndMap;
(function(UpcastFloat32AndMap$1) {
	UpcastFloat32AndMap$1["float32"] = "float32";
	UpcastFloat32AndMap$1["int32"] = "float32";
	UpcastFloat32AndMap$1["bool"] = "float32";
	UpcastFloat32AndMap$1["complex64"] = "complex64";
})(UpcastFloat32AndMap || (UpcastFloat32AndMap = {}));
var UpcastComplex64AndMap;
(function(UpcastComplex64AndMap$1) {
	UpcastComplex64AndMap$1["float32"] = "complex64";
	UpcastComplex64AndMap$1["int32"] = "complex64";
	UpcastComplex64AndMap$1["bool"] = "complex64";
	UpcastComplex64AndMap$1["complex64"] = "complex64";
})(UpcastComplex64AndMap || (UpcastComplex64AndMap = {}));
var upcastTypeMap = {
	"float32": UpcastFloat32AndMap,
	"int32": UpcastInt32AndMap,
	"bool": UpcastBoolAndMap,
	"complex64": UpcastComplex64AndMap
};
function upcastType(typeA, typeB) {
	if (typeA === "string" || typeB === "string") {
		if (typeA === "string" && typeB === "string") return "string";
		throw new Error(`Can not upcast ${typeA} with ${typeB}`);
	}
	return upcastTypeMap[typeA][typeB];
}
/** Returns the output type after summation. */
function sumOutType(type) {
	return upcastType(type, "int32");
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/tensor_util.js
var tensor_util_exports = /* @__PURE__ */ __export({
	assertTypesMatch: () => assertTypesMatch,
	getTensorsInContainer: () => getTensorsInContainer,
	isTensorInList: () => isTensorInList,
	makeTypesMatch: () => makeTypesMatch
});
function makeTypesMatch(a, b) {
	if (a.dtype === b.dtype) return [a, b];
	const dtype = upcastType(a.dtype, b.dtype);
	return [a.cast(dtype), b.cast(dtype)];
}
function assertTypesMatch(a, b) {
	assert(a.dtype === b.dtype, () => `The dtypes of the first(${a.dtype}) and second(${b.dtype}) input must match`);
}
function isTensorInList(tensor$1, tensorList) {
	return tensorList.some((x) => x.id === tensor$1.id);
}
/**
* Extracts any `Tensor`s found within the provided object.
*
* @param container an object that may be a `Tensor` or may directly contain
*   `Tensor`s, such as a `Tensor[]` or `{key: Tensor, ...}`. In general it
*   is safe to pass any object here, except that `Promise`s are not
*   supported.
* @returns An array of `Tensors` found within the passed object. If the
*   argument is simply a `Tensor', a list containing that `Tensor` is
*   returned. If the object is not a `Tensor` or does not
*   contain `Tensors`, an empty list is returned.
*/
function getTensorsInContainer(result) {
	const list = [];
	walkTensorContainer(result, list, /* @__PURE__ */ new Set());
	return list;
}
function walkTensorContainer(container, list, seen) {
	if (container == null) return;
	if (container instanceof Tensor) {
		list.push(container);
		return;
	}
	if (!isIterable(container)) return;
	const iterable = container;
	for (const k in iterable) {
		const val = iterable[k];
		if (!seen.has(val)) {
			seen.add(val);
			walkTensorContainer(val, list, seen);
		}
	}
}
function isIterable(obj) {
	return Array.isArray(obj) || typeof obj === "object";
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/engine.js
function isRegisteredKernelInvocation(kernelInvocation) {
	return kernelInvocation.kernelName != null;
}
var EngineState = class {
	constructor() {
		this.registeredVariables = {};
		this.nextTapeNodeId = 0;
		this.numBytes = 0;
		this.numTensors = 0;
		this.numStringTensors = 0;
		this.numDataBuffers = 0;
		this.gradientDepth = 0;
		this.kernelDepth = 0;
		this.scopeStack = [];
		/**
		* Keeps track of the number of data moves during a kernel execution. We
		* maintain a stack since kernels can call other kernels, recursively.
		*/
		this.numDataMovesStack = [];
		this.nextScopeId = 0;
		this.tensorInfo = /* @__PURE__ */ new WeakMap();
		this.profiling = false;
		this.activeProfile = {
			newBytes: 0,
			newTensors: 0,
			peakBytes: 0,
			kernels: [],
			result: null,
			get kernelNames() {
				return Array.from(new Set(this.kernels.map((k) => k.name)));
			}
		};
	}
	dispose() {
		for (const variableName in this.registeredVariables) this.registeredVariables[variableName].dispose();
	}
};
var Engine = class Engine {
	constructor(ENV$2) {
		this.ENV = ENV$2;
		this.registry = {};
		this.registryFactory = {};
		this.pendingBackendInitId = 0;
		this.state = new EngineState();
	}
	async ready() {
		if (this.pendingBackendInit != null) return this.pendingBackendInit.then(() => {});
		if (this.backendInstance != null) return;
		const sortedBackends = this.getSortedBackends();
		for (let i = 0; i < sortedBackends.length; i++) {
			const backendName = sortedBackends[i];
			if (await this.initializeBackend(backendName).success) {
				await this.setBackend(backendName);
				return;
			}
		}
		throw new Error("Could not initialize any backends, all backend initializations failed.");
	}
	get backend() {
		if (this.pendingBackendInit != null) throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);
		if (this.backendInstance == null) {
			const { name, asyncInit } = this.initializeBackendsAndReturnBest();
			if (asyncInit) throw new Error(`The highest priority backend '${name}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);
			this.setBackend(name);
		}
		return this.backendInstance;
	}
	backendNames() {
		return Object.keys(this.registryFactory);
	}
	findBackend(backendName) {
		if (!(backendName in this.registry)) if (backendName in this.registryFactory) {
			const { asyncInit } = this.initializeBackend(backendName);
			if (asyncInit) return null;
		} else return null;
		return this.registry[backendName];
	}
	findBackendFactory(backendName) {
		if (!(backendName in this.registryFactory)) return null;
		return this.registryFactory[backendName].factory;
	}
	registerBackend(backendName, factory, priority = 1) {
		if (backendName in this.registryFactory) {
			warn(`${backendName} backend was already registered. Reusing existing backend factory.`);
			return false;
		}
		this.registryFactory[backendName] = {
			factory,
			priority
		};
		return true;
	}
	async setBackend(backendName) {
		if (this.registryFactory[backendName] == null) throw new Error(`Backend name '${backendName}' not found in registry`);
		this.backendName = backendName;
		if (this.registry[backendName] == null) {
			this.backendInstance = null;
			const { success, asyncInit } = this.initializeBackend(backendName);
			if (!(asyncInit ? await success : success)) return false;
		}
		this.backendInstance = this.registry[backendName];
		this.setupRegisteredKernels();
		this.profiler = new Profiler(this.backendInstance);
		return true;
	}
	setupRegisteredKernels() {
		getKernelsForBackend(this.backendName).forEach((kernel) => {
			if (kernel.setupFunc != null) kernel.setupFunc(this.backendInstance);
		});
	}
	disposeRegisteredKernels(backendName) {
		getKernelsForBackend(backendName).forEach((kernel) => {
			if (kernel.disposeFunc != null) kernel.disposeFunc(this.registry[backendName]);
		});
	}
	/**
	* Initializes a backend by looking up the backend name in the factory
	* registry and calling the factory method. Returns a boolean representing
	* whether the initialization of the backend suceeded. Throws an error if
	* there is no backend in the factory registry.
	*/
	initializeBackend(backendName) {
		const registryFactoryEntry = this.registryFactory[backendName];
		if (registryFactoryEntry == null) throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);
		try {
			const backend$1 = registryFactoryEntry.factory();
			if (backend$1 && !(backend$1 instanceof KernelBackend) && typeof backend$1.then === "function") {
				const promiseId = ++this.pendingBackendInitId;
				const success = backend$1.then((backendInstance) => {
					if (promiseId < this.pendingBackendInitId) return false;
					this.registry[backendName] = backendInstance;
					this.pendingBackendInit = null;
					return true;
				}).catch((err) => {
					if (promiseId < this.pendingBackendInitId) return false;
					this.pendingBackendInit = null;
					warn(`Initialization of backend ${backendName} failed`);
					warn(err.stack || err.message);
					return false;
				});
				this.pendingBackendInit = success;
				return {
					success,
					asyncInit: true
				};
			} else {
				this.registry[backendName] = backend$1;
				return {
					success: true,
					asyncInit: false
				};
			}
		} catch (err) {
			warn(`Initialization of backend ${backendName} failed`);
			warn(err.stack || err.message);
			return {
				success: false,
				asyncInit: false
			};
		}
	}
	removeBackend(backendName) {
		if (!(backendName in this.registryFactory)) throw new Error(`${backendName} backend not found in registry`);
		if (this.backendName === backendName && this.pendingBackendInit != null) this.pendingBackendInitId++;
		if (backendName in this.registry) {
			this.disposeRegisteredKernels(backendName);
			this.registry[backendName].dispose();
			delete this.registry[backendName];
		}
		delete this.registryFactory[backendName];
		if (this.backendName === backendName) {
			this.pendingBackendInit = null;
			this.backendName = null;
			this.backendInstance = null;
		}
	}
	getSortedBackends() {
		if (Object.keys(this.registryFactory).length === 0) throw new Error("No backend found in registry.");
		return Object.keys(this.registryFactory).sort((a, b) => {
			return this.registryFactory[b].priority - this.registryFactory[a].priority;
		});
	}
	initializeBackendsAndReturnBest() {
		const sortedBackends = this.getSortedBackends();
		for (let i = 0; i < sortedBackends.length; i++) {
			const backendName = sortedBackends[i];
			const { success, asyncInit } = this.initializeBackend(backendName);
			if (asyncInit || success) return {
				name: backendName,
				asyncInit
			};
		}
		throw new Error("Could not initialize any backends, all backend initializations failed.");
	}
	moveData(backend$1, dataId) {
		const info = this.state.tensorInfo.get(dataId);
		const srcBackend = info.backend;
		const values = this.readSync(dataId);
		const refCount = srcBackend.refCount(dataId);
		srcBackend.disposeData(dataId, true);
		info.backend = backend$1;
		backend$1.move(dataId, values, info.shape, info.dtype, refCount);
		if (this.shouldCheckForMemLeaks()) this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;
	}
	tidy(nameOrFn, fn) {
		let name = null;
		if (fn == null) {
			if (typeof nameOrFn !== "function") throw new Error("Please provide a function to tidy()");
			fn = nameOrFn;
		} else {
			if (typeof nameOrFn !== "string" && !(nameOrFn instanceof String)) throw new Error("When calling with two arguments, the first argument to tidy() must be a string");
			if (typeof fn !== "function") throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");
			name = nameOrFn;
		}
		let result;
		return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {
			result = fn();
			if (result instanceof Promise) console.error("Cannot return a Promise inside of tidy.");
			return result;
		});
	}
	scopedRun(start, end, f) {
		start();
		try {
			const res = f();
			end();
			return res;
		} catch (ex) {
			end();
			throw ex;
		}
	}
	nextTensorId() {
		return Engine.nextTensorId++;
	}
	nextVariableId() {
		return Engine.nextVariableId++;
	}
	/**
	* This method is called instead of the public-facing tensor.clone() when
	* saving a tensor for backwards pass. It makes sure to add the clone
	* operation to the tape regardless of being called inside a kernel
	* execution.
	*/
	clone(x) {
		const y = ENGINE.runKernel(Identity, { x });
		const inputs = { x };
		const grad$1 = (dy) => ({ x: () => {
			const dtype = "float32";
			const gradInputs = { x: dy };
			const attrs = { dtype };
			return ENGINE.runKernel(Cast, gradInputs, attrs);
		} });
		this.addTapeNode(this.state.activeScope.name, inputs, [y], grad$1, [], {});
		return y;
	}
	/**
	* Execute a kernel with the given name and return the output tensor.
	*
	* @param kernelName The name of the kernel to execute.
	* @param inputs A map of input names to tensors.
	* @param attrs A map of attribute names to their values. An attribute is a
	*     primitive (non-tensor) input to the kernel.
	* @param inputsToSave A list of tensors, inputs to save for the backprop
	*     computation.
	* @param outputsToSave A list of booleans, specifying which output to save
	*     for the backprop computation. These are booleans since the output
	* tensors are not visible to the user.
	*/
	runKernel(kernelName, inputs, attrs) {
		if (this.backendName == null) this.backend;
		if (!(getKernel(kernelName, this.backendName) != null)) throw new Error(`Kernel '${kernelName}' not registered for backend '${this.backendName}'`);
		return this.runKernelFunc({
			kernelName,
			inputs,
			attrs
		});
	}
	shouldCheckForMemLeaks() {
		return this.ENV.getBool("IS_TEST");
	}
	checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {
		const numDataIdsAfter = this.backend.numDataIds();
		let numOutputDataIds = 0;
		outInfos.forEach((info) => {
			numOutputDataIds += info.dtype === "complex64" ? 3 : 1;
		});
		const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];
		const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;
		if (dataIdsLeaked > 0) throw new Error(`Backend '${this.backendName}' has an internal memory leak (${dataIdsLeaked} data ids) after running '${kernelName}'`);
	}
	/**
	* Internal helper method to execute a kernel Func
	*
	* Use `runKernel` to execute kernels from outside of engine.
	*/
	runKernelFunc(kernelParams) {
		let outputs;
		let saved = [];
		const isTapeOn = this.isTapeOn();
		const startingBytecount = this.state.numBytes;
		const startingNumTensors = this.state.numTensors;
		if (this.shouldCheckForMemLeaks()) this.state.numDataMovesStack.push(0);
		let kernelFunc;
		if (this.backendName == null) this.backend;
		let out;
		const kernelOrScopeName = isRegisteredKernelInvocation(kernelParams) ? kernelParams.kernelName : this.state.activeScope != null ? this.state.activeScope.name : "";
		if (isRegisteredKernelInvocation(kernelParams)) {
			const { kernelName, inputs: inputs$1, attrs: attrs$1 } = kernelParams;
			if (this.backendName == null) this.backend;
			const kernel = getKernel(kernelName, this.backendName);
			assert(kernel != null, () => `Cannot find registered kernel '${kernelName}' for backend '${this.backendName}'`);
			kernelFunc = () => {
				const numDataIdsBefore = this.backend.numDataIds();
				out = kernel.kernelFunc({
					inputs: inputs$1,
					attrs: attrs$1,
					backend: this.backend
				});
				const outInfos = Array.isArray(out) ? out : [out];
				if (this.shouldCheckForMemLeaks()) this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);
				const outTensors = outInfos.map((outInfo) => {
					if (outInfo.rank != null) return outInfo;
					const { dataId, shape, dtype } = outInfo;
					return this.makeTensorFromDataId(dataId, shape, dtype);
				});
				if (isTapeOn) {
					const tensorsToSave = this.getTensorsForGradient(kernelName, inputs$1, outTensors);
					saved = this.saveTensorsForBackwardMode(tensorsToSave);
				}
				return outTensors;
			};
		} else {
			const { forwardFunc } = kernelParams;
			const saveFunc = (tensors) => {
				if (!isTapeOn) return;
				saved = tensors.map((tensor$1) => this.keep(this.clone(tensor$1)));
			};
			kernelFunc = () => {
				const numDataIdsBefore = this.backend.numDataIds();
				out = this.tidy(() => forwardFunc(this.backend, saveFunc));
				const outs = Array.isArray(out) ? out : [out];
				if (this.shouldCheckForMemLeaks()) this.checkKernelForMemLeak(kernelOrScopeName, numDataIdsBefore, outs);
				return outs;
			};
		}
		const { inputs, attrs } = kernelParams;
		const backwardsFunc = isRegisteredKernelInvocation(kernelParams) ? null : kernelParams.backwardsFunc;
		let kernelProfile;
		this.scopedRun(() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {
			if (!this.ENV.getBool("DEBUG") && !this.state.profiling) outputs = kernelFunc();
			else {
				kernelProfile = this.profiler.profileKernel(kernelOrScopeName, inputs, () => kernelFunc());
				if (this.ENV.getBool("DEBUG")) this.profiler.logKernelProfile(kernelProfile);
				outputs = kernelProfile.outputs;
			}
		});
		if (isTapeOn) this.addTapeNode(kernelOrScopeName, inputs, outputs, backwardsFunc, saved, attrs);
		if (this.state.profiling) this.state.activeProfile.kernels.push({
			name: kernelOrScopeName,
			bytesAdded: this.state.numBytes - startingBytecount,
			totalBytesSnapshot: this.state.numBytes,
			tensorsAdded: this.state.numTensors - startingNumTensors,
			totalTensorsSnapshot: this.state.numTensors,
			inputShapes: Object.keys(inputs).map((key) => inputs[key] != null ? inputs[key].shape : null),
			outputShapes: outputs.map((item) => item.shape),
			kernelTimeMs: kernelProfile.timeMs,
			extraInfo: kernelProfile.extraInfo
		});
		return Array.isArray(out) ? outputs : outputs[0];
	}
	/**
	* Saves tensors used in forward mode for use in backward mode.
	*
	* @param tensors the list of tensors to save.
	*/
	saveTensorsForBackwardMode(tensors) {
		return tensors.map((tensor$1) => this.keep(this.clone(tensor$1)));
	}
	/**
	* Returns a list of tensors to save for a given gradient calculation.
	*
	* @param kernelName name of kernel to look up gradient for.
	* @param inputs a map of input tensors.
	* @param outputs an array of output tensors from forward mode of kernel.
	*/
	getTensorsForGradient(kernelName, inputs, outputs) {
		const gradConfig = getGradient(kernelName);
		if (gradConfig != null) {
			const inputsToSave = gradConfig.inputsToSave || [];
			const outputsToSave = gradConfig.outputsToSave || [];
			let inputTensorsToSave;
			if (gradConfig.saveAllInputs) {
				assert(Array.isArray(inputs), () => "saveAllInputs is true, expected inputs to be an array.");
				inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);
			} else inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);
			const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);
			return inputTensorsToSave.concat(outputTensorsToSave);
		}
		return [];
	}
	/**
	* Internal method used by public APIs for tensor creation. Makes a new
	* tensor with the provided shape, dtype and values. It always
	* creates a new data id and writes the values to the underlying backend.
	*/
	makeTensor(values, shape, dtype, backend$1) {
		if (values == null) throw new Error("Values passed to engine.makeTensor() are null");
		dtype = dtype || "float32";
		backend$1 = backend$1 || this.backend;
		let backendVals = values;
		if (dtype === "string" && isString(values[0])) backendVals = values.map((d) => encodeString(d));
		const dataId = backend$1.write(backendVals, shape, dtype);
		const t = new Tensor(shape, dtype, dataId, this.nextTensorId());
		this.trackTensor(t, backend$1);
		if (dtype === "string") {
			const info = this.state.tensorInfo.get(dataId);
			const newBytes = bytesFromStringArray(backendVals);
			this.state.numBytes += newBytes - info.bytes;
			info.bytes = newBytes;
		}
		return t;
	}
	/**
	* Internal method used by backends. Makes a new tensor
	* that is a wrapper around an existing data id. It doesn't create
	* a new data id, only increments the ref count used in memory tracking.
	*/
	makeTensorFromDataId(dataId, shape, dtype, backend$1) {
		dtype = dtype || "float32";
		const t = new Tensor(shape, dtype, dataId, this.nextTensorId());
		this.trackTensor(t, backend$1);
		return t;
	}
	makeVariable(initialValue, trainable = true, name, dtype) {
		name = name || this.nextVariableId().toString();
		if (dtype != null && dtype !== initialValue.dtype) initialValue = initialValue.cast(dtype);
		const v = new Variable(initialValue, trainable, name, this.nextTensorId());
		if (this.state.registeredVariables[v.name] != null) throw new Error(`Variable with name ${v.name} was already registered`);
		this.state.registeredVariables[v.name] = v;
		this.incRef(v, this.backend);
		return v;
	}
	trackTensor(a, backend$1) {
		this.state.numTensors++;
		if (a.dtype === "string") this.state.numStringTensors++;
		let bytes = 0;
		if (a.dtype !== "complex64" && a.dtype !== "string") bytes = a.size * bytesPerElement(a.dtype);
		this.state.numBytes += bytes;
		if (!this.state.tensorInfo.has(a.dataId)) {
			this.state.numDataBuffers++;
			this.state.tensorInfo.set(a.dataId, {
				backend: backend$1 || this.backend,
				dtype: a.dtype,
				shape: a.shape,
				bytes
			});
		}
		if (!(a instanceof Variable)) this.track(a);
	}
	incRef(a, backend$1) {
		this.trackTensor(a, backend$1);
		this.backend.incRef(a.dataId);
	}
	removeDataId(dataId, backend$1) {
		if (this.state.tensorInfo.has(dataId) && this.state.tensorInfo.get(dataId).backend === backend$1) {
			this.state.tensorInfo.delete(dataId);
			this.state.numDataBuffers--;
		}
	}
	disposeTensor(a) {
		if (!this.state.tensorInfo.has(a.dataId)) return;
		const info = this.state.tensorInfo.get(a.dataId);
		this.state.numTensors--;
		if (a.dtype === "string") {
			this.state.numStringTensors--;
			this.state.numBytes -= info.bytes;
		}
		if (a.dtype !== "complex64" && a.dtype !== "string") {
			const bytes = a.size * bytesPerElement(a.dtype);
			this.state.numBytes -= bytes;
		}
		if (info.backend.disposeData(a.dataId)) this.removeDataId(a.dataId, info.backend);
	}
	disposeVariables() {
		for (const varName in this.state.registeredVariables) {
			const v = this.state.registeredVariables[varName];
			this.disposeVariable(v);
		}
	}
	disposeVariable(v) {
		this.disposeTensor(v);
		if (this.state.registeredVariables[v.name] != null) delete this.state.registeredVariables[v.name];
	}
	memory() {
		const info = this.backend.memory();
		info.numTensors = this.state.numTensors;
		info.numDataBuffers = this.state.numDataBuffers;
		info.numBytes = this.state.numBytes;
		if (this.state.numStringTensors > 0) {
			info.unreliable = true;
			if (info.reasons == null) info.reasons = [];
			info.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)");
		}
		return info;
	}
	async profile(query) {
		this.state.profiling = true;
		const startBytes = this.state.numBytes;
		const startNumTensors = this.state.numTensors;
		this.state.activeProfile.kernels = [];
		this.state.activeProfile.result = await query();
		this.state.profiling = false;
		this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map((d) => d.totalBytesSnapshot));
		this.state.activeProfile.newBytes = this.state.numBytes - startBytes;
		this.state.activeProfile.newTensors = this.state.numTensors - startNumTensors;
		for (const kernel of this.state.activeProfile.kernels) {
			kernel.kernelTimeMs = await kernel.kernelTimeMs;
			kernel.extraInfo = await kernel.extraInfo;
		}
		return this.state.activeProfile;
	}
	isTapeOn() {
		return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;
	}
	addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {
		const tapeNode = {
			id: this.state.nextTapeNodeId++,
			kernelName,
			inputs,
			outputs,
			saved
		};
		const gradConfig = getGradient(kernelName);
		if (gradConfig != null) gradientsFunc = gradConfig.gradFunc;
		if (gradientsFunc != null) tapeNode.gradient = (dys) => {
			dys = dys.map((dy, i) => {
				if (dy == null) {
					const output = outputs[i];
					const vals = makeZerosTypedArray(output.size, output.dtype);
					return this.makeTensor(vals, output.shape, output.dtype);
				}
				return dy;
			});
			return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);
		};
		this.state.activeTape.push(tapeNode);
	}
	keep(result) {
		result.kept = true;
		return result;
	}
	startTape() {
		if (this.state.gradientDepth === 0) this.state.activeTape = [];
		this.state.gradientDepth++;
	}
	endTape() {
		this.state.gradientDepth--;
	}
	/**
	* Start a scope. Use this with endScope() to achieve the same functionality
	* as scope() without the need for a function closure.
	*/
	startScope(name) {
		const scopeInfo = {
			track: [],
			name: "unnamed scope",
			id: this.state.nextScopeId++
		};
		if (name) scopeInfo.name = name;
		this.state.scopeStack.push(scopeInfo);
		this.state.activeScope = scopeInfo;
	}
	/**
	* End a scope. Use this with startScope() to achieve the same functionality
	* as scope() without the need for a function closure.
	*/
	endScope(result) {
		const tensorsToTrackInParent = getTensorsInContainer(result);
		const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map((t) => t.id));
		for (let i = 0; i < this.state.activeScope.track.length; i++) {
			const tensor$1 = this.state.activeScope.track[i];
			if (!tensor$1.kept && !tensorsToTrackInParentSet.has(tensor$1.id)) tensor$1.dispose();
		}
		const oldScope = this.state.scopeStack.pop();
		this.state.activeScope = this.state.scopeStack.length === 0 ? null : this.state.scopeStack[this.state.scopeStack.length - 1];
		tensorsToTrackInParent.forEach((tensor$1) => {
			if (!tensor$1.kept && tensor$1.scopeId === oldScope.id) this.track(tensor$1);
		});
	}
	/**
	* Returns gradients of `f` with respect to each of the `xs`. The gradients
	* returned are of the same length as `xs`, but some might be null if `f`
	* was not a function of that `x`. It also takes optional dy to multiply the
	* gradient, which defaults to `1`.
	*/
	gradients(f, xs, dy, allowNoGradients = false) {
		assert(xs.length > 0, () => "gradients() received an empty list of xs.");
		if (dy != null && dy.dtype !== "float32") throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);
		const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy("forward", f));
		assert(y instanceof Tensor, () => "The result y returned by f() must be a tensor.");
		const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);
		if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");
		return this.tidy("backward", () => {
			const accumulatedGradientMap = {};
			accumulatedGradientMap[y.id] = dy == null ? ones$1(y.shape) : dy;
			backpropagateGradients(accumulatedGradientMap, filteredTape, (f$1) => this.tidy(f$1), add$1);
			const grads$1 = xs.map((x) => accumulatedGradientMap[x.id]);
			if (this.state.gradientDepth === 0) {
				this.state.activeTape.forEach((node) => {
					for (const tensor$1 of node.saved) tensor$1.dispose();
				});
				this.state.activeTape = null;
			}
			return {
				value: y,
				grads: grads$1
			};
		});
	}
	customGrad(f) {
		assert(isFunction(f), () => "The f passed in customGrad(f) must be a function.");
		return (...inputs) => {
			assert(inputs.every((t) => t instanceof Tensor), () => "The args passed in customGrad(f)(x1, x2,...) must all be tensors");
			let res;
			const inputMap = {};
			inputs.forEach((input, i) => {
				inputMap[i] = input;
			});
			const forwardFunc = (_, save) => {
				res = f(...[...inputs, save]);
				assert(res.value instanceof Tensor, () => "The function f passed in customGrad(f) must return an object where `obj.value` is a tensor");
				assert(isFunction(res.gradFunc), () => "The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.");
				return res.value;
			};
			const backwardsFunc = (dy, saved) => {
				const gradRes = res.gradFunc(dy, saved);
				const grads$1 = Array.isArray(gradRes) ? gradRes : [gradRes];
				assert(grads$1.length === inputs.length, () => "The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).");
				assert(grads$1.every((t) => t instanceof Tensor), () => "The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors.");
				const gradMap = {};
				grads$1.forEach((grad$1, i) => {
					gradMap[i] = () => grad$1;
				});
				return gradMap;
			};
			return this.runKernelFunc({
				forwardFunc,
				backwardsFunc,
				inputs: inputMap
			});
		};
	}
	readSync(dataId) {
		return this.state.tensorInfo.get(dataId).backend.readSync(dataId);
	}
	read(dataId) {
		return this.state.tensorInfo.get(dataId).backend.read(dataId);
	}
	async time(query) {
		const start = now();
		const timingInfo = await this.backend.time(query);
		timingInfo.wallMs = now() - start;
		return timingInfo;
	}
	/**
	* Tracks a Tensor in the current scope to be automatically cleaned up
	* when the current scope ends, and returns the value.
	*
	* @param result The Tensor to track in the current scope.
	*/
	track(result) {
		if (this.state.activeScope != null) {
			result.scopeId = this.state.activeScope.id;
			this.state.activeScope.track.push(result);
		}
		return result;
	}
	get registeredVariables() {
		return this.state.registeredVariables;
	}
	/**
	* Resets the engine state. Removes all backends but does not remove
	* registered backend factories.
	*/
	reset() {
		this.pendingBackendInitId++;
		this.state.dispose();
		this.ENV.reset();
		this.state = new EngineState();
		for (const backendName in this.registry) {
			this.disposeRegisteredKernels(backendName);
			this.registry[backendName].dispose();
			delete this.registry[backendName];
		}
		this.backendName = null;
		this.backendInstance = null;
		this.pendingBackendInit = null;
	}
};
Engine.nextTensorId = 0;
Engine.nextVariableId = 0;
function ones$1(shape) {
	const values = makeOnesTypedArray(sizeFromShape(shape), "float32");
	return ENGINE.makeTensor(values, shape, "float32");
}
function getOrMakeEngine() {
	const ns = getGlobalNamespace();
	if (ns._tfengine == null) {
		const environment = new Environment(ns);
		ns._tfengine = new Engine(environment);
	}
	setEnvironmentGlobal(ns._tfengine.ENV);
	setTensorTracker(() => ns._tfengine);
	return ns._tfengine;
}
const ENGINE = getOrMakeEngine();
/**
* A implementation of the add op for use within engine and tape.
*
* This allows us to avoid a circular dependency between add.ts and engine.
* It is exported to be available in tape tests.
*/
function add$1(a, b) {
	const inputs = {
		a,
		b
	};
	return ENGINE.runKernel(Add, inputs);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/device_util.js
var device_util_exports = /* @__PURE__ */ __export({
	isBrowser: () => isBrowser,
	isMobile: () => isMobile,
	mockIsMobile: () => mockIsMobile
});
/**
* @license
* Copyright 2017 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
function _isNavigatorDefined() {
	return typeof navigator !== "undefined" && navigator != null;
}
var isMobileMockValue;
function mockIsMobile(value) {
	isMobileMockValue = value;
}
function isMobile(nav) {
	if (isMobileMockValue !== void 0) return isMobileMockValue;
	if (nav || _isNavigatorDefined()) {
		if (!nav) nav = navigator;
		if (nav.product === "ReactNative") return true;
		const a = nav.userAgent || nav.vendor || (typeof window !== "undefined" ? window.opera : "");
		if (!a) {
			const navAny = nav;
			return navAny.userAgentData && navAny.userAgentData.mobile;
		}
		return /(android|bb\d+|meego).+mobile|avantgo|bada\/|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\/|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino/i.test(a) || /1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\/|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\/)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\/)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\/(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\/|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\/|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\/|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-/i.test(a.substr(0, 4));
	}
	return false;
}
function isBrowser() {
	return typeof window !== "undefined" && window.document != null || typeof WorkerGlobalScope !== "undefined";
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/flags.js
var ENV = env();
/**
* This file contains environment-related flag registrations.
*/
/** Whether to enable debug mode. */
ENV.registerFlag("DEBUG", () => false, (debugValue) => {
	if (debugValue) console.warn("Debugging mode is ON. The output of every math call will be downloaded to CPU and checked for NaNs. This significantly impacts performance.");
});
/** Whether we are in a browser (as versus, say, node.js) environment. */
ENV.registerFlag("IS_BROWSER", () => isBrowser());
/** Whether we are in a browser (as versus, say, node.js) environment. */
ENV.registerFlag("IS_NODE", () => typeof process !== "undefined" && typeof process.versions !== "undefined" && typeof process.versions.node !== "undefined");
/** Whether this browser is Chrome. */
ENV.registerFlag("IS_CHROME", () => typeof navigator !== "undefined" && navigator != null && navigator.userAgent != null && /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor));
/**
* True when the environment is "production" where we disable safety checks
* to gain performance.
*/
ENV.registerFlag("PROD", () => false);
/**
* Whether to do sanity checks when inferring a shape from user-provided
* values, used when creating a new tensor.
*/
ENV.registerFlag("TENSORLIKE_CHECK_SHAPE_CONSISTENCY", () => ENV.getBool("DEBUG"));
/** Whether deprecation warnings are enabled. */
ENV.registerFlag("DEPRECATION_WARNINGS_ENABLED", () => true);
/** True if running unit tests. */
ENV.registerFlag("IS_TEST", () => false);
/** Whether to check computation result for errors. */
ENV.registerFlag("CHECK_COMPUTATION_FOR_ERRORS", () => true);
/** Whether the backend needs to wrap input to imageBitmap. */
ENV.registerFlag("WRAP_TO_IMAGEBITMAP", () => false);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/tensor_util_env.js
function inferShape(val, dtype) {
	let firstElem = val;
	if (isTypedArray(val)) return dtype === "string" ? [] : [val.length];
	if (!Array.isArray(val)) return [];
	const shape = [];
	while (Array.isArray(firstElem) || isTypedArray(firstElem) && dtype !== "string") {
		shape.push(firstElem.length);
		firstElem = firstElem[0];
	}
	if (Array.isArray(val) && env().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")) deepAssertShapeConsistency(val, shape, []);
	return shape;
}
function deepAssertShapeConsistency(val, shape, indices) {
	indices = indices || [];
	if (!Array.isArray(val) && !isTypedArray(val)) {
		assert(shape.length === 0, () => `Element arr[${indices.join("][")}] is a primitive, but should be an array/TypedArray of ${shape[0]} elements`);
		return;
	}
	assert(shape.length > 0, () => `Element arr[${indices.join("][")}] should be a primitive, but is an array of ${val.length} elements`);
	assert(val.length === shape[0], () => `Element arr[${indices.join("][")}] should have ${shape[0]} elements, but has ${val.length} elements`);
	const subShape = shape.slice(1);
	for (let i = 0; i < val.length; ++i) deepAssertShapeConsistency(val[i], subShape, indices.concat(i));
}
function assertDtype(expectedDtype, actualDType, argName, functionName) {
	if (expectedDtype === "string_or_numeric") return;
	if (expectedDtype == null) throw new Error(`Expected dtype cannot be null.`);
	if (expectedDtype !== "numeric" && expectedDtype !== actualDType || expectedDtype === "numeric" && actualDType === "string") throw new Error(`Argument '${argName}' passed to '${functionName}' must be ${expectedDtype} tensor, but got ${actualDType} tensor`);
}
function convertToTensor(x, argName, functionName, parseAsDtype = "numeric") {
	if (x instanceof Tensor) {
		assertDtype(parseAsDtype, x.dtype, argName, functionName);
		return x;
	}
	let inferredDtype = inferDtype(x);
	if (inferredDtype !== "string" && [
		"bool",
		"int32",
		"float32"
	].indexOf(parseAsDtype) >= 0) inferredDtype = parseAsDtype;
	assertDtype(parseAsDtype, inferredDtype, argName, functionName);
	if (x == null || !isTypedArray(x) && !Array.isArray(x) && typeof x !== "number" && typeof x !== "boolean" && typeof x !== "string") {
		const type = x == null ? "null" : x.constructor.name;
		throw new Error(`Argument '${argName}' passed to '${functionName}' must be a Tensor or TensorLike, but got '${type}'`);
	}
	const inferredShape = inferShape(x, inferredDtype);
	if (!isTypedArray(x) && !Array.isArray(x)) x = [x];
	const values = inferredDtype !== "string" ? toTypedArray(x, inferredDtype) : flatten(x, [], true);
	return ENGINE.makeTensor(values, inferredShape, inferredDtype);
}
function convertToTensorArray(arg, argName, functionName, parseAsDtype = "numeric") {
	if (!Array.isArray(arg)) throw new Error(`Argument ${argName} passed to ${functionName} must be a \`Tensor[]\` or \`TensorLike[]\``);
	return arg.map((t, i) => convertToTensor(t, `${argName}[${i}]`, functionName, parseAsDtype));
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/operation.js
const OP_SCOPE_SUFFIX = "__op";
/**
* Used for wrapping functions that perform math operations on
* Tensors. The function will be wrapped in a named scope that cleans all
* memory usage after the function is done.
*/
function op(f) {
	const keys = Object.keys(f);
	if (keys.length !== 1) throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${keys.length} keys.`);
	let opName = keys[0];
	const fn = f[opName];
	if (opName.endsWith("_")) opName = opName.substring(0, opName.length - 1);
	opName = opName + OP_SCOPE_SUFFIX;
	const f2 = (...args) => {
		ENGINE.startScope(opName);
		try {
			const result = fn(...args);
			if (isPromise(result)) console.error("Cannot return a Promise inside of tidy.");
			ENGINE.endScope(result);
			return result;
		} catch (ex) {
			ENGINE.endScope(null);
			throw ex;
		}
	};
	Object.defineProperty(f2, "name", {
		value: opName,
		configurable: true
	});
	return f2;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/complex.js
/**
* Converts two real numbers to a complex number.
*
* Given a tensor `real` representing the real part of a complex number, and a
* tensor `imag` representing the imaginary part of a complex number, this
* operation returns complex numbers elementwise of the form [r0, i0, r1, i1],
* where r represents the real part and i represents the imag part.
*
* The input tensors real and imag must have the same shape.
*
* ```js
* const real = tf.tensor1d([2.25, 3.25]);
* const imag = tf.tensor1d([4.75, 5.75]);
* const complex = tf.complex(real, imag);
*
* complex.print();
* ```
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function complex_(real$1, imag$1) {
	const $real = convertToTensor(real$1, "real", "complex");
	const $imag = convertToTensor(imag$1, "imag", "complex");
	assertShapesMatch($real.shape, $imag.shape, `real and imag shapes, ${$real.shape} and ${$imag.shape}, must match in call to tf.complex().`);
	const inputs = {
		real: $real,
		imag: $imag
	};
	return ENGINE.runKernel(Complex, inputs);
}
const complex = op({ complex_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor_ops_util.js
/** This is shared code across all tensor creation methods. */
function makeTensor(values, shape, inferredShape, dtype) {
	if (dtype == null) dtype = inferDtype(values);
	if (dtype === "complex64") throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");
	if (!isTypedArray(values) && !Array.isArray(values) && typeof values !== "number" && typeof values !== "boolean" && typeof values !== "string") throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");
	if (shape != null) {
		assertNonNegativeIntegerDimensions(shape);
		const providedSize = sizeFromShape(shape);
		const inferredSize = sizeFromShape(inferredShape);
		assert(providedSize === inferredSize, () => `Based on the provided shape, [${shape}], the tensor should have ${providedSize} values but has ${inferredSize}`);
		for (let i = 0; i < inferredShape.length; ++i) {
			const inferred = inferredShape[i];
			const flatDimsDontMatch = i === inferredShape.length - 1 ? inferred !== sizeFromShape(shape.slice(i)) : true;
			assert(inferredShape[i] === shape[i] || !flatDimsDontMatch, () => `Error creating a new Tensor. Inferred shape (${inferredShape}) does not match the provided shape (${shape}). `);
		}
	}
	if (!isTypedArray(values) && !Array.isArray(values)) values = [values];
	shape = shape || inferredShape;
	values = dtype !== "string" ? toTypedArray(values, dtype) : flatten(values, [], true);
	return ENGINE.makeTensor(values, shape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor.js
/**
* Creates a `tf.Tensor` with the provided values, shape and dtype.
*
* ```js
* // Pass an array of values to create a vector.
* tf.tensor([1, 2, 3, 4]).print();
* ```
*
* ```js
* // Pass a nested array of values to make a matrix or a higher
* // dimensional tensor.
* tf.tensor([[1, 2], [3, 4]]).print();
* ```
*
* ```js
* // Pass a flat array and specify a shape yourself.
* tf.tensor([1, 2, 3, 4], [2, 2]).print();
* ```
*
* @param values The values of the tensor. Can be nested array of numbers,
*     or a flat array, or a `TypedArray`. If the values are strings,
*     they will be encoded as utf-8 and kept as `Uint8Array[]`.
* @param shape The shape of the tensor. Optional. If not provided,
*   it is inferred from `values`.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function tensor(values, shape, dtype) {
	const inferredShape = inferShape(values, dtype);
	return makeTensor(values, shape, inferredShape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/types.js
/**
* @license
* Copyright 2018 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
* A map from Tensor dtype to number of bytes per element of the Tensor.
*/
const DTYPE_VALUE_SIZE_MAP = {
	"float32": 4,
	"float16": 2,
	"int32": 4,
	"uint16": 2,
	"uint8": 1,
	"bool": 1,
	"complex64": 8
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/io_utils.js
/** Number of bytes reserved for the length of the string. (32bit integer). */
var NUM_BYTES_STRING_LENGTH = 4;
/**
* Encode a map from names to weight values as an ArrayBuffer, along with an
* `Array` of `WeightsManifestEntry` as specification of the encoded weights.
*
* This function does not perform sharding.
*
* This function is the reverse of `decodeWeights`.
*
* @param tensors A map ("dict") from names to tensors.
* @param group Group to which the weights belong (optional).
* @returns A `Promise` of
*   - A flat `ArrayBuffer` with all the binary values of the `Tensor`s
*     concatenated.
*   - An `Array` of `WeightManifestEntry`s, carrying information including
*     tensor names, `dtype`s and shapes.
* @throws Error: on unsupported tensor `dtype`.
*/
async function encodeWeights(tensors, group) {
	const specs = [];
	const dataPromises = [];
	const names = Array.isArray(tensors) ? tensors.map((tensor$1) => tensor$1.name) : Object.keys(tensors);
	for (let i = 0; i < names.length; ++i) {
		const name = names[i];
		const t = Array.isArray(tensors) ? tensors[i].tensor : tensors[name];
		if (t.dtype !== "float32" && t.dtype !== "int32" && t.dtype !== "bool" && t.dtype !== "string" && t.dtype !== "complex64") throw new Error(`Unsupported dtype in weight '${name}': ${t.dtype}`);
		const spec = {
			name,
			shape: t.shape,
			dtype: t.dtype
		};
		if (t.dtype === "string") {
			const utf8bytes = new Promise(async (resolve) => {
				const vals = await t.bytes();
				const totalNumBytes = vals.reduce((p, c) => p + c.length, 0) + NUM_BYTES_STRING_LENGTH * vals.length;
				const bytes = new Uint8Array(totalNumBytes);
				let offset = 0;
				for (let i$1 = 0; i$1 < vals.length; i$1++) {
					const val = vals[i$1];
					const bytesOfLength = new Uint8Array(new Uint32Array([val.length]).buffer);
					bytes.set(bytesOfLength, offset);
					offset += NUM_BYTES_STRING_LENGTH;
					bytes.set(val, offset);
					offset += val.length;
				}
				resolve(bytes);
			});
			dataPromises.push(utf8bytes);
		} else dataPromises.push(t.data());
		if (group != null) spec.group = group;
		specs.push(spec);
	}
	const tensorValues = await Promise.all(dataPromises);
	return {
		data: concatenateTypedArrays(tensorValues),
		specs
	};
}
/**
* Decode flat ArrayBuffer as weights.
*
* This function does not handle sharding.
*
* This function is the reverse of `encodeWeights`.
*
* @param buffer A flat ArrayBuffer carrying the binary values of the tensors
*   concatenated in the order specified in `specs`.
* @param specs Specifications of the names, dtypes and shapes of the tensors
*   whose value are encoded by `buffer`.
* @return A map from tensor name to tensor value, with the names corresponding
*   to names in `specs`.
* @throws Error, if any of the tensors has unsupported dtype.
*/
function decodeWeights(buffer$1, specs) {
	const out = {};
	let float16Decode;
	let offset = 0;
	for (const spec of specs) {
		const name = spec.name;
		const dtype = spec.dtype;
		const shape = spec.shape;
		const size = sizeFromShape(shape);
		let values;
		if ("quantization" in spec) {
			const quantization = spec.quantization;
			if (quantization.dtype === "uint8" || quantization.dtype === "uint16") {
				if (!("min" in quantization && "scale" in quantization)) throw new Error(`Weight ${spec.name} with quantization ${quantization.dtype} doesn't have corresponding metadata min and scale.`);
			} else if (quantization.dtype === "float16") {
				if (dtype !== "float32") throw new Error(`Weight ${spec.name} is quantized with ${quantization.dtype} which only supports weights of type float32 not ${dtype}.`);
			} else throw new Error(`Weight ${spec.name} has unknown quantization dtype ${quantization.dtype}. Supported quantization dtypes are: 'uint8', 'uint16', and 'float16'.`);
			const quantizationSizeFactor = DTYPE_VALUE_SIZE_MAP[quantization.dtype];
			const byteBuffer = buffer$1.slice(offset, offset + size * quantizationSizeFactor);
			const quantizedArray = quantization.dtype === "uint8" ? new Uint8Array(byteBuffer) : new Uint16Array(byteBuffer);
			if (dtype === "float32") if (quantization.dtype === "uint8" || quantization.dtype === "uint16") {
				values = new Float32Array(quantizedArray.length);
				for (let i = 0; i < quantizedArray.length; i++) values[i] = quantizedArray[i] * quantization.scale + quantization.min;
			} else if (quantization.dtype === "float16") {
				if (float16Decode === void 0) float16Decode = getFloat16Decoder();
				values = float16Decode(quantizedArray);
			} else throw new Error(`Unsupported quantization type ${quantization.dtype} for weight type float32.`);
			else if (dtype === "int32") {
				if (quantization.dtype !== "uint8" && quantization.dtype !== "uint16") throw new Error(`Unsupported quantization type ${quantization.dtype} for weight type int32.`);
				values = new Int32Array(quantizedArray.length);
				for (let i = 0; i < quantizedArray.length; i++) {
					const v = quantizedArray[i];
					values[i] = Math.round(v * quantization.scale + quantization.min);
				}
			} else throw new Error(`Unsupported dtype in weight '${name}': ${dtype}`);
			offset += size * quantizationSizeFactor;
		} else if (dtype === "string") {
			const size$1 = sizeFromShape(spec.shape);
			values = [];
			for (let i = 0; i < size$1; i++) {
				const byteLength = new Uint32Array(buffer$1.slice(offset, offset + NUM_BYTES_STRING_LENGTH))[0];
				offset += NUM_BYTES_STRING_LENGTH;
				const bytes = new Uint8Array(buffer$1.slice(offset, offset + byteLength));
				values.push(bytes);
				offset += byteLength;
			}
		} else {
			const dtypeFactor = DTYPE_VALUE_SIZE_MAP[dtype];
			const byteBuffer = buffer$1.slice(offset, offset + size * dtypeFactor);
			if (dtype === "float32") values = new Float32Array(byteBuffer);
			else if (dtype === "int32") values = new Int32Array(byteBuffer);
			else if (dtype === "bool") values = new Uint8Array(byteBuffer);
			else if (dtype === "complex64") {
				values = new Float32Array(byteBuffer);
				const real$1 = new Float32Array(values.length / 2);
				const image$1 = new Float32Array(values.length / 2);
				for (let i = 0; i < real$1.length; i++) {
					real$1[i] = values[i * 2];
					image$1[i] = values[i * 2 + 1];
				}
				const realTensor = tensor(real$1, shape, "float32");
				const imageTensor = tensor(image$1, shape, "float32");
				out[name] = complex(realTensor, imageTensor);
				realTensor.dispose();
				imageTensor.dispose();
			} else throw new Error(`Unsupported dtype in weight '${name}': ${dtype}`);
			offset += size * dtypeFactor;
		}
		if (dtype !== "complex64") out[name] = tensor(values, shape, dtype);
	}
	return out;
}
/**
* Concatenate TypedArrays into an ArrayBuffer.
*/
function concatenateTypedArrays(xs) {
	if (xs === null) throw new Error(`Invalid input value: ${JSON.stringify(xs)}`);
	let totalByteLength = 0;
	const normalizedXs = [];
	xs.forEach((x) => {
		totalByteLength += x.byteLength;
		normalizedXs.push(x.byteLength === x.buffer.byteLength ? x : new x.constructor(x));
		if (!(x instanceof Float32Array || x instanceof Int32Array || x instanceof Uint8Array)) throw new Error(`Unsupported TypedArray subtype: ${x.constructor.name}`);
	});
	const y = new Uint8Array(totalByteLength);
	let offset = 0;
	normalizedXs.forEach((x) => {
		y.set(new Uint8Array(x.buffer), offset);
		offset += x.byteLength;
	});
	return y.buffer;
}
var useNodeBuffer = typeof Buffer !== "undefined" && (typeof Blob === "undefined" || typeof atob === "undefined" || typeof btoa === "undefined");
/**
* Calculate the byte length of a JavaScript string.
*
* Note that a JavaScript string can contain wide characters, therefore the
* length of the string is not necessarily equal to the byte length.
*
* @param str Input string.
* @returns Byte length.
*/
function stringByteLength(str) {
	if (useNodeBuffer) return Buffer.byteLength(str);
	return new Blob([str]).size;
}
/**
* Encode an ArrayBuffer as a base64 encoded string.
*
* @param buffer `ArrayBuffer` to be converted.
* @returns A string that base64-encodes `buffer`.
*/
function arrayBufferToBase64String(buffer$1) {
	if (useNodeBuffer) return Buffer.from(buffer$1).toString("base64");
	const buf = new Uint8Array(buffer$1);
	let s = "";
	for (let i = 0, l = buf.length; i < l; i++) s += String.fromCharCode(buf[i]);
	return btoa(s);
}
/**
* Decode a base64 string as an ArrayBuffer.
*
* @param str Base64 string.
* @returns Decoded `ArrayBuffer`.
*/
function base64StringToArrayBuffer(str) {
	if (useNodeBuffer) {
		const buf = Buffer.from(str, "base64");
		return buf.buffer.slice(buf.byteOffset, buf.byteOffset + buf.byteLength);
	}
	const s = atob(str);
	const buffer$1 = new Uint8Array(s.length);
	for (let i = 0; i < s.length; ++i) buffer$1.set([s.charCodeAt(i)], i);
	return buffer$1.buffer;
}
/**
* Concatenate a number of ArrayBuffers into one.
*
* @param buffers A number of array buffers to concatenate.
* @returns Result of concatenating `buffers` in order.
*/
function concatenateArrayBuffers(buffers) {
	if (buffers.length === 1) return buffers[0];
	let totalByteLength = 0;
	buffers.forEach((buffer$1) => {
		totalByteLength += buffer$1.byteLength;
	});
	const temp = new Uint8Array(totalByteLength);
	let offset = 0;
	buffers.forEach((buffer$1) => {
		temp.set(new Uint8Array(buffer$1), offset);
		offset += buffer$1.byteLength;
	});
	return temp.buffer;
}
/**
* Get the basename of a path.
*
* Behaves in a way analogous to Linux's basename command.
*
* @param path
*/
function basename(path) {
	const SEPARATOR = "/";
	path = path.trim();
	while (path.endsWith(SEPARATOR)) path = path.slice(0, path.length - 1);
	const items = path.split(SEPARATOR);
	return items[items.length - 1];
}
/**
* Create `ModelJSON` from `ModelArtifacts`.
*
* @param artifacts Model artifacts, describing the model and its weights.
* @param manifest Weight manifest, describing where the weights of the
*     `ModelArtifacts` are stored, and some metadata about them.
* @returns Object representing the `model.json` file describing the model
*     artifacts and weights
*/
function getModelJSONForModelArtifacts(artifacts, manifest) {
	const result = {
		modelTopology: artifacts.modelTopology,
		format: artifacts.format,
		generatedBy: artifacts.generatedBy,
		convertedBy: artifacts.convertedBy,
		weightsManifest: manifest
	};
	if (artifacts.signature != null) result.signature = artifacts.signature;
	if (artifacts.userDefinedMetadata != null) result.userDefinedMetadata = artifacts.userDefinedMetadata;
	if (artifacts.modelInitializer != null) result.modelInitializer = artifacts.modelInitializer;
	if (artifacts.trainingConfig != null) result.trainingConfig = artifacts.trainingConfig;
	return result;
}
/**
* Create `ModelArtifacts` from a JSON file.
*
* @param modelJSON Object containing the parsed JSON of `model.json`
* @param loadWeights Function that takes the JSON file's weights manifest,
*     reads weights from the listed path(s), and returns a Promise of the
*     weight manifest entries along with the weights data.
* @returns A Promise of the `ModelArtifacts`, as described by the JSON file.
*/
async function getModelArtifactsForJSON(modelJSON, loadWeights$1) {
	const modelArtifacts = {
		modelTopology: modelJSON.modelTopology,
		format: modelJSON.format,
		generatedBy: modelJSON.generatedBy,
		convertedBy: modelJSON.convertedBy
	};
	if (modelJSON.trainingConfig != null) modelArtifacts.trainingConfig = modelJSON.trainingConfig;
	if (modelJSON.weightsManifest != null) {
		const [weightSpecs, weightData] = await loadWeights$1(modelJSON.weightsManifest);
		modelArtifacts.weightSpecs = weightSpecs;
		modelArtifacts.weightData = weightData;
	}
	if (modelJSON.signature != null) modelArtifacts.signature = modelJSON.signature;
	if (modelJSON.userDefinedMetadata != null) modelArtifacts.userDefinedMetadata = modelJSON.userDefinedMetadata;
	if (modelJSON.modelInitializer != null) modelArtifacts.modelInitializer = modelJSON.modelInitializer;
	return modelArtifacts;
}
/**
* Populate ModelArtifactsInfo fields for a model with JSON topology.
* @param modelArtifacts
* @returns A ModelArtifactsInfo object.
*/
function getModelArtifactsInfoForJSON(modelArtifacts) {
	if (modelArtifacts.modelTopology instanceof ArrayBuffer) throw new Error("Expected JSON model topology, received ArrayBuffer.");
	return {
		dateSaved: /* @__PURE__ */ new Date(),
		modelTopologyType: "JSON",
		modelTopologyBytes: modelArtifacts.modelTopology == null ? 0 : stringByteLength(JSON.stringify(modelArtifacts.modelTopology)),
		weightSpecsBytes: modelArtifacts.weightSpecs == null ? 0 : stringByteLength(JSON.stringify(modelArtifacts.weightSpecs)),
		weightDataBytes: modelArtifacts.weightData == null ? 0 : modelArtifacts.weightData.byteLength
	};
}
/**
* Computes mantisa table for casting Float16 to Float32
* See http://www.fox-toolkit.org/ftp/fasthalffloatconversion.pdf
*
* @returns Uint32Array, 2048 mantissa lookup values.
*/
function computeFloat16MantisaTable() {
	const convertMantissa = (i) => {
		let m = i << 13;
		let e = 0;
		while ((m & 8388608) === 0) {
			e -= 8388608;
			m <<= 1;
		}
		m &= -8388609;
		e += 947912704;
		return m | e;
	};
	const mantisaTable = new Uint32Array(2048);
	mantisaTable[0] = 0;
	for (let i = 1; i < 1024; i++) mantisaTable[i] = convertMantissa(i);
	for (let i = 1024; i < 2048; i++) mantisaTable[i] = 939524096 + (i - 1024 << 13);
	return mantisaTable;
}
/**
* Computes exponent table for casting Float16 to Float32
* See http://www.fox-toolkit.org/ftp/fasthalffloatconversion.pdf
*
* @returns Uint32Array, 64 exponent lookup values.
*/
function computeFloat16ExponentTable() {
	const exponentTable = new Uint32Array(64);
	exponentTable[0] = 0;
	exponentTable[31] = 1199570944;
	exponentTable[32] = 2147483648;
	exponentTable[63] = 3347054592;
	for (let i = 1; i < 31; i++) exponentTable[i] = i << 23;
	for (let i = 33; i < 63; i++) exponentTable[i] = 2147483648 + (i - 32 << 23);
	return exponentTable;
}
/**
* Computes offset table for casting Float16 to Float32
* See http://www.fox-toolkit.org/ftp/fasthalffloatconversion.pdf
*
* @returns Uint32Array, 6d offset values.
*/
function computeFloat16OffsetTable() {
	const offsetTable = new Uint32Array(64);
	for (let i = 0; i < 64; i++) offsetTable[i] = 1024;
	offsetTable[0] = offsetTable[32] = 0;
	return offsetTable;
}
/**
* Retrieve a Float16 decoder which will decode a ByteArray of Float16 values
* to a Float32Array.
*
* @returns Function (buffer: Uint16Array) => Float32Array which decodes
*          the Uint16Array of Float16 bytes to a Float32Array.
*/
function getFloat16Decoder() {
	const mantisaTable = computeFloat16MantisaTable();
	const exponentTable = computeFloat16ExponentTable();
	const offsetTable = computeFloat16OffsetTable();
	return (quantizedArray) => {
		const buffer$1 = /* @__PURE__ */ new ArrayBuffer(4 * quantizedArray.length);
		const bufferUint32View = new Uint32Array(buffer$1);
		for (let index = 0; index < quantizedArray.length; index++) {
			const float16Bits = quantizedArray[index];
			bufferUint32View[index] = mantisaTable[offsetTable[float16Bits >> 10] + (float16Bits & 1023)] + exponentTable[float16Bits >> 10];
		}
		return new Float32Array(buffer$1);
	};
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/router_registry.js
/**
* @license
* Copyright 2018 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
var IORouterRegistry = class IORouterRegistry {
	constructor() {
		this.saveRouters = [];
		this.loadRouters = [];
	}
	static getInstance() {
		if (IORouterRegistry.instance == null) IORouterRegistry.instance = new IORouterRegistry();
		return IORouterRegistry.instance;
	}
	/**
	* Register a save-handler router.
	*
	* @param saveRouter A function that maps a URL-like string onto an instance
	* of `IOHandler` with the `save` method defined or `null`.
	*/
	static registerSaveRouter(saveRouter) {
		IORouterRegistry.getInstance().saveRouters.push(saveRouter);
	}
	/**
	* Register a load-handler router.
	*
	* @param loadRouter A function that maps a URL-like string onto an instance
	* of `IOHandler` with the `load` method defined or `null`.
	*/
	static registerLoadRouter(loadRouter) {
		IORouterRegistry.getInstance().loadRouters.push(loadRouter);
	}
	/**
	* Look up IOHandler for saving, given a URL-like string.
	*
	* @param url
	* @returns If only one match is found, an instance of IOHandler with the
	* `save` method defined. If no match is found, `null`.
	* @throws Error, if more than one match is found.
	*/
	static getSaveHandlers(url) {
		return IORouterRegistry.getHandlers(url, "save");
	}
	/**
	* Look up IOHandler for loading, given a URL-like string.
	*
	* @param url
	* @param loadOptions Optional, custom load options.
	* @returns All valid handlers for `url`, given the currently registered
	*   handler routers.
	*/
	static getLoadHandlers(url, loadOptions) {
		return IORouterRegistry.getHandlers(url, "load", loadOptions);
	}
	static getHandlers(url, handlerType, loadOptions) {
		const validHandlers = [];
		(handlerType === "load" ? IORouterRegistry.getInstance().loadRouters : IORouterRegistry.getInstance().saveRouters).forEach((router) => {
			const handler = router(url, loadOptions);
			if (handler !== null) validHandlers.push(handler);
		});
		return validHandlers;
	}
};
const registerSaveRouter = (loudRouter) => IORouterRegistry.registerSaveRouter(loudRouter);
const registerLoadRouter = (loudRouter) => IORouterRegistry.registerLoadRouter(loudRouter);
const getSaveHandlers = (url) => IORouterRegistry.getSaveHandlers(url);
const getLoadHandlers = (url, loadOptions) => IORouterRegistry.getLoadHandlers(url, loadOptions);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/indexed_db.js
var DATABASE_NAME = "tensorflowjs";
var DATABASE_VERSION = 1;
var MODEL_STORE_NAME = "models_store";
var INFO_STORE_NAME = "model_info_store";
function getIndexedDBFactory() {
	if (!env().getBool("IS_BROWSER")) throw new Error("Failed to obtain IndexedDB factory because the current environmentis not a web browser.");
	const theWindow = typeof window === "undefined" ? self : window;
	const factory = theWindow.indexedDB || theWindow.mozIndexedDB || theWindow.webkitIndexedDB || theWindow.msIndexedDB || theWindow.shimIndexedDB;
	if (factory == null) throw new Error("The current browser does not appear to support IndexedDB.");
	return factory;
}
function setUpDatabase(openRequest) {
	const db = openRequest.result;
	db.createObjectStore(MODEL_STORE_NAME, { keyPath: "modelPath" });
	db.createObjectStore(INFO_STORE_NAME, { keyPath: "modelPath" });
}
/**
* IOHandler subclass: Browser IndexedDB.
*
* See the doc string of `browserIndexedDB` for more details.
*/
var BrowserIndexedDB = class {
	constructor(modelPath) {
		this.indexedDB = getIndexedDBFactory();
		if (modelPath == null || !modelPath) throw new Error("For IndexedDB, modelPath must not be null, undefined or empty.");
		this.modelPath = modelPath;
	}
	async save(modelArtifacts) {
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) throw new Error("BrowserLocalStorage.save() does not support saving model topology in binary formats yet.");
		return this.databaseAction(this.modelPath, modelArtifacts);
	}
	async load() {
		return this.databaseAction(this.modelPath);
	}
	/**
	* Perform database action to put model artifacts into or read model artifacts
	* from IndexedDB object store.
	*
	* Whether the action is put or get depends on whether `modelArtifacts` is
	* specified. If it is specified, the action will be put; otherwise the action
	* will be get.
	*
	* @param modelPath A unique string path for the model.
	* @param modelArtifacts If specified, it will be the model artifacts to be
	*   stored in IndexedDB.
	* @returns A `Promise` of `SaveResult`, if the action is put, or a `Promise`
	*   of `ModelArtifacts`, if the action is get.
	*/
	databaseAction(modelPath, modelArtifacts) {
		return new Promise((resolve, reject) => {
			const openRequest = this.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);
			openRequest.onupgradeneeded = () => setUpDatabase(openRequest);
			openRequest.onsuccess = () => {
				const db = openRequest.result;
				if (modelArtifacts == null) {
					const modelTx = db.transaction(MODEL_STORE_NAME, "readonly");
					const getRequest = modelTx.objectStore(MODEL_STORE_NAME).get(this.modelPath);
					getRequest.onsuccess = () => {
						if (getRequest.result == null) {
							db.close();
							return reject(/* @__PURE__ */ new Error(`Cannot find model with path '${this.modelPath}' in IndexedDB.`));
						} else resolve(getRequest.result.modelArtifacts);
					};
					getRequest.onerror = (error) => {
						db.close();
						return reject(getRequest.error);
					};
					modelTx.oncomplete = () => db.close();
				} else {
					const modelArtifactsInfo = getModelArtifactsInfoForJSON(modelArtifacts);
					const infoTx = db.transaction(INFO_STORE_NAME, "readwrite");
					let infoStore = infoTx.objectStore(INFO_STORE_NAME);
					const putInfoRequest = infoStore.put({
						modelPath: this.modelPath,
						modelArtifactsInfo
					});
					let modelTx;
					putInfoRequest.onsuccess = () => {
						modelTx = db.transaction(MODEL_STORE_NAME, "readwrite");
						const putModelRequest = modelTx.objectStore(MODEL_STORE_NAME).put({
							modelPath: this.modelPath,
							modelArtifacts,
							modelArtifactsInfo
						});
						putModelRequest.onsuccess = () => resolve({ modelArtifactsInfo });
						putModelRequest.onerror = (error) => {
							infoStore = infoTx.objectStore(INFO_STORE_NAME);
							const deleteInfoRequest = infoStore.delete(this.modelPath);
							deleteInfoRequest.onsuccess = () => {
								db.close();
								return reject(putModelRequest.error);
							};
							deleteInfoRequest.onerror = (error$1) => {
								db.close();
								return reject(putModelRequest.error);
							};
						};
					};
					putInfoRequest.onerror = (error) => {
						db.close();
						return reject(putInfoRequest.error);
					};
					infoTx.oncomplete = () => {
						if (modelTx == null) db.close();
						else modelTx.oncomplete = () => db.close();
					};
				}
			};
			openRequest.onerror = (error) => reject(openRequest.error);
		});
	}
};
BrowserIndexedDB.URL_SCHEME = "indexeddb://";
const indexedDBRouter = (url) => {
	if (!env().getBool("IS_BROWSER")) return null;
	else if (!Array.isArray(url) && url.startsWith(BrowserIndexedDB.URL_SCHEME)) return browserIndexedDB(url.slice(BrowserIndexedDB.URL_SCHEME.length));
	else return null;
};
IORouterRegistry.registerSaveRouter(indexedDBRouter);
IORouterRegistry.registerLoadRouter(indexedDBRouter);
/**
* Creates a browser IndexedDB IOHandler for saving and loading models.
*
* ```js
* const model = tf.sequential();
* model.add(
*     tf.layers.dense({units: 1, inputShape: [100], activation: 'sigmoid'}));
*
* const saveResult = await model.save('indexeddb://MyModel'));
* console.log(saveResult);
* ```
*
* @param modelPath A unique identifier for the model to be saved. Must be a
*   non-empty string.
* @returns An instance of `BrowserIndexedDB` (sublcass of `IOHandler`),
*   which can be used with, e.g., `tf.Model.save`.
*/
function browserIndexedDB(modelPath) {
	return new BrowserIndexedDB(modelPath);
}
function maybeStripScheme$1(key) {
	return key.startsWith(BrowserIndexedDB.URL_SCHEME) ? key.slice(BrowserIndexedDB.URL_SCHEME.length) : key;
}
var BrowserIndexedDBManager = class {
	constructor() {
		this.indexedDB = getIndexedDBFactory();
	}
	async listModels() {
		return new Promise((resolve, reject) => {
			const openRequest = this.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);
			openRequest.onupgradeneeded = () => setUpDatabase(openRequest);
			openRequest.onsuccess = () => {
				const db = openRequest.result;
				const tx = db.transaction(INFO_STORE_NAME, "readonly");
				const getAllInfoRequest = tx.objectStore(INFO_STORE_NAME).getAll();
				getAllInfoRequest.onsuccess = () => {
					const out = {};
					for (const item of getAllInfoRequest.result) out[item.modelPath] = item.modelArtifactsInfo;
					resolve(out);
				};
				getAllInfoRequest.onerror = (error) => {
					db.close();
					return reject(getAllInfoRequest.error);
				};
				tx.oncomplete = () => db.close();
			};
			openRequest.onerror = (error) => reject(openRequest.error);
		});
	}
	async removeModel(path) {
		path = maybeStripScheme$1(path);
		return new Promise((resolve, reject) => {
			const openRequest = this.indexedDB.open(DATABASE_NAME, DATABASE_VERSION);
			openRequest.onupgradeneeded = () => setUpDatabase(openRequest);
			openRequest.onsuccess = () => {
				const db = openRequest.result;
				const infoTx = db.transaction(INFO_STORE_NAME, "readwrite");
				const infoStore = infoTx.objectStore(INFO_STORE_NAME);
				const getInfoRequest = infoStore.get(path);
				let modelTx;
				getInfoRequest.onsuccess = () => {
					if (getInfoRequest.result == null) {
						db.close();
						return reject(/* @__PURE__ */ new Error(`Cannot find model with path '${path}' in IndexedDB.`));
					} else {
						const deleteInfoRequest = infoStore.delete(path);
						const deleteModelData = () => {
							modelTx = db.transaction(MODEL_STORE_NAME, "readwrite");
							const deleteModelRequest = modelTx.objectStore(MODEL_STORE_NAME).delete(path);
							deleteModelRequest.onsuccess = () => resolve(getInfoRequest.result.modelArtifactsInfo);
							deleteModelRequest.onerror = (error) => reject(getInfoRequest.error);
						};
						deleteInfoRequest.onsuccess = deleteModelData;
						deleteInfoRequest.onerror = (error) => {
							deleteModelData();
							db.close();
							return reject(getInfoRequest.error);
						};
					}
				};
				getInfoRequest.onerror = (error) => {
					db.close();
					return reject(getInfoRequest.error);
				};
				infoTx.oncomplete = () => {
					if (modelTx == null) db.close();
					else modelTx.oncomplete = () => db.close();
				};
			};
			openRequest.onerror = (error) => reject(openRequest.error);
		});
	}
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/local_storage.js
var PATH_SEPARATOR = "/";
var PATH_PREFIX = "tensorflowjs_models";
var INFO_SUFFIX = "info";
var MODEL_TOPOLOGY_SUFFIX = "model_topology";
var WEIGHT_SPECS_SUFFIX = "weight_specs";
var WEIGHT_DATA_SUFFIX = "weight_data";
var MODEL_METADATA_SUFFIX = "model_metadata";
function getModelKeys(path) {
	return {
		info: [
			PATH_PREFIX,
			path,
			INFO_SUFFIX
		].join(PATH_SEPARATOR),
		topology: [
			PATH_PREFIX,
			path,
			MODEL_TOPOLOGY_SUFFIX
		].join(PATH_SEPARATOR),
		weightSpecs: [
			PATH_PREFIX,
			path,
			WEIGHT_SPECS_SUFFIX
		].join(PATH_SEPARATOR),
		weightData: [
			PATH_PREFIX,
			path,
			WEIGHT_DATA_SUFFIX
		].join(PATH_SEPARATOR),
		modelMetadata: [
			PATH_PREFIX,
			path,
			MODEL_METADATA_SUFFIX
		].join(PATH_SEPARATOR)
	};
}
function removeItems(keys) {
	for (const key of Object.values(keys)) window.localStorage.removeItem(key);
}
/**
* Get model path from a local-storage key.
*
* E.g., 'tensorflowjs_models/my/model/1/info' --> 'my/model/1'
*
* @param key
*/
function getModelPathFromKey(key) {
	const items = key.split(PATH_SEPARATOR);
	if (items.length < 3) throw new Error(`Invalid key format: ${key}`);
	return items.slice(1, items.length - 1).join(PATH_SEPARATOR);
}
function maybeStripScheme(key) {
	return key.startsWith(BrowserLocalStorage.URL_SCHEME) ? key.slice(BrowserLocalStorage.URL_SCHEME.length) : key;
}
/**
* IOHandler subclass: Browser Local Storage.
*
* See the doc string to `browserLocalStorage` for more details.
*/
var BrowserLocalStorage = class {
	constructor(modelPath) {
		if (!env().getBool("IS_BROWSER") || typeof window === "undefined" || typeof window.localStorage === "undefined") throw new Error("The current environment does not support local storage.");
		this.LS = window.localStorage;
		if (modelPath == null || !modelPath) throw new Error("For local storage, modelPath must not be null, undefined or empty.");
		this.modelPath = modelPath;
		this.keys = getModelKeys(this.modelPath);
	}
	/**
	* Save model artifacts to browser local storage.
	*
	* See the documentation to `browserLocalStorage` for details on the saved
	* artifacts.
	*
	* @param modelArtifacts The model artifacts to be stored.
	* @returns An instance of SaveResult.
	*/
	async save(modelArtifacts) {
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) throw new Error("BrowserLocalStorage.save() does not support saving model topology in binary formats yet.");
		else {
			const topology = JSON.stringify(modelArtifacts.modelTopology);
			const weightSpecs = JSON.stringify(modelArtifacts.weightSpecs);
			const modelArtifactsInfo = getModelArtifactsInfoForJSON(modelArtifacts);
			try {
				this.LS.setItem(this.keys.info, JSON.stringify(modelArtifactsInfo));
				this.LS.setItem(this.keys.topology, topology);
				this.LS.setItem(this.keys.weightSpecs, weightSpecs);
				this.LS.setItem(this.keys.weightData, arrayBufferToBase64String(modelArtifacts.weightData));
				const metadata = {
					format: modelArtifacts.format,
					generatedBy: modelArtifacts.generatedBy,
					convertedBy: modelArtifacts.convertedBy,
					signature: modelArtifacts.signature != null ? modelArtifacts.signature : void 0,
					userDefinedMetadata: modelArtifacts.userDefinedMetadata != null ? modelArtifacts.userDefinedMetadata : void 0,
					modelInitializer: modelArtifacts.modelInitializer != null ? modelArtifacts.modelInitializer : void 0,
					trainingConfig: modelArtifacts.trainingConfig != null ? modelArtifacts.trainingConfig : void 0
				};
				this.LS.setItem(this.keys.modelMetadata, JSON.stringify(metadata));
				return { modelArtifactsInfo };
			} catch (err) {
				removeItems(this.keys);
				throw new Error(`Failed to save model '${this.modelPath}' to local storage: size quota being exceeded is a possible cause of this failure: modelTopologyBytes=${modelArtifactsInfo.modelTopologyBytes}, weightSpecsBytes=${modelArtifactsInfo.weightSpecsBytes}, weightDataBytes=${modelArtifactsInfo.weightDataBytes}.`);
			}
		}
	}
	/**
	* Load a model from local storage.
	*
	* See the documentation to `browserLocalStorage` for details on the saved
	* artifacts.
	*
	* @returns The loaded model (if loading succeeds).
	*/
	async load() {
		const info = JSON.parse(this.LS.getItem(this.keys.info));
		if (info == null) throw new Error(`In local storage, there is no model with name '${this.modelPath}'`);
		if (info.modelTopologyType !== "JSON") throw new Error("BrowserLocalStorage does not support loading non-JSON model topology yet.");
		const out = {};
		const topology = JSON.parse(this.LS.getItem(this.keys.topology));
		if (topology == null) throw new Error(`In local storage, the topology of model '${this.modelPath}' is missing.`);
		out.modelTopology = topology;
		const weightSpecs = JSON.parse(this.LS.getItem(this.keys.weightSpecs));
		if (weightSpecs == null) throw new Error(`In local storage, the weight specs of model '${this.modelPath}' are missing.`);
		out.weightSpecs = weightSpecs;
		const metadataString = this.LS.getItem(this.keys.modelMetadata);
		if (metadataString != null) {
			const metadata = JSON.parse(metadataString);
			out.format = metadata.format;
			out.generatedBy = metadata.generatedBy;
			out.convertedBy = metadata.convertedBy;
			if (metadata.signature != null) out.signature = metadata.signature;
			if (metadata.userDefinedMetadata != null) out.userDefinedMetadata = metadata.userDefinedMetadata;
			if (metadata.modelInitializer != null) out.modelInitializer = metadata.modelInitializer;
			if (metadata.trainingConfig != null) out.trainingConfig = metadata.trainingConfig;
		}
		const weightDataBase64 = this.LS.getItem(this.keys.weightData);
		if (weightDataBase64 == null) throw new Error(`In local storage, the binary weight values of model '${this.modelPath}' are missing.`);
		out.weightData = base64StringToArrayBuffer(weightDataBase64);
		return out;
	}
};
BrowserLocalStorage.URL_SCHEME = "localstorage://";
const localStorageRouter = (url) => {
	if (!env().getBool("IS_BROWSER")) return null;
	else if (!Array.isArray(url) && url.startsWith(BrowserLocalStorage.URL_SCHEME)) return browserLocalStorage(url.slice(BrowserLocalStorage.URL_SCHEME.length));
	else return null;
};
IORouterRegistry.registerSaveRouter(localStorageRouter);
IORouterRegistry.registerLoadRouter(localStorageRouter);
/**
* Factory function for local storage IOHandler.
*
* This `IOHandler` supports both `save` and `load`.
*
* For each model's saved artifacts, four items are saved to local storage.
*   - `${PATH_SEPARATOR}/${modelPath}/info`: Contains meta-info about the
*     model, such as date saved, type of the topology, size in bytes, etc.
*   - `${PATH_SEPARATOR}/${modelPath}/topology`: Model topology. For Keras-
*     style models, this is a stringized JSON.
*   - `${PATH_SEPARATOR}/${modelPath}/weight_specs`: Weight specs of the
*     model, can be used to decode the saved binary weight values (see
*     item below).
*   - `${PATH_SEPARATOR}/${modelPath}/weight_data`: Concatenated binary
*     weight values, stored as a base64-encoded string.
*
* Saving may throw an `Error` if the total size of the artifacts exceed the
* browser-specific quota.
*
* @param modelPath A unique identifier for the model to be saved. Must be a
*   non-empty string.
* @returns An instance of `IOHandler`, which can be used with, e.g.,
*   `tf.Model.save`.
*/
function browserLocalStorage(modelPath) {
	return new BrowserLocalStorage(modelPath);
}
var BrowserLocalStorageManager = class {
	constructor() {
		assert(env().getBool("IS_BROWSER"), () => "Current environment is not a web browser");
		assert(typeof window === "undefined" || typeof window.localStorage !== "undefined", () => "Current browser does not appear to support localStorage");
		this.LS = window.localStorage;
	}
	async listModels() {
		const out = {};
		const prefix = PATH_PREFIX + PATH_SEPARATOR;
		const suffix = PATH_SEPARATOR + INFO_SUFFIX;
		for (let i = 0; i < this.LS.length; ++i) {
			const key = this.LS.key(i);
			if (key.startsWith(prefix) && key.endsWith(suffix)) {
				const modelPath = getModelPathFromKey(key);
				out[modelPath] = JSON.parse(this.LS.getItem(key));
			}
		}
		return out;
	}
	async removeModel(path) {
		path = maybeStripScheme(path);
		const keys = getModelKeys(path);
		if (this.LS.getItem(keys.info) == null) throw new Error(`Cannot find model at path '${path}'`);
		const info = JSON.parse(this.LS.getItem(keys.info));
		removeItems(keys);
		return info;
	}
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/model_management.js
var URL_SCHEME_SUFFIX = "://";
var ModelStoreManagerRegistry = class ModelStoreManagerRegistry {
	constructor() {
		this.managers = {};
	}
	static getInstance() {
		if (ModelStoreManagerRegistry.instance == null) ModelStoreManagerRegistry.instance = new ModelStoreManagerRegistry();
		return ModelStoreManagerRegistry.instance;
	}
	/**
	* Register a save-handler router.
	*
	* @param saveRouter A function that maps a URL-like string onto an instance
	* of `IOHandler` with the `save` method defined or `null`.
	*/
	static registerManager(scheme, manager) {
		assert(scheme != null, () => "scheme must not be undefined or null.");
		if (scheme.endsWith(URL_SCHEME_SUFFIX)) scheme = scheme.slice(0, scheme.indexOf(URL_SCHEME_SUFFIX));
		assert(scheme.length > 0, () => "scheme must not be an empty string.");
		const registry = ModelStoreManagerRegistry.getInstance();
		assert(registry.managers[scheme] == null, () => `A model store manager is already registered for scheme '${scheme}'.`);
		registry.managers[scheme] = manager;
	}
	static getManager(scheme) {
		const manager = this.getInstance().managers[scheme];
		if (manager == null) throw new Error(`Cannot find model manager for scheme '${scheme}'`);
		return manager;
	}
	static getSchemes() {
		return Object.keys(this.getInstance().managers);
	}
};
/**
* Helper method for parsing a URL string into a scheme and a path.
*
* @param url E.g., 'localstorage://my-model'
* @returns A dictionary with two fields: scheme and path.
*   Scheme: e.g., 'localstorage' in the example above.
*   Path: e.g., 'my-model' in the example above.
*/
function parseURL(url) {
	if (url.indexOf(URL_SCHEME_SUFFIX) === -1) throw new Error(`The url string provided does not contain a scheme. Supported schemes are: ${ModelStoreManagerRegistry.getSchemes().join(",")}`);
	return {
		scheme: url.split(URL_SCHEME_SUFFIX)[0],
		path: url.split(URL_SCHEME_SUFFIX)[1]
	};
}
async function cloneModelInternal(sourceURL, destURL, deleteSource = false) {
	assert(sourceURL !== destURL, () => `Old path and new path are the same: '${sourceURL}'`);
	const loadHandlers = IORouterRegistry.getLoadHandlers(sourceURL);
	assert(loadHandlers.length > 0, () => `Copying failed because no load handler is found for source URL ${sourceURL}.`);
	assert(loadHandlers.length < 2, () => `Copying failed because more than one (${loadHandlers.length}) load handlers for source URL ${sourceURL}.`);
	const loadHandler = loadHandlers[0];
	const saveHandlers = IORouterRegistry.getSaveHandlers(destURL);
	assert(saveHandlers.length > 0, () => `Copying failed because no save handler is found for destination URL ${destURL}.`);
	assert(saveHandlers.length < 2, () => `Copying failed because more than one (${loadHandlers.length}) save handlers for destination URL ${destURL}.`);
	const saveHandler = saveHandlers[0];
	const sourceScheme = parseURL(sourceURL).scheme;
	const sourcePath = parseURL(sourceURL).path;
	const sameMedium = sourceScheme === parseURL(sourceURL).scheme;
	const modelArtifacts = await loadHandler.load();
	if (deleteSource && sameMedium) await ModelStoreManagerRegistry.getManager(sourceScheme).removeModel(sourcePath);
	const saveResult = await saveHandler.save(modelArtifacts);
	if (deleteSource && !sameMedium) await ModelStoreManagerRegistry.getManager(sourceScheme).removeModel(sourcePath);
	return saveResult.modelArtifactsInfo;
}
/**
* List all models stored in registered storage mediums.
*
* For a web browser environment, the registered mediums are Local Storage and
* IndexedDB.
*
* ```js
* // First create and save a model.
* const model = tf.sequential();
* model.add(tf.layers.dense(
*     {units: 1, inputShape: [10], activation: 'sigmoid'}));
* await model.save('localstorage://demo/management/model1');
*
* // Then list existing models.
* console.log(JSON.stringify(await tf.io.listModels()));
*
* // Delete the model.
* await tf.io.removeModel('localstorage://demo/management/model1');
*
* // List models again.
* console.log(JSON.stringify(await tf.io.listModels()));
* ```
*
* @returns A `Promise` of a dictionary mapping URLs of existing models to
* their model artifacts info. URLs include medium-specific schemes, e.g.,
*   'indexeddb://my/model/1'. Model artifacts info include type of the
* model's topology, byte sizes of the topology, weights, etc.
*
* @doc {
*   heading: 'Models',
*   subheading: 'Management',
*   namespace: 'io',
*   ignoreCI: true
* }
*/
async function listModels() {
	const schemes = ModelStoreManagerRegistry.getSchemes();
	const out = {};
	for (const scheme of schemes) {
		const schemeOut = await ModelStoreManagerRegistry.getManager(scheme).listModels();
		for (const path in schemeOut) {
			const url = scheme + URL_SCHEME_SUFFIX + path;
			out[url] = schemeOut[path];
		}
	}
	return out;
}
/**
* Remove a model specified by URL from a reigstered storage medium.
*
* ```js
* // First create and save a model.
* const model = tf.sequential();
* model.add(tf.layers.dense(
*     {units: 1, inputShape: [10], activation: 'sigmoid'}));
* await model.save('localstorage://demo/management/model1');
*
* // Then list existing models.
* console.log(JSON.stringify(await tf.io.listModels()));
*
* // Delete the model.
* await tf.io.removeModel('localstorage://demo/management/model1');
*
* // List models again.
* console.log(JSON.stringify(await tf.io.listModels()));
* ```
*
* @param url A URL to a stored model, with a scheme prefix, e.g.,
*   'localstorage://my-model-1', 'indexeddb://my/model/2'.
* @returns ModelArtifactsInfo of the deleted model (if and only if deletion
*   is successful).
* @throws Error if deletion fails, e.g., if no model exists at `path`.
*
* @doc {
*   heading: 'Models',
*   subheading: 'Management',
*   namespace: 'io',
*   ignoreCI: true
* }
*/
async function removeModel(url) {
	const schemeAndPath = parseURL(url);
	return ModelStoreManagerRegistry.getManager(schemeAndPath.scheme).removeModel(schemeAndPath.path);
}
/**
* Copy a model from one URL to another.
*
* This function supports:
*
* 1. Copying within a storage medium, e.g.,
*    `tf.io.copyModel('localstorage://model-1', 'localstorage://model-2')`
* 2. Copying between two storage mediums, e.g.,
*    `tf.io.copyModel('localstorage://model-1', 'indexeddb://model-1')`
*
* ```js
* // First create and save a model.
* const model = tf.sequential();
* model.add(tf.layers.dense(
*     {units: 1, inputShape: [10], activation: 'sigmoid'}));
* await model.save('localstorage://demo/management/model1');
*
* // Then list existing models.
* console.log(JSON.stringify(await tf.io.listModels()));
*
* // Copy the model, from Local Storage to IndexedDB.
* await tf.io.copyModel(
*     'localstorage://demo/management/model1',
*     'indexeddb://demo/management/model1');
*
* // List models again.
* console.log(JSON.stringify(await tf.io.listModels()));
*
* // Remove both models.
* await tf.io.removeModel('localstorage://demo/management/model1');
* await tf.io.removeModel('indexeddb://demo/management/model1');
* ```
*
* @param sourceURL Source URL of copying.
* @param destURL Destination URL of copying.
* @returns ModelArtifactsInfo of the copied model (if and only if copying
*   is successful).
* @throws Error if copying fails, e.g., if no model exists at `sourceURL`, or
*   if `oldPath` and `newPath` are identical.
*
* @doc {
*   heading: 'Models',
*   subheading: 'Management',
*   namespace: 'io',
*   ignoreCI: true
* }
*/
async function copyModel(sourceURL, destURL) {
	return cloneModelInternal(sourceURL, destURL, false);
}
/**
* Move a model from one URL to another.
*
* This function supports:
*
* 1. Moving within a storage medium, e.g.,
*    `tf.io.moveModel('localstorage://model-1', 'localstorage://model-2')`
* 2. Moving between two storage mediums, e.g.,
*    `tf.io.moveModel('localstorage://model-1', 'indexeddb://model-1')`
*
* ```js
* // First create and save a model.
* const model = tf.sequential();
* model.add(tf.layers.dense(
*     {units: 1, inputShape: [10], activation: 'sigmoid'}));
* await model.save('localstorage://demo/management/model1');
*
* // Then list existing models.
* console.log(JSON.stringify(await tf.io.listModels()));
*
* // Move the model, from Local Storage to IndexedDB.
* await tf.io.moveModel(
*     'localstorage://demo/management/model1',
*     'indexeddb://demo/management/model1');
*
* // List models again.
* console.log(JSON.stringify(await tf.io.listModels()));
*
* // Remove the moved model.
* await tf.io.removeModel('indexeddb://demo/management/model1');
* ```
*
* @param sourceURL Source URL of moving.
* @param destURL Destination URL of moving.
* @returns ModelArtifactsInfo of the copied model (if and only if copying
*   is successful).
* @throws Error if moving fails, e.g., if no model exists at `sourceURL`, or
*   if `oldPath` and `newPath` are identical.
*
* @doc {
*   heading: 'Models',
*   subheading: 'Management',
*   namespace: 'io',
*   ignoreCI: true
* }
*/
async function moveModel(sourceURL, destURL) {
	return cloneModelInternal(sourceURL, destURL, true);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/platforms/platform_browser.js
var PlatformBrowser = class {
	fetch(path, init) {
		return fetch(path, init);
	}
	now() {
		return performance.now();
	}
	encode(text, encoding) {
		if (encoding !== "utf-8" && encoding !== "utf8") throw new Error(`Browser's encoder only supports utf-8, but got ${encoding}`);
		if (this.textEncoder == null) this.textEncoder = new TextEncoder();
		return this.textEncoder.encode(text);
	}
	decode(bytes, encoding) {
		return new TextDecoder(encoding).decode(bytes);
	}
};
if (env().get("IS_BROWSER")) {
	env().setPlatform("browser", new PlatformBrowser());
	try {
		ModelStoreManagerRegistry.registerManager(BrowserLocalStorage.URL_SCHEME, new BrowserLocalStorageManager());
	} catch (err) {}
	try {
		ModelStoreManagerRegistry.registerManager(BrowserIndexedDB.URL_SCHEME, new BrowserIndexedDBManager());
	} catch (err) {}
}

//#endregion
//#region browser-external:node-fetch
var require_browser_external_node_fetch = /* @__PURE__ */ __commonJS({ "browser-external:node-fetch": ((exports, module) => {
	module.exports = Object.create(new Proxy({}, { get(_, key) {
		if (key !== "__esModule" && key !== "__proto__" && key !== "constructor" && key !== "splice") console.warn(`Module "node-fetch" has been externalized for browser compatibility. Cannot access "node-fetch.${key}" in client code. See http://vite.dev/guide/troubleshooting.html#module-externalized-for-browser-compatibility for more details.`);
	} }));
}) });

//#endregion
//#region browser-external:util
var require_browser_external_util = /* @__PURE__ */ __commonJS({ "browser-external:util": ((exports, module) => {
	module.exports = Object.create(new Proxy({}, { get(_, key) {
		if (key !== "__esModule" && key !== "__proto__" && key !== "constructor" && key !== "splice") console.warn(`Module "util" has been externalized for browser compatibility. Cannot access "util.${key}" in client code. See http://vite.dev/guide/troubleshooting.html#module-externalized-for-browser-compatibility for more details.`);
	} }));
}) });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/platforms/platform_node.js
const getNodeFetch = { importFetch: () => require_browser_external_node_fetch() };
var systemFetch;
var PlatformNode = class {
	constructor() {
		this.util = require_browser_external_util();
		this.textEncoder = new this.util.TextEncoder();
	}
	fetch(path, requestInits) {
		if (env().global.fetch != null) return env().global.fetch(path, requestInits);
		if (systemFetch == null) systemFetch = getNodeFetch.importFetch();
		return systemFetch(path, requestInits);
	}
	now() {
		const time$1 = process.hrtime();
		return time$1[0] * 1e3 + time$1[1] / 1e6;
	}
	encode(text, encoding) {
		if (encoding !== "utf-8" && encoding !== "utf8") throw new Error(`Node built-in encoder only supports utf-8, but got ${encoding}`);
		return this.textEncoder.encode(text);
	}
	decode(bytes, encoding) {
		if (bytes.length === 0) return "";
		return new this.util.TextDecoder(encoding).decode(bytes);
	}
};
if (env().get("IS_NODE")) env().setPlatform("node", new PlatformNode());

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/buffer.js
/**
* Creates an empty `tf.TensorBuffer` with the specified `shape` and `dtype`.
*
* The values are stored in CPU as `TypedArray`. Fill the buffer using
* `buffer.set()`, or by modifying directly `buffer.values`.
*
* When done, call `buffer.toTensor()` to get an immutable `tf.Tensor` with
* those values.
*
* ```js
* // Create a buffer and set values at particular indices.
* const buffer = tf.buffer([2, 2]);
* buffer.set(3, 0, 0);
* buffer.set(5, 1, 0);
*
* // Convert the buffer back to a tensor.
* buffer.toTensor().print();
* ```
*
* @param shape An array of integers defining the output tensor shape.
* @param dtype The dtype of the buffer. Defaults to 'float32'.
* @param values The values of the buffer as `TypedArray`. Defaults to
* zeros.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function buffer(shape, dtype = "float32", values) {
	dtype = dtype || "float32";
	assertNonNegativeIntegerDimensions(shape);
	return new TensorBuffer(shape, dtype, values);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/cast.js
/**
* Casts a `tf.Tensor` to a new dtype.
*
* ```js
* const x = tf.tensor1d([1.5, 2.5, 3]);
* tf.cast(x, 'int32').print();
* ```
* @param x The input tensor to be casted.
* @param dtype The dtype to cast the input tensor to.
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function cast_(x, dtype) {
	const $x = convertToTensor(x, "x", "cast");
	if (!isValidDtype(dtype)) throw new Error(`Failed to cast to unknown dtype ${dtype}`);
	if (dtype === "string" && $x.dtype !== "string" || dtype !== "string" && $x.dtype === "string") throw new Error("Only strings can be casted to strings");
	const inputs = { x: $x };
	const attrs = { dtype };
	return ENGINE.runKernel(Cast, inputs, attrs);
}
const cast = op({ cast_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/clone.js
/**
* Creates a new tensor with the same values and shape as the specified
* tensor.
*
* ```js
* const x = tf.tensor([1, 2]);
*
* x.clone().print();
* ```
*
* @param x The tensor to clone.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function clone_(x) {
	const inputs = { x: convertToTensor(x, "x", "clone", "string_or_numeric") };
	return ENGINE.runKernel(Identity, inputs);
}
const clone = op({ clone_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/print.js
/**
* @license
* Copyright 2020 Google Inc. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
* Prints information about the `tf.Tensor` including its data.
*
* ```js
* const verbose = true;
* tf.tensor2d([1, 2, 3, 4], [2, 2]).print(verbose);
* ```
* @param x The tensor to be printed.
* @param verbose Whether to print verbose information about the ` Tensor`,
* including dtype and size.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function print(x, verbose = false) {
	console.log(x.toString(verbose));
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/base_side_effects.js
getOrMakeEngine();
setOpHandler({
	buffer,
	cast,
	clone,
	print
});

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/browser_files.js
var DEFAULT_FILE_NAME_PREFIX = "model";
var DEFAULT_JSON_EXTENSION_NAME = ".json";
var DEFAULT_WEIGHT_DATA_EXTENSION_NAME = ".weights.bin";
function defer(f) {
	return new Promise((resolve) => setTimeout(resolve)).then(f);
}
var BrowserDownloads = class BrowserDownloads {
	constructor(fileNamePrefix) {
		if (!env().getBool("IS_BROWSER")) throw new Error("browserDownloads() cannot proceed because the current environment is not a browser.");
		if (fileNamePrefix.startsWith(BrowserDownloads.URL_SCHEME)) fileNamePrefix = fileNamePrefix.slice(BrowserDownloads.URL_SCHEME.length);
		if (fileNamePrefix == null || fileNamePrefix.length === 0) fileNamePrefix = DEFAULT_FILE_NAME_PREFIX;
		this.modelJsonFileName = fileNamePrefix + DEFAULT_JSON_EXTENSION_NAME;
		this.weightDataFileName = fileNamePrefix + DEFAULT_WEIGHT_DATA_EXTENSION_NAME;
	}
	async save(modelArtifacts) {
		if (typeof document === "undefined") throw new Error("Browser downloads are not supported in this environment since `document` is not present");
		const weightsURL = window.URL.createObjectURL(new Blob([modelArtifacts.weightData], { type: "application/octet-stream" }));
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) throw new Error("BrowserDownloads.save() does not support saving model topology in binary formats yet.");
		else {
			const weightsManifest = [{
				paths: ["./" + this.weightDataFileName],
				weights: modelArtifacts.weightSpecs
			}];
			const modelJSON = getModelJSONForModelArtifacts(modelArtifacts, weightsManifest);
			const modelJsonURL = window.URL.createObjectURL(new Blob([JSON.stringify(modelJSON)], { type: "application/json" }));
			const jsonAnchor = this.modelJsonAnchor == null ? document.createElement("a") : this.modelJsonAnchor;
			jsonAnchor.download = this.modelJsonFileName;
			jsonAnchor.href = modelJsonURL;
			await defer(() => jsonAnchor.dispatchEvent(new MouseEvent("click")));
			if (modelArtifacts.weightData != null) {
				const weightDataAnchor = this.weightDataAnchor == null ? document.createElement("a") : this.weightDataAnchor;
				weightDataAnchor.download = this.weightDataFileName;
				weightDataAnchor.href = weightsURL;
				await defer(() => weightDataAnchor.dispatchEvent(new MouseEvent("click")));
			}
			return { modelArtifactsInfo: getModelArtifactsInfoForJSON(modelArtifacts) };
		}
	}
};
BrowserDownloads.URL_SCHEME = "downloads://";
var BrowserFiles = class {
	constructor(files) {
		if (files == null || files.length < 1) throw new Error(`When calling browserFiles, at least 1 file is required, but received ${files}`);
		this.jsonFile = files[0];
		this.weightsFiles = files.slice(1);
	}
	async load() {
		return new Promise((resolve, reject) => {
			const jsonReader = new FileReader();
			jsonReader.onload = (event) => {
				const modelJSON = JSON.parse(event.target.result);
				const modelTopology = modelJSON.modelTopology;
				if (modelTopology == null) {
					reject(/* @__PURE__ */ new Error(`modelTopology field is missing from file ${this.jsonFile.name}`));
					return;
				}
				if (modelJSON.weightsManifest == null) {
					reject(/* @__PURE__ */ new Error(`weightManifest field is missing from file ${this.jsonFile.name}`));
					return;
				}
				if (this.weightsFiles.length === 0) {
					resolve({ modelTopology });
					return;
				}
				const modelArtifactsPromise = getModelArtifactsForJSON(modelJSON, (weightsManifest) => this.loadWeights(weightsManifest));
				resolve(modelArtifactsPromise);
			};
			jsonReader.onerror = (error) => reject(`Failed to read model topology and weights manifest JSON from file '${this.jsonFile.name}'. BrowserFiles supports loading Keras-style tf.Model artifacts only.`);
			jsonReader.readAsText(this.jsonFile);
		});
	}
	loadWeights(weightsManifest) {
		const weightSpecs = [];
		const paths = [];
		for (const entry of weightsManifest) {
			weightSpecs.push(...entry.weights);
			paths.push(...entry.paths);
		}
		const pathToFile = this.checkManifestAndWeightFiles(weightsManifest);
		const promises = paths.map((path) => this.loadWeightsFile(path, pathToFile[path]));
		return Promise.all(promises).then((buffers) => [weightSpecs, concatenateArrayBuffers(buffers)]);
	}
	loadWeightsFile(path, file) {
		return new Promise((resolve, reject) => {
			const weightFileReader = new FileReader();
			weightFileReader.onload = (event) => {
				const weightData = event.target.result;
				resolve(weightData);
			};
			weightFileReader.onerror = (error) => reject(`Failed to weights data from file of path '${path}'.`);
			weightFileReader.readAsArrayBuffer(file);
		});
	}
	/**
	* Check the compatibility between weights manifest and weight files.
	*/
	checkManifestAndWeightFiles(manifest) {
		const basenames = [];
		const fileNames = this.weightsFiles.map((file) => basename(file.name));
		const pathToFile = {};
		for (const group of manifest) group.paths.forEach((path) => {
			const pathBasename = basename(path);
			if (basenames.indexOf(pathBasename) !== -1) throw new Error(`Duplicate file basename found in weights manifest: '${pathBasename}'`);
			basenames.push(pathBasename);
			if (fileNames.indexOf(pathBasename) === -1) throw new Error(`Weight file with basename '${pathBasename}' is not provided.`);
			else pathToFile[path] = this.weightsFiles[fileNames.indexOf(pathBasename)];
		});
		if (basenames.length !== this.weightsFiles.length) throw new Error(`Mismatch in the number of files in weights manifest (${basenames.length}) and the number of weight files provided (${this.weightsFiles.length}).`);
		return pathToFile;
	}
};
const browserDownloadsRouter = (url) => {
	if (!env().getBool("IS_BROWSER")) return null;
	else if (!Array.isArray(url) && url.startsWith(BrowserDownloads.URL_SCHEME)) return browserDownloads(url.slice(BrowserDownloads.URL_SCHEME.length));
	else return null;
};
IORouterRegistry.registerSaveRouter(browserDownloadsRouter);
/**
* Creates an IOHandler that triggers file downloads from the browser.
*
* The returned `IOHandler` instance can be used as model exporting methods such
* as `tf.Model.save` and supports only saving.
*
* ```js
* const model = tf.sequential();
* model.add(tf.layers.dense(
*     {units: 1, inputShape: [10], activation: 'sigmoid'}));
* const saveResult = await model.save('downloads://mymodel');
* // This will trigger downloading of two files:
* //   'mymodel.json' and 'mymodel.weights.bin'.
* console.log(saveResult);
* ```
*
* @param fileNamePrefix Prefix name of the files to be downloaded. For use with
*   `tf.Model`, `fileNamePrefix` should follow either of the following two
*   formats:
*   1. `null` or `undefined`, in which case the default file
*      names will be used:
*      - 'model.json' for the JSON file containing the model topology and
*        weights manifest.
*      - 'model.weights.bin' for the binary file containing the binary weight
*        values.
*   2. A single string or an Array of a single string, as the file name prefix.
*      For example, if `'foo'` is provided, the downloaded JSON
*      file and binary weights file will be named 'foo.json' and
*      'foo.weights.bin', respectively.
* @param config Additional configuration for triggering downloads.
* @returns An instance of `BrowserDownloads` `IOHandler`.
*
* @doc {
*   heading: 'Models',
*   subheading: 'Loading',
*   namespace: 'io',
*   ignoreCI: true
* }
*/
function browserDownloads(fileNamePrefix = "model") {
	return new BrowserDownloads(fileNamePrefix);
}
/**
* Creates an IOHandler that loads model artifacts from user-selected files.
*
* This method can be used for loading from files such as user-selected files
* in the browser.
* When used in conjunction with `tf.loadLayersModel`, an instance of
* `tf.LayersModel` (Keras-style) can be constructed from the loaded artifacts.
*
* ```js
* // Note: This code snippet won't run properly without the actual file input
* //   elements in the HTML DOM.
*
* // Suppose there are two HTML file input (`<input type="file" ...>`)
* // elements.
* const uploadJSONInput = document.getElementById('upload-json');
* const uploadWeightsInput = document.getElementById('upload-weights');
* const model = await tf.loadLayersModel(tf.io.browserFiles(
*     [uploadJSONInput.files[0], uploadWeightsInput.files[0]]));
* ```
*
* @param files `File`s to load from. Currently, this function supports only
*   loading from files that contain Keras-style models (i.e., `tf.Model`s), for
*   which an `Array` of `File`s is expected (in that order):
*   - A JSON file containing the model topology and weight manifest.
*   - Optionally, One or more binary files containing the binary weights.
*     These files must have names that match the paths in the `weightsManifest`
*     contained by the aforementioned JSON file, or errors will be thrown
*     during loading. These weights files have the same format as the ones
*     generated by `tensorflowjs_converter` that comes with the `tensorflowjs`
*     Python PIP package. If no weights files are provided, only the model
*     topology will be loaded from the JSON file above.
* @returns An instance of `Files` `IOHandler`.
*
* @doc {
*   heading: 'Models',
*   subheading: 'Loading',
*   namespace: 'io',
*   ignoreCI: true
* }
*/
function browserFiles(files) {
	return new BrowserFiles(files);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/progress.js
/**
* Monitor Promise.all progress, fire onProgress callback function.
*
* @param promises Promise list going to be monitored
* @param onProgress Callback function. Fired when a promise resolved.
* @param startFraction Optional fraction start. Default to 0.
* @param endFraction Optional fraction end. Default to 1.
*/
function monitorPromisesProgress(promises, onProgress, startFraction, endFraction) {
	checkPromises(promises);
	startFraction = startFraction == null ? 0 : startFraction;
	endFraction = endFraction == null ? 1 : endFraction;
	checkFraction(startFraction, endFraction);
	let resolvedPromise = 0;
	const registerMonitor = (promise) => {
		promise.then((value) => {
			const fraction = startFraction + ++resolvedPromise / promises.length * (endFraction - startFraction);
			onProgress(fraction);
			return value;
		});
		return promise;
	};
	function checkPromises(promises$1) {
		assert(promises$1 != null && Array.isArray(promises$1) && promises$1.length > 0, () => "promises must be a none empty array");
	}
	function checkFraction(startFraction$1, endFraction$1) {
		assert(startFraction$1 >= 0 && startFraction$1 <= 1, () => `Progress fraction must be in range [0, 1], but got startFraction ${startFraction$1}`);
		assert(endFraction$1 >= 0 && endFraction$1 <= 1, () => `Progress fraction must be in range [0, 1], but got endFraction ${endFraction$1}`);
		assert(endFraction$1 >= startFraction$1, () => `startFraction must be no more than endFraction, but got startFraction ${startFraction$1} and endFraction ${endFraction$1}`);
	}
	return Promise.all(promises.map(registerMonitor));
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/weights_loader.js
/**
* Reads binary weights data from a number of URLs.
*
* @param fetchURLs URLs to send the HTTP requests at, using `fetch` calls.
* @param requestOptions RequestInit (options) for the HTTP requests.
* @param fetchFunc Optional overriding value for the `window.fetch` function.
* @param onProgress Optional, progress callback function, fired periodically
*   before the load is completed.
* @returns A `Promise` of an Array of `ArrayBuffer`. The Array has the same
*   length as `fetchURLs`.
*/
async function loadWeightsAsArrayBuffer(fetchURLs, loadOptions) {
	if (loadOptions == null) loadOptions = {};
	const fetchFunc = loadOptions.fetchFunc == null ? env().platform.fetch : loadOptions.fetchFunc;
	const requests = fetchURLs.map((fetchURL) => fetchFunc(fetchURL, loadOptions.requestInit, { isBinary: true }));
	const bufferPromises = (loadOptions.onProgress == null ? await Promise.all(requests) : await monitorPromisesProgress(requests, loadOptions.onProgress, 0, .5)).map((response) => response.arrayBuffer());
	return loadOptions.onProgress == null ? await Promise.all(bufferPromises) : await monitorPromisesProgress(bufferPromises, loadOptions.onProgress, .5, 1);
}
/**
* Reads a weights manifest JSON configuration, fetches the weights and
* returns them as `Tensor`s.
*
* @param manifest The weights manifest JSON.
* @param filePathPrefix The path prefix for filenames given in the manifest.
*     Defaults to the empty string.
* @param weightNames The names of the weights to be fetched.
*/
async function loadWeights(manifest, filePathPrefix = "", weightNames, requestInit) {
	const fetchWeights = (fetchUrls) => loadWeightsAsArrayBuffer(fetchUrls, { requestInit });
	return weightsLoaderFactory(fetchWeights)(manifest, filePathPrefix, weightNames);
}
/**
* Creates a function, which reads a weights manifest JSON configuration,
* fetches the weight files using the specified function and returns them as
* `Tensor`s.
*
* ```js
* // example for creating a nodejs weight loader, which reads the weight files
* // from disk using fs.readFileSync
*
* import * as fs from 'fs'
*
* const fetchWeightsFromDisk = (filePaths: string[]) =>
*   filePaths.map(filePath => fs.readFileSync(filePath).buffer)
*
* const loadWeights = tf.io.weightsLoaderFactory(fetchWeightsFromDisk)
*
* const manifest = JSON.parse(
*   fs.readFileSync('./my_model-weights_manifest').toString()
* )
* const weightMap = await loadWeights(manifest, './')
* ```
* @param fetchWeightsFunction The function used for fetching the weight files.
* @returns Weight loading function.
*/
function weightsLoaderFactory(fetchWeightsFunction) {
	return async (manifest, filePathPrefix = "", weightNames) => {
		const groupIndicesToFetchMap = manifest.map(() => false);
		const groupWeightsToFetch = {};
		const weightsFound = weightNames != null ? weightNames.map(() => false) : [];
		const allManifestWeightNames = [];
		manifest.forEach((manifestGroupConfig, groupIndex) => {
			let groupOffset = 0;
			manifestGroupConfig.weights.forEach((weightsEntry) => {
				const rawDtype = "quantization" in weightsEntry ? weightsEntry.quantization.dtype : weightsEntry.dtype;
				const weightsBytes = DTYPE_VALUE_SIZE_MAP[rawDtype] * sizeFromShape(weightsEntry.shape);
				const enqueueWeightsForFetchingFn = () => {
					groupIndicesToFetchMap[groupIndex] = true;
					if (groupWeightsToFetch[groupIndex] == null) groupWeightsToFetch[groupIndex] = [];
					groupWeightsToFetch[groupIndex].push({
						manifestEntry: weightsEntry,
						groupOffset,
						sizeBytes: weightsBytes
					});
				};
				if (weightNames != null) weightNames.forEach((weightName, weightIndex) => {
					if (weightName === weightsEntry.name) {
						enqueueWeightsForFetchingFn();
						weightsFound[weightIndex] = true;
					}
				});
				else enqueueWeightsForFetchingFn();
				allManifestWeightNames.push(weightsEntry.name);
				groupOffset += weightsBytes;
			});
		});
		if (!weightsFound.every((found) => found)) {
			const weightsNotFound = weightNames.filter((_, i) => !weightsFound[i]);
			throw new Error(`Could not find weights in manifest with names: ${weightsNotFound.join(", ")}. \nManifest JSON has weights with names: ${allManifestWeightNames.join(", ")}.`);
		}
		const groupIndicesToFetch = groupIndicesToFetchMap.reduce((accumulator, shouldFetch, i) => {
			if (shouldFetch) accumulator.push(i);
			return accumulator;
		}, []);
		const fetchUrls = [];
		groupIndicesToFetch.forEach((i) => {
			manifest[i].paths.forEach((filepath) => {
				const fetchUrl = filePathPrefix + (!filePathPrefix.endsWith("/") ? "/" : "") + filepath;
				fetchUrls.push(fetchUrl);
			});
		});
		const buffers = await fetchWeightsFunction(fetchUrls);
		const weightsTensorMap = {};
		let bufferIndexOffset = 0;
		groupIndicesToFetch.forEach((i) => {
			const numBuffers = manifest[i].paths.length;
			let groupBytes = 0;
			for (let i$1 = 0; i$1 < numBuffers; i$1++) groupBytes += buffers[bufferIndexOffset + i$1].byteLength;
			const groupBuffer = new ArrayBuffer(groupBytes);
			const groupByteBuffer = new Uint8Array(groupBuffer);
			let groupBufferOffset = 0;
			for (let i$1 = 0; i$1 < numBuffers; i$1++) {
				const buffer$1 = new Uint8Array(buffers[bufferIndexOffset + i$1]);
				groupByteBuffer.set(buffer$1, groupBufferOffset);
				groupBufferOffset += buffer$1.byteLength;
			}
			groupWeightsToFetch[i].forEach((weightsEntry) => {
				const byteBuffer = groupBuffer.slice(weightsEntry.groupOffset, weightsEntry.groupOffset + weightsEntry.sizeBytes);
				const nameToTensorMap = decodeWeights(byteBuffer, [weightsEntry.manifestEntry]);
				for (const name in nameToTensorMap) weightsTensorMap[name] = nameToTensorMap[name];
			});
			bufferIndexOffset += numBuffers;
		});
		return weightsTensorMap;
	};
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/http.js
var OCTET_STREAM_MIME_TYPE = "application/octet-stream";
var JSON_TYPE = "application/json";
var HTTPRequest = class {
	constructor(path, loadOptions) {
		this.DEFAULT_METHOD = "POST";
		if (loadOptions == null) loadOptions = {};
		this.weightPathPrefix = loadOptions.weightPathPrefix;
		this.onProgress = loadOptions.onProgress;
		this.weightUrlConverter = loadOptions.weightUrlConverter;
		if (loadOptions.fetchFunc != null) {
			assert(typeof loadOptions.fetchFunc === "function", () => "Must pass a function that matches the signature of `fetch` (see https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API)");
			this.fetch = loadOptions.fetchFunc;
		} else this.fetch = env().platform.fetch;
		assert(path != null && path.length > 0, () => "URL path for http must not be null, undefined or empty.");
		if (Array.isArray(path)) assert(path.length === 2, () => `URL paths for http must have a length of 2, (actual length is ${path.length}).`);
		this.path = path;
		if (loadOptions.requestInit != null && loadOptions.requestInit.body != null) throw new Error("requestInit is expected to have no pre-existing body, but has one.");
		this.requestInit = loadOptions.requestInit || {};
	}
	async save(modelArtifacts) {
		if (modelArtifacts.modelTopology instanceof ArrayBuffer) throw new Error("BrowserHTTPRequest.save() does not support saving model topology in binary formats yet.");
		const init = Object.assign({ method: this.DEFAULT_METHOD }, this.requestInit);
		init.body = new FormData();
		const weightsManifest = [{
			paths: ["./model.weights.bin"],
			weights: modelArtifacts.weightSpecs
		}];
		const modelTopologyAndWeightManifest = getModelJSONForModelArtifacts(modelArtifacts, weightsManifest);
		init.body.append("model.json", new Blob([JSON.stringify(modelTopologyAndWeightManifest)], { type: JSON_TYPE }), "model.json");
		if (modelArtifacts.weightData != null) init.body.append("model.weights.bin", new Blob([modelArtifacts.weightData], { type: OCTET_STREAM_MIME_TYPE }), "model.weights.bin");
		const response = await this.fetch(this.path, init);
		if (response.ok) return {
			modelArtifactsInfo: getModelArtifactsInfoForJSON(modelArtifacts),
			responses: [response]
		};
		else throw new Error(`BrowserHTTPRequest.save() failed due to HTTP response status ${response.status}.`);
	}
	/**
	* Load model artifacts via HTTP request(s).
	*
	* See the documentation to `tf.io.http` for details on the saved
	* artifacts.
	*
	* @returns The loaded model artifacts (if loading succeeds).
	*/
	async load() {
		const modelConfigRequest = await this.fetch(this.path, this.requestInit);
		if (!modelConfigRequest.ok) throw new Error(`Request to ${this.path} failed with status code ${modelConfigRequest.status}. Please verify this URL points to the model JSON of the model to load.`);
		let modelJSON;
		try {
			modelJSON = await modelConfigRequest.json();
		} catch (e) {
			let message = `Failed to parse model JSON of response from ${this.path}.`;
			if (this.path.endsWith(".pb")) message += " Your path contains a .pb file extension. Support for .pb models have been removed in TensorFlow.js 1.0 in favor of .json models. You can re-convert your Python TensorFlow model using the TensorFlow.js 1.0 conversion scripts or you can convert your.pb models with the 'pb2json'NPM script in the tensorflow/tfjs-converter repository.";
			else message += " Please make sure the server is serving valid JSON for this request.";
			throw new Error(message);
		}
		const modelTopology = modelJSON.modelTopology;
		const weightsManifest = modelJSON.weightsManifest;
		if (modelTopology == null && weightsManifest == null) throw new Error(`The JSON from HTTP path ${this.path} contains neither model topology or manifest for weights.`);
		return getModelArtifactsForJSON(modelJSON, (weightsManifest$1) => this.loadWeights(weightsManifest$1));
	}
	async loadWeights(weightsManifest) {
		const weightPath = Array.isArray(this.path) ? this.path[1] : this.path;
		const [prefix, suffix] = parseUrl(weightPath);
		const pathPrefix = this.weightPathPrefix || prefix;
		const weightSpecs = [];
		for (const entry of weightsManifest) weightSpecs.push(...entry.weights);
		const fetchURLs = [];
		const urlPromises = [];
		for (const weightsGroup of weightsManifest) for (const path of weightsGroup.paths) if (this.weightUrlConverter != null) urlPromises.push(this.weightUrlConverter(path));
		else fetchURLs.push(pathPrefix + path + suffix);
		if (this.weightUrlConverter) fetchURLs.push(...await Promise.all(urlPromises));
		const buffers = await loadWeightsAsArrayBuffer(fetchURLs, {
			requestInit: this.requestInit,
			fetchFunc: this.fetch,
			onProgress: this.onProgress
		});
		return [weightSpecs, concatenateArrayBuffers(buffers)];
	}
};
HTTPRequest.URL_SCHEME_REGEX = /^https?:\/\//;
/**
* Extract the prefix and suffix of the url, where the prefix is the path before
* the last file, and suffix is the search params after the last file.
* ```
* const url = 'http://tfhub.dev/model/1/tensorflowjs_model.pb?tfjs-format=file'
* [prefix, suffix] = parseUrl(url)
* // prefix = 'http://tfhub.dev/model/1/'
* // suffix = '?tfjs-format=file'
* ```
* @param url the model url to be parsed.
*/
function parseUrl(url) {
	const lastSlash = url.lastIndexOf("/");
	const lastSearchParam = url.lastIndexOf("?");
	const prefix = url.substring(0, lastSlash);
	const suffix = lastSearchParam > lastSlash ? url.substring(lastSearchParam) : "";
	return [prefix + "/", suffix];
}
function isHTTPScheme(url) {
	return url.match(HTTPRequest.URL_SCHEME_REGEX) != null;
}
const httpRouter = (url, loadOptions) => {
	if (typeof fetch === "undefined" && (loadOptions == null || loadOptions.fetchFunc == null)) return null;
	else {
		let isHTTP = true;
		if (Array.isArray(url)) isHTTP = url.every((urlItem) => isHTTPScheme(urlItem));
		else isHTTP = isHTTPScheme(url);
		if (isHTTP) return http(url, loadOptions);
	}
	return null;
};
IORouterRegistry.registerSaveRouter(httpRouter);
IORouterRegistry.registerLoadRouter(httpRouter);
/**
* Creates an IOHandler subtype that sends model artifacts to HTTP server.
*
* An HTTP request of the `multipart/form-data` mime type will be sent to the
* `path` URL. The form data includes artifacts that represent the topology
* and/or weights of the model. In the case of Keras-style `tf.Model`, two
* blobs (files) exist in form-data:
*   - A JSON file consisting of `modelTopology` and `weightsManifest`.
*   - A binary weights file consisting of the concatenated weight values.
* These files are in the same format as the one generated by
* [tfjs_converter](https://js.tensorflow.org/tutorials/import-keras.html).
*
* The following code snippet exemplifies the client-side code that uses this
* function:
*
* ```js
* const model = tf.sequential();
* model.add(
*     tf.layers.dense({units: 1, inputShape: [100], activation: 'sigmoid'}));
*
* const saveResult = await model.save(tf.io.http(
*     'http://model-server:5000/upload', {requestInit: {method: 'PUT'}}));
* console.log(saveResult);
* ```
*
* If the default `POST` method is to be used, without any custom parameters
* such as headers, you can simply pass an HTTP or HTTPS URL to `model.save`:
*
* ```js
* const saveResult = await model.save('http://model-server:5000/upload');
* ```
*
* The following GitHub Gist
* https://gist.github.com/dsmilkov/1b6046fd6132d7408d5257b0976f7864
* implements a server based on [flask](https://github.com/pallets/flask) that
* can receive the request. Upon receiving the model artifacts via the requst,
* this particular server reconsistutes instances of [Keras
* Models](https://keras.io/models/model/) in memory.
*
*
* @param path A URL path to the model.
*   Can be an absolute HTTP path (e.g.,
*   'http://localhost:8000/model-upload)') or a relative path (e.g.,
*   './model-upload').
* @param requestInit Request configurations to be used when sending
*    HTTP request to server using `fetch`. It can contain fields such as
*    `method`, `credentials`, `headers`, `mode`, etc. See
*    https://developer.mozilla.org/en-US/docs/Web/API/Request/Request
*    for more information. `requestInit` must not have a body, because the
* body will be set by TensorFlow.js. File blobs representing the model
* topology (filename: 'model.json') and the weights of the model (filename:
* 'model.weights.bin') will be appended to the body. If `requestInit` has a
* `body`, an Error will be thrown.
* @param loadOptions Optional configuration for the loading. It includes the
*   following fields:
*   - weightPathPrefix Optional, this specifies the path prefix for weight
*     files, by default this is calculated from the path param.
*   - fetchFunc Optional, custom `fetch` function. E.g., in Node.js,
*     the `fetch` from node-fetch can be used here.
*   - onProgress Optional, progress callback function, fired periodically
*     before the load is completed.
* @returns An instance of `IOHandler`.
*
* @doc {
*   heading: 'Models',
*   subheading: 'Loading',
*   namespace: 'io',
*   ignoreCI: true
* }
*/
function http(path, loadOptions) {
	return new HTTPRequest(path, loadOptions);
}
/**
* Deprecated. Use `tf.io.http`.
* @param path
* @param loadOptions
*/
function browserHTTPRequest(path, loadOptions) {
	return http(path, loadOptions);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/passthrough.js
/**
* @license
* Copyright 2018 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
var PassthroughLoader = class {
	constructor(modelArtifacts) {
		this.modelArtifacts = modelArtifacts;
	}
	async load() {
		return this.modelArtifacts;
	}
};
var PassthroughSaver = class {
	constructor(saveHandler) {
		this.saveHandler = saveHandler;
	}
	async save(modelArtifacts) {
		return this.saveHandler(modelArtifacts);
	}
};
/**
* Creates an IOHandler that loads model artifacts from memory.
*
* When used in conjunction with `tf.loadLayersModel`, an instance of
* `tf.LayersModel` (Keras-style) can be constructed from the loaded artifacts.
*
* ```js
* const model = await tf.loadLayersModel(tf.io.fromMemory(
*     modelTopology, weightSpecs, weightData));
* ```
*
* @param modelArtifacts a object containing model topology (i.e., parsed from
*   the JSON format).
* @param weightSpecs An array of `WeightsManifestEntry` objects describing the
*   names, shapes, types, and quantization of the weight data.
* @param weightData A single `ArrayBuffer` containing the weight data,
*   concatenated in the order described by the weightSpecs.
* @param trainingConfig Model training configuration. Optional.
*
* @returns A passthrough `IOHandler` that simply loads the provided data.
*/
function fromMemory(modelArtifacts, weightSpecs, weightData, trainingConfig) {
	if (arguments.length === 1) if (modelArtifacts.modelTopology != null || modelArtifacts.weightSpecs != null) return new PassthroughLoader(modelArtifacts);
	else {
		console.warn("Please call tf.io.fromMemory() with only one argument. The argument should be of type ModelArtifacts. The multi-argument signature of tf.io.fromMemory() has been deprecated and will be removed in a future release.");
		return new PassthroughLoader({ modelTopology: modelArtifacts });
	}
	else {
		console.warn("Please call tf.io.fromMemory() with only one argument. The argument should be of type ModelArtifacts. The multi-argument signature of tf.io.fromMemory() has been deprecated and will be removed in a future release.");
		return new PassthroughLoader({
			modelTopology: modelArtifacts,
			weightSpecs,
			weightData,
			trainingConfig
		});
	}
}
/**
* Creates an IOHandler that passes saved model artifacts to a callback.
*
* ```js
* function handleSave(artifacts) {
*   // ... do something with the artifacts ...
*   return {modelArtifactsInfo: {...}, ...};
* }
*
* const saveResult = model.save(tf.io.withSaveHandler(handleSave));
* ```
*
* @param saveHandler A function that accepts a `ModelArtifacts` and returns a
*     `SaveResult`.
*/
function withSaveHandler(saveHandler) {
	return new PassthroughSaver(saveHandler);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/io/io.js
var io_exports = /* @__PURE__ */ __export({
	browserFiles: () => browserFiles,
	browserHTTPRequest: () => browserHTTPRequest,
	concatenateArrayBuffers: () => concatenateArrayBuffers,
	copyModel: () => copyModel,
	decodeWeights: () => decodeWeights,
	encodeWeights: () => encodeWeights,
	fromMemory: () => fromMemory,
	getLoadHandlers: () => getLoadHandlers,
	getModelArtifactsForJSON: () => getModelArtifactsForJSON,
	getModelArtifactsInfoForJSON: () => getModelArtifactsInfoForJSON,
	getSaveHandlers: () => getSaveHandlers,
	http: () => http,
	isHTTPScheme: () => isHTTPScheme,
	listModels: () => listModels,
	loadWeights: () => loadWeights,
	moveModel: () => moveModel,
	registerLoadRouter: () => registerLoadRouter,
	registerSaveRouter: () => registerSaveRouter,
	removeModel: () => removeModel,
	weightsLoaderFactory: () => weightsLoaderFactory,
	withSaveHandler: () => withSaveHandler
});

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/mat_mul.js
/**
* Computes the dot product of two matrices, A * B. These must be matrices.
*
* ```js
* const a = tf.tensor2d([1, 2], [1, 2]);
* const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* a.matMul(b).print();  // or tf.matMul(a, b)
* ```
* @param a First matrix in dot product operation.
* @param b Second matrix in dot product operation.
* @param transposeA If true, `a` is transposed before multiplication.
* @param transposeB If true, `b` is transposed before multiplication.
*
* @doc {heading: 'Operations', subheading: 'Matrices'}
*/
function matMul_(a, b, transposeA = false, transposeB = false) {
	let $a = convertToTensor(a, "a", "matMul");
	let $b = convertToTensor(b, "b", "matMul");
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	const attrs = {
		transposeA,
		transposeB
	};
	return ENGINE.runKernel(BatchMatMul, inputs, attrs);
}
const matMul = op({ matMul_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/one_hot.js
/**
* Creates a one-hot `tf.Tensor`. The locations represented by `indices` take
* value `onValue` (defaults to 1), while all other locations take value
* `offValue` (defaults to 0). If `indices` is rank `R`, the output has rank
* `R+1` with the last axis of size `depth`.
*
* ```js
* tf.oneHot(tf.tensor1d([0, 1], 'int32'), 3).print();
* ```
*
* @param indices `tf.Tensor` of indices with dtype `int32`.
* @param depth The depth of the one hot dimension.
* @param onValue A number used to fill in the output when the index matches
* the location.
* @param offValue A number used to fill in the output when the index does
*     not match the location.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function oneHot_(indices, depth, onValue = 1, offValue = 0) {
	if (depth < 2) throw new Error(`Error in oneHot: depth must be >=2, but it is ${depth}`);
	const inputs = { indices: convertToTensor(indices, "indices", "oneHot", "int32") };
	const attrs = {
		depth,
		onValue,
		offValue
	};
	return ENGINE.runKernel(OneHot, inputs, attrs);
}
const oneHot = op({ oneHot_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/transpose.js
/**
* Transposes the `tf.Tensor`. Permutes the dimensions according to `perm`.
*
* The returned `tf.Tensor`'s dimension `i` will correspond to the input
* dimension `perm[i]`. If `perm` is not given, it is set to `[n-1...0]`,
* where `n` is the rank of the input `tf.Tensor`. Hence by default, this
* operation performs a regular matrix transpose on 2-D input `tf.Tensor`s.
*
* ```js
* const a = tf.tensor2d([1, 2, 3, 4, 5, 6], [2, 3]);
*
* a.transpose().print();  // or tf.transpose(a)
* ```
*
* @param x The tensor to transpose.
* @param perm The permutation of the dimensions of a.
*
* @doc {heading: 'Operations', subheading: 'Matrices'}
*/
function transpose_(x, perm) {
	const $x = convertToTensor(x, "x", "transpose");
	if (perm == null) perm = $x.shape.map((s, i) => i).reverse();
	assert($x.rank === perm.length, () => `Error in transpose: rank of input ${$x.rank} must match length of perm ${perm}.`);
	perm.forEach((axis) => {
		assert(axis >= 0 && axis < $x.rank, () => `All entries in 'perm' must be between 0 and ${$x.rank - 1} but got ${perm}`);
	});
	if ($x.rank <= 1) return $x.clone();
	const inputs = { x: $x };
	const attrs = { perm };
	return ENGINE.runKernel(Transpose, inputs, attrs);
}
const transpose = op({ transpose_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/confusion_matrix.js
/**
* Computes the confusion matrix from true labels and predicted labels.
*
* ```js
* const labels = tf.tensor1d([0, 1, 2, 1, 0], 'int32');
* const predictions = tf.tensor1d([0, 2, 2, 1, 0], 'int32');
* const numClasses = 3;
* const out = tf.math.confusionMatrix(labels, predictions, numClasses);
* out.print();
* // Expected output matrix:
* // [[2, 0, 0],
* //  [0, 1, 1],
* //  [0, 0, 1]]
* ```
*
* @param labels The target labels, assumed to be 0-based integers
*   for the classes. The shape is `[numExamples]`, where
*   `numExamples` is the number of examples included.
* @param predictions The predicted classes, assumed to be
*   0-based integers for the classes. Must have the same shape as `labels`.
* @param numClasses Number of all classes, as an integer.
*   Its value must be larger than the largest element in `labels` and
*   `predictions`.
* @returns The confusion matrix as a int32-type 2D tensor. The value at
*   row `r` and column `c` is the number of times examples of actual class
*   `r` were predicted as class `c`.
*
* @doc {heading: 'Operations', subheading: 'Evaluation'}
*/
function confusionMatrix_(labels, predictions, numClasses) {
	const $labels = convertToTensor(labels, "labels", "confusionMatrix");
	const $predictions = convertToTensor(predictions, "predictions", "confusionMatrix");
	assert(numClasses == null || numClasses > 0 && Number.isInteger(numClasses), () => `If provided, numClasses must be a positive integer, but got ${numClasses}`);
	assert($labels.rank === 1, () => `Expected the rank of labels to be 1, but got ${$labels.rank}`);
	assert($predictions.rank === 1, () => `Expected the rank of predictions to be 1, but got ${$predictions.rank}`);
	assert($labels.shape[0] === $predictions.shape[0], () => `Mismatch in the number of examples: ${$labels.shape[0]} vs. ${$predictions.shape[0]}. Labels and predictions should have the same number of elements.`);
	assert(numClasses > 0 && Number.isInteger(numClasses), () => `numClasses is required to be a positive integer, but got ${numClasses}`);
	const oneHotLabels = oneHot(cast($labels, "int32"), numClasses);
	const oneHotPredictions = oneHot(cast($predictions, "int32"), numClasses);
	const oneHotLabelsT = transpose(oneHotLabels);
	const product = matMul(oneHotLabelsT, oneHotPredictions);
	return cast(product, "int32");
}
const confusionMatrix = op({ confusionMatrix_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/math.js
var math_exports = /* @__PURE__ */ __export({ confusionMatrix: () => confusionMatrix });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor3d.js
/**
* Creates rank-3 `tf.Tensor` with the provided values, shape and dtype.
*
* The same functionality can be achieved with `tf.tensor`, but in general
* we recommend using `tf.tensor3d` as it makes the code more readable.
*
*  ```js
* // Pass a nested array.
* tf.tensor3d([[[1], [2]], [[3], [4]]]).print();
* ```
* ```js
* // Pass a flat array and specify a shape.
* tf.tensor3d([1, 2, 3, 4], [2, 2, 1]).print();
* ```
*
* @param values The values of the tensor. Can be nested array of numbers,
*     or a flat array, or a `TypedArray`.
* @param shape The shape of the tensor. If not provided,  it is inferred from
*     `values`.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function tensor3d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 3) throw new Error("tensor3d() requires shape to have three numbers");
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 3 && inferredShape.length !== 1) throw new Error("tensor3d() requires values to be number[][][] or flat/TypedArray");
	if (inferredShape.length === 1 && shape == null) throw new Error("tensor3d() requires shape to be provided when `values` are a flat array");
	return makeTensor(values, shape, inferredShape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/browser.js
var browser_exports = /* @__PURE__ */ __export({
	fromPixels: () => fromPixels,
	fromPixelsAsync: () => fromPixelsAsync,
	toPixels: () => toPixels
});
var fromPixels2DContext;
/**
* Creates a `tf.Tensor` from an image.
*
* ```js
* const image = new ImageData(1, 1);
* image.data[0] = 100;
* image.data[1] = 150;
* image.data[2] = 200;
* image.data[3] = 255;
*
* tf.browser.fromPixels(image).print();
* ```
*
* @param pixels The input image to construct the tensor from. The
* supported image types are all 4-channel. You can also pass in an image
* object with following attributes:
* `{data: Uint8Array; width: number; height: number}`
* @param numChannels The number of channels of the output tensor. A
* numChannels value less than 4 allows you to ignore channels. Defaults to
* 3 (ignores alpha channel of input image).
*
* @returns A Tensor3D with the shape `[height, width, numChannels]`.
*
* @doc {heading: 'Browser', namespace: 'browser', ignoreCI: true}
*/
function fromPixels_(pixels, numChannels = 3) {
	if (numChannels > 4) throw new Error("Cannot construct Tensor with more than 4 channels from pixels.");
	if (pixels == null) throw new Error("pixels passed to tf.browser.fromPixels() can not be null");
	let isPixelData$1 = false;
	let isImageData = false;
	let isVideo = false;
	let isImage = false;
	let isCanvasLike = false;
	let isImageBitmap = false;
	if (pixels.data instanceof Uint8Array) isPixelData$1 = true;
	else if (typeof ImageData !== "undefined" && pixels instanceof ImageData) isImageData = true;
	else if (typeof HTMLVideoElement !== "undefined" && pixels instanceof HTMLVideoElement) isVideo = true;
	else if (typeof HTMLImageElement !== "undefined" && pixels instanceof HTMLImageElement) isImage = true;
	else if (pixels.getContext != null) isCanvasLike = true;
	else if (typeof ImageBitmap !== "undefined" && pixels instanceof ImageBitmap) isImageBitmap = true;
	else throw new Error(`pixels passed to tf.browser.fromPixels() must be either an HTMLVideoElement, HTMLImageElement, HTMLCanvasElement, ImageData in browser, or OffscreenCanvas, ImageData in webworker or {data: Uint32Array, width: number, height: number}, but was ${pixels.constructor.name}`);
	if (isVideo) {
		if (isVideo && pixels.readyState < 2) throw new Error("The video element has not loaded data yet. Please wait for `loadeddata` event on the <video> element.");
	}
	if (getKernel(FromPixels, ENGINE.backendName) != null) {
		const inputs = { pixels };
		const attrs = { numChannels };
		return ENGINE.runKernel(FromPixels, inputs, attrs);
	}
	const [width, height] = isVideo ? [pixels.videoWidth, pixels.videoHeight] : [pixels.width, pixels.height];
	let vals;
	if (isCanvasLike) vals = pixels.getContext("2d").getImageData(0, 0, width, height).data;
	else if (isImageData || isPixelData$1) vals = pixels.data;
	else if (isImage || isVideo || isImageBitmap) {
		if (fromPixels2DContext == null) if (typeof document === "undefined") if (typeof OffscreenCanvas !== "undefined" && typeof OffscreenCanvasRenderingContext2D !== "undefined") fromPixels2DContext = new OffscreenCanvas(1, 1).getContext("2d");
		else throw new Error("Cannot parse input in current context. Reason: OffscreenCanvas Context2D rendering is not supported.");
		else fromPixels2DContext = document.createElement("canvas").getContext("2d");
		fromPixels2DContext.canvas.width = width;
		fromPixels2DContext.canvas.height = height;
		fromPixels2DContext.drawImage(pixels, 0, 0, width, height);
		vals = fromPixels2DContext.getImageData(0, 0, width, height).data;
	}
	let values;
	if (numChannels === 4) values = new Int32Array(vals);
	else {
		const numPixels = width * height;
		values = new Int32Array(numPixels * numChannels);
		for (let i = 0; i < numPixels; i++) for (let channel = 0; channel < numChannels; ++channel) values[i * numChannels + channel] = vals[i * 4 + channel];
	}
	return tensor3d(values, [
		height,
		width,
		numChannels
	], "int32");
}
function isPixelData(pixels) {
	return pixels != null && pixels.data instanceof Uint8Array;
}
function isImageBitmapFullySupported() {
	return typeof window !== "undefined" && typeof ImageBitmap !== "undefined" && window.hasOwnProperty("createImageBitmap");
}
function isNonEmptyPixels(pixels) {
	return pixels != null && pixels.width !== 0 && pixels.height !== 0;
}
function canWrapPixelsToImageBitmap(pixels) {
	return isImageBitmapFullySupported() && !(pixels instanceof ImageBitmap) && isNonEmptyPixels(pixels) && !isPixelData(pixels);
}
/**
* Creates a `tf.Tensor` from an image in async way.
*
* ```js
* const image = new ImageData(1, 1);
* image.data[0] = 100;
* image.data[1] = 150;
* image.data[2] = 200;
* image.data[3] = 255;
*
* (await tf.browser.fromPixelsAsync(image)).print();
* ```
* This API is the async version of fromPixels. The API will first
* check |WRAP_TO_IMAGEBITMAP| flag, and try to wrap the input to
* imageBitmap if the flag is set to true.
*
* @param pixels The input image to construct the tensor from. The
* supported image types are all 4-channel. You can also pass in an image
* object with following attributes:
* `{data: Uint8Array; width: number; height: number}`
* @param numChannels The number of channels of the output tensor. A
* numChannels value less than 4 allows you to ignore channels. Defaults to
* 3 (ignores alpha channel of input image).
*
* @doc {heading: 'Browser', namespace: 'browser', ignoreCI: true}
*/
async function fromPixelsAsync(pixels, numChannels = 3) {
	let inputs = null;
	if (env().getBool("WRAP_TO_IMAGEBITMAP") && canWrapPixelsToImageBitmap(pixels)) {
		let imageBitmap;
		try {
			imageBitmap = await createImageBitmap(pixels, { premultiplyAlpha: "none" });
		} catch (e) {
			imageBitmap = null;
		}
		if (imageBitmap != null && imageBitmap.width === pixels.width && imageBitmap.height === pixels.height) inputs = imageBitmap;
		else inputs = pixels;
	} else inputs = pixels;
	return fromPixels_(inputs, numChannels);
}
/**
* Draws a `tf.Tensor` of pixel values to a byte array or optionally a
* canvas.
*
* When the dtype of the input is 'float32', we assume values in the range
* [0-1]. Otherwise, when input is 'int32', we assume values in the range
* [0-255].
*
* Returns a promise that resolves when the canvas has been drawn to.
*
* @param img A rank-2 tensor with shape `[height, width]`, or a rank-3 tensor
* of shape `[height, width, numChannels]`. If rank-2, draws grayscale. If
* rank-3, must have depth of 1, 3 or 4. When depth of 1, draws
* grayscale. When depth of 3, we draw with the first three components of
* the depth dimension corresponding to r, g, b and alpha = 1. When depth of
* 4, all four components of the depth dimension correspond to r, g, b, a.
* @param canvas The canvas to draw to.
*
* @doc {heading: 'Browser', namespace: 'browser'}
*/
async function toPixels(img, canvas) {
	let $img = convertToTensor(img, "img", "toPixels");
	if (!(img instanceof Tensor)) {
		const originalImgTensor = $img;
		$img = cast(originalImgTensor, "int32");
		originalImgTensor.dispose();
	}
	if ($img.rank !== 2 && $img.rank !== 3) throw new Error(`toPixels only supports rank 2 or 3 tensors, got rank ${$img.rank}.`);
	const [height, width] = $img.shape.slice(0, 2);
	const depth = $img.rank === 2 ? 1 : $img.shape[2];
	if (depth > 4 || depth === 2) throw new Error(`toPixels only supports depth of size 1, 3 or 4 but got ${depth}`);
	if ($img.dtype !== "float32" && $img.dtype !== "int32") throw new Error(`Unsupported type for toPixels: ${$img.dtype}. Please use float32 or int32 tensors.`);
	const data = await $img.data();
	const multiplier = $img.dtype === "float32" ? 255 : 1;
	const bytes = new Uint8ClampedArray(width * height * 4);
	for (let i = 0; i < height * width; ++i) {
		const rgba = [
			0,
			0,
			0,
			255
		];
		for (let d = 0; d < depth; d++) {
			const value = data[i * depth + d];
			if ($img.dtype === "float32") {
				if (value < 0 || value > 1) throw new Error(`Tensor values for a float32 Tensor must be in the range [0 - 1] but encountered ${value}.`);
			} else if ($img.dtype === "int32") {
				if (value < 0 || value > 255) throw new Error(`Tensor values for a int32 Tensor must be in the range [0 - 255] but encountered ${value}.`);
			}
			if (depth === 1) {
				rgba[0] = value * multiplier;
				rgba[1] = value * multiplier;
				rgba[2] = value * multiplier;
			} else rgba[d] = value * multiplier;
		}
		const j = i * 4;
		bytes[j + 0] = Math.round(rgba[0]);
		bytes[j + 1] = Math.round(rgba[1]);
		bytes[j + 2] = Math.round(rgba[2]);
		bytes[j + 3] = Math.round(rgba[3]);
	}
	if (canvas != null) {
		canvas.width = width;
		canvas.height = height;
		const ctx = canvas.getContext("2d");
		const imageData = new ImageData(bytes, width, height);
		ctx.putImageData(imageData, 0, 0);
	}
	if ($img !== img) $img.dispose();
	return bytes;
}
const fromPixels = op({ fromPixels_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/gather_nd_util.js
var gather_nd_util_exports = /* @__PURE__ */ __export({ prepareAndValidate: () => prepareAndValidate });
/**
* Validate gather nd inputs.
*
* @param tensor The tensor contains the source values.
* @param indices The tensor contains the indices to slice the source.
*
* @returns [resultShape, numUpdates, sliceSize, strides]
*/
function prepareAndValidate(tensor$1, indices) {
	const tensorRank = tensor$1.shape.length;
	const indicesRank = indices.shape.length;
	if (tensorRank < 1) throw new Error(`tf.gatherND() expects the input to be rank 1 or higher, but the rank was ${tensorRank}.`);
	if (indicesRank < 1) throw new Error(`tf.gatherND() expects the indices to be rank 1 or higher, but the rank was ${indicesRank}.`);
	if (indices.dtype !== "int32") throw new Error(`tf.gatherND() expects the indices to be int32 type, but the dtype was ${indices.dtype}.`);
	if (indices.shape[indicesRank - 1] > tensorRank) throw new Error(`index innermost dimension length must be <= tensor rank; saw: ${indices.shape[indicesRank - 1]} vs. ${tensorRank}`);
	if (sizeFromShape(tensor$1.shape) === 0) throw new Error(`Requested more than 0 entries, but input is empty. Input shape: ${tensor$1.shape}.`);
	const indicesShape = indices.shape;
	const sliceRank = indicesShape[indicesShape.length - 1];
	let nResult = 1;
	for (let i = 0; i < indicesShape.length - 1; ++i) nResult *= indicesShape[i];
	const inputShape = tensor$1.shape;
	const resultShape = indicesShape.slice();
	resultShape.pop();
	let sliceSize = 1;
	for (let i = sliceRank; i < tensorRank; ++i) {
		sliceSize *= inputShape[i];
		resultShape.push(inputShape[i]);
	}
	const strides = [...computeStrides(tensor$1.shape).map((stride) => stride / sliceSize), 1].slice(0, sliceRank);
	return [
		resultShape,
		nResult,
		sliceSize,
		strides
	];
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/scatter_nd_util.js
var scatter_nd_util_exports = /* @__PURE__ */ __export({
	calculateShapes: () => calculateShapes,
	validateInput: () => validateInput$1,
	validateUpdateShape: () => validateUpdateShape
});
/**
* Check whether updates.shape = indices.shape[:batchDim] +
* shape[sliceDim:]
*
* @param x The input tensor.
*/
function validateUpdateShape(shape, indices, updates) {
	const sliceDim = indices.rank > 1 ? indices.shape[indices.rank - 1] : 1;
	const batchDim = indices.rank > 1 ? indices.rank - 1 : 1;
	const shapeError = `Must have updates.shape = indices.shape[:batchDim] + shape[sliceDim:], got updates.shape: ${updates.shape}, indices.shape: ${indices.shape}, shape: ${shape}, sliceDim: ${sliceDim}, and batchDim: ${batchDim}.`;
	if (updates.rank < batchDim) throw new Error(shapeError + ` update.rank < ${batchDim}. `);
	if (shape.length < sliceDim + (updates.rank - batchDim)) throw new Error(shapeError + ` Output shape length < ${sliceDim + (updates.rank - batchDim)}`);
	if (updates.rank !== batchDim + shape.length - sliceDim) throw new Error(shapeError + ` update.rank != ${batchDim + shape.length - sliceDim}`);
	for (let d = 0; d < batchDim; ++d) if (updates.shape[d] !== indices.shape[d]) throw new Error(shapeError + ` updates.shape[${d}] (${updates.shape[d]}) != indices.shape[${d}] (${indices.shape[d]}).`);
	for (let d = 0; d < updates.rank - batchDim; ++d) if (updates.shape[d + batchDim] !== shape[d + sliceDim]) throw new Error(shapeError + ` updates.shape[${d + batchDim}] (${updates.shape[d + batchDim]}) != shape[${d + batchDim}] (${shape[d + batchDim]})`);
}
/**
* Validate scatter nd inputs.
*
* @param update The tensor contains the update values.
* @param indices The tensor contains the indices for the update values.
* @param shape The shape of the output tensor.
*/
function validateInput$1(updates, indices, shape) {
	if (indices.rank < 1) throw new Error(`tf.scatterND() expects the indices to be rank 1 or higher, but the rank was ${indices.rank}.`);
	if (updates.rank < 1) throw new Error(`tf.scatterND() expects the updates to be rank 1 or higher, but the rank was ${updates.rank}.`);
	if (indices.dtype !== "int32") throw new Error(`The dtype of 'indices' should be int32, but got dtype: ${indices.dtype}`);
	if (shape.length < 1) throw new Error(`Output rank must be greater or equal to 1, but got shape: ${shape}`);
	if (shape.length === 0) {
		if (indices.size === 0) throw new Error(`Indices specified for empty output. indices shape: ${indices.shape}`);
		if (updates.size === 0) throw new Error(`Updates specified for empty output. updates shape: ${updates.shape}`);
	}
	validateUpdateShape(shape, indices, updates);
}
/**
* Calculate the shape information for the output.
*
* @param update The tensor contains the update values.
* @param indices The tensor contains the indices for the update values.
* @param shape The shape of the output tensor.
*
* @returns ScatterShapeInfo
*/
function calculateShapes(updates, indices, shape) {
	const indicesRank = indices.shape.length;
	const sliceRank = indicesRank > 1 ? indices.shape[indicesRank - 1] : 1;
	const totalNd = shape.length;
	let sliceSize = 1;
	for (let i = sliceRank; i < totalNd; ++i) sliceSize *= shape[i];
	const safeSliceDim = sliceRank < 1 ? 1 : sliceRank;
	const numUpdates = sizeFromShape(indices.shape) / safeSliceDim;
	const strides = [...computeStrides(shape.slice(0, sliceRank)), 1];
	const outputSize = sizeFromShape(shape);
	return {
		sliceRank,
		numUpdates,
		sliceSize,
		strides,
		outputSize
	};
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/slice_util.js
var slice_util_exports = /* @__PURE__ */ __export({
	assertParamsValid: () => assertParamsValid,
	computeFlatOffset: () => computeFlatOffset,
	computeOutShape: () => computeOutShape$2,
	getNormalizedAxes: () => getNormalizedAxes,
	isSliceContinous: () => isSliceContinous,
	maskToAxes: () => maskToAxes,
	parseSliceParams: () => parseSliceParams,
	sliceInfo: () => sliceInfo,
	startForAxis: () => startForAxis,
	startIndicesWithElidedDims: () => startIndicesWithElidedDims,
	stopForAxis: () => stopForAxis,
	stopIndicesWithElidedDims: () => stopIndicesWithElidedDims,
	stridesForAxis: () => stridesForAxis,
	stridesWithElidedDims: () => stridesWithElidedDims
});
var NEW_AXIS = -2;
var SHRINK_AXIS = -1;
function assertParamsValid(input, begin, size) {
	const inputRank = input.shape.length;
	assert(inputRank === begin.length, () => `Error in slice${inputRank}D: Length of begin ${begin} must match the rank of the array (${inputRank}).`);
	assert(inputRank === size.length, () => `Error in slice${inputRank}D: Length of size ${size} must match the rank of the array (${inputRank}).`);
	for (let i = 0; i < inputRank; ++i) assert(begin[i] + size[i] <= input.shape[i], () => `Error in slice${inputRank}D: begin[${i}] + size[${i}] (${begin[i] + size[i]}) would overflow input.shape[${i}] (${input.shape[i]})`);
}
/** Converts a binary mask to an array of axes. Used in stridedSlice(). */
function maskToAxes(mask) {
	const axes = [];
	let axis = 0;
	while (mask > 0) {
		if (mask & 1) axes.push(axis);
		mask /= 2;
		axis++;
	}
	return axes;
}
/** Computes the output shape given the strided slice params. */
function computeOutShape$2(begin, end, strides) {
	const size = [];
	for (let axis = 0; axis < begin.length; axis++) size[axis] = Math.ceil((end[axis] - begin[axis]) / strides[axis]);
	return size;
}
function stridesWithElidedDims(strides, ellipsisInsertionIndex, numElidedAxes, inputShape) {
	const newStrides = [...strides];
	for (let i = newStrides.length; i < inputShape.length; i++) newStrides.push(1);
	for (let i = 0; i < numElidedAxes; i++) if (i === 0) newStrides[ellipsisInsertionIndex] = 1;
	else {
		newStrides.splice(ellipsisInsertionIndex, 0, 1);
		newStrides.pop();
	}
	return newStrides;
}
function unnormalizeAxis(ellipsisInsertionIndex, numElidedAxes, normalizedAxis) {
	if (normalizedAxis <= ellipsisInsertionIndex) return normalizedAxis;
	return normalizedAxis - (numElidedAxes - 1);
}
function getElidedAxes(numElidedAxes, ellipsisInsertionIndex) {
	const elidedAxes = [];
	for (let i = 0; i < numElidedAxes; i++) elidedAxes.push(ellipsisInsertionIndex + i);
	return elidedAxes;
}
function getNormalizedAxes(inputShape, ellipsisAxes, numInterpolatedAxes, begin, end, strides, beginMask, endMask, ellipsisMask) {
	const inputRank = inputShape.length;
	let normalizedBegin = new Array(inputRank), normalizedEnd = new Array(inputRank), normalizedStrides = new Array(inputRank);
	if (ellipsisAxes.length && numInterpolatedAxes > 0) {
		const fullIndex = ellipsisAxes[0];
		const numElidedAxes = numInterpolatedAxes + 1;
		normalizedBegin = startIndicesWithElidedDims(beginMask, fullIndex, numElidedAxes, begin, inputShape);
		normalizedEnd = stopIndicesWithElidedDims(endMask, fullIndex, numElidedAxes, end, inputShape);
		normalizedStrides = stridesWithElidedDims(strides, fullIndex, numElidedAxes, inputShape);
	} else for (let axis = 0; axis < inputRank; axis++) {
		normalizedBegin[axis] = startForAxis(beginMask, begin, strides, inputShape, axis, ellipsisMask);
		normalizedEnd[axis] = stopForAxis(endMask, end, strides, inputShape, axis, ellipsisMask);
		normalizedStrides[axis] = stridesForAxis(strides, axis, ellipsisMask);
	}
	return {
		begin: normalizedBegin,
		end: normalizedEnd,
		strides: normalizedStrides
	};
}
function startIndicesWithElidedDims(beginMask, ellipsisInsertionIndex, numElidedAxes, originalBegin, inputShape) {
	const newIndices = [...inputShape];
	const elidedAxes = getElidedAxes(numElidedAxes, ellipsisInsertionIndex);
	for (let axis = 0; axis < newIndices.length; axis++) if (elidedAxes.indexOf(axis) > -1) newIndices[axis] = 0;
	else {
		const originalAxis = unnormalizeAxis(ellipsisInsertionIndex, numElidedAxes, axis);
		let originalValue = originalBegin[originalAxis];
		if (beginMask & 1 << originalAxis) originalValue = 0;
		newIndices[axis] = originalValue;
	}
	return newIndices;
}
function stopIndicesWithElidedDims(endMask, ellipsisInsertionIndex, numElidedAxes, originalEnd, inputShape) {
	const newIndices = [...inputShape];
	const elidedAxes = getElidedAxes(numElidedAxes, ellipsisInsertionIndex);
	for (let axis = 0; axis < newIndices.length; axis++) if (elidedAxes.indexOf(axis) > -1) newIndices[axis] = Number.MAX_SAFE_INTEGER;
	else {
		const originalAxis = unnormalizeAxis(ellipsisInsertionIndex, numElidedAxes, axis);
		let originalValue = originalEnd[originalAxis];
		if (endMask & 1 << originalAxis) originalValue = Number.MAX_SAFE_INTEGER;
		newIndices[axis] = originalValue;
	}
	for (let i = 0; i < newIndices.length; i++) {
		const axisSize = inputShape[i];
		if (newIndices[i] < 0) newIndices[i] += axisSize;
		newIndices[i] = clamp(0, newIndices[i], inputShape[i]);
	}
	return newIndices;
}
function stridesForAxis(strides, axis, ellipsisMask) {
	let stride = strides[axis];
	if (ellipsisMask & 1 << axis || stride == null) stride = 1;
	return stride;
}
function startForAxis(beginMask, startIndices, strides, inputShape, axis, ellipsisMask) {
	let start = startIndices[axis];
	const stride = strides[axis] || 1;
	if (beginMask & 1 << axis || ellipsisMask & 1 << axis || start == null) if (stride > 0) start = Number.MIN_SAFE_INTEGER;
	else start = Number.MAX_SAFE_INTEGER;
	const axisSize = inputShape[axis];
	if (start < 0) start += axisSize;
	start = clamp(0, start, axisSize - 1);
	return start;
}
function stopForAxis(endMask, stopIndices, strides, inputShape, axis, ellipsisMask) {
	let stop = stopIndices[axis];
	const stride = strides[axis] || 1;
	if (endMask & 1 << axis || ellipsisMask & 1 << axis || stop == null) if (stride > 0) stop = Number.MAX_SAFE_INTEGER;
	else stop = Number.MIN_SAFE_INTEGER;
	const axisSize = inputShape[axis];
	if (stop < 0) stop += axisSize;
	if (stride > 0) stop = clamp(0, stop, axisSize);
	else stop = clamp(-1, stop, axisSize - 1);
	return stop;
}
/**
* Returns true if the slice occupies a continous set of elements in the
* 'flat' space.
*/
function isSliceContinous(shape, begin, size) {
	let firstNonOneAxis = size.length;
	for (let i = 0; i < size.length; i++) if (size[i] > 1) {
		firstNonOneAxis = i;
		break;
	}
	for (let i = firstNonOneAxis + 1; i < size.length; i++) if (begin[i] > 0 || size[i] !== shape[i]) return false;
	return true;
}
function computeFlatOffset(begin, strides) {
	let flatOffset = begin.length > 0 ? begin[begin.length - 1] : 1;
	for (let i = 0; i < begin.length - 1; i++) flatOffset += begin[i] * strides[i];
	return flatOffset;
}
function parseSliceParams(x, begin, size) {
	let begin_;
	const xRank = x.shape.length;
	if (typeof begin === "number") begin_ = [begin, ...new Array(xRank - 1).fill(0)];
	else if (begin.length < xRank) begin_ = begin.concat(new Array(xRank - begin.length).fill(0));
	else begin_ = begin.slice();
	begin_.forEach((d) => {
		assert(d !== -1, () => "slice() does not support negative begin indexing.");
	});
	let size_;
	if (size == null) size_ = new Array(xRank).fill(-1);
	else if (typeof size === "number") size_ = [size, ...new Array(xRank - 1).fill(-1)];
	else if (size.length < xRank) size_ = size.concat(new Array(xRank - size.length).fill(-1));
	else size_ = size;
	size_ = size_.map((d, i) => {
		if (d >= 0) return d;
		else {
			assert(d === -1, () => `Negative size values should be exactly -1 but got ${d} for the slice() size at index ${i}.`);
			return x.shape[i] - begin_[i];
		}
	});
	return [begin_, size_];
}
function sliceInfo(xShape, begin, end, strides, beginMask, endMask, ellipsisMask, newAxisMask, shrinkAxisMask) {
	let stridesNonNull;
	if (strides == null) {
		stridesNonNull = new Array(begin.length);
		stridesNonNull.fill(1);
	} else stridesNonNull = strides;
	if (ellipsisMask != null && (ellipsisMask & ellipsisMask - 1) !== 0) throw new Error("Multiple ellipses in slice is not allowed.");
	let ellipsisSeen = false;
	const sparseSpec = {
		dims: stridesNonNull.length,
		numAddAxisAfterEllipsis: 0,
		begin: begin.slice(),
		end: end.slice(),
		strides: stridesNonNull.slice(),
		beginMask,
		endMask,
		ellipsisMask,
		newAxisMask,
		shrinkAxisMask
	};
	for (let i = 0; i < sparseSpec.dims; i++) {
		if (ellipsisSeen && (1 << i & newAxisMask) !== 0) sparseSpec.numAddAxisAfterEllipsis++;
		if (1 << i & ellipsisMask) ellipsisSeen = true;
	}
	if (!ellipsisSeen) {
		sparseSpec.ellipsisMask |= 1 << sparseSpec.dims;
		sparseSpec.dims++;
	}
	const denseSpec = {
		dims: xShape.length,
		beginMask: 0,
		endMask: 0,
		beginValid: false,
		endValid: false
	};
	buildDenseSpec(sparseSpec, denseSpec);
	let isIdentity = true;
	let sliceDim0 = true;
	let isSimpleSlice = true;
	const processingShape = [];
	const finalShape = [];
	for (let i = 0; i < xShape.length; ++i) {
		if (denseSpec.strides[i] === 0) throw Error(`strides[${i}] must be non-zero`);
		const shrinkI = !!(denseSpec.shrinkAxisMask & 1 << i);
		const dimI = xShape[i];
		if (dimI === -1) {
			processingShape.push(shrinkI ? 1 : -1);
			continue;
		}
		const masks = [denseSpec.beginMask & 1 << i, denseSpec.endMask & 1 << i];
		const validRange = [denseSpec.strides[i] > 0 ? 0 : -1, denseSpec.strides[i] > 0 ? dimI : dimI - 1];
		if (shrinkI && denseSpec.strides[i] <= 0) throw Error("only stride 1 allowed on non-range indexing.");
		isSimpleSlice = isSimpleSlice && denseSpec.strides[i] === 1;
		const beginAndEndMasked = !!(denseSpec.beginMask & 1 << i && denseSpec.endMask & 1 << i);
		if (denseSpec.beginValid && denseSpec.endValid) {
			if (shrinkI) {
				const xFwd = denseSpec.begin[i] < 0 ? dimI + denseSpec.begin[i] : denseSpec.begin[i];
				denseSpec.begin[i] = xFwd;
				denseSpec.end[i] = denseSpec.begin[i] + 1;
				if (xFwd < 0 || xFwd >= dimI) throw Error(`slice index ${denseSpec.begin[i]} of dimension ${i} out of bounds.`);
			} else {
				denseSpec.begin[i] = canonical(denseSpec.begin[i], 0, denseSpec.strides[i], dimI, masks, validRange);
				denseSpec.end[i] = canonical(denseSpec.end[i], 1, denseSpec.strides[i], dimI, masks, validRange);
			}
			const takeAllInDimension = denseSpec.strides[i] === 1 && denseSpec.begin[i] === 0 && denseSpec.end[i] === dimI;
			isIdentity = isIdentity && takeAllInDimension;
			sliceDim0 = sliceDim0 && (i === 0 && denseSpec.strides[i] === 1 || takeAllInDimension);
		} else {
			isIdentity = isIdentity && denseSpec.strides[i] === 1 && beginAndEndMasked;
			sliceDim0 = sliceDim0 && (i === 0 && denseSpec.strides[i] === 1 || beginAndEndMasked);
		}
		let intervalLength;
		let knownInterval = false;
		if (denseSpec.beginValid && denseSpec.endValid) {
			intervalLength = denseSpec.end[i] - denseSpec.begin[i];
			knownInterval = true;
		} else if (shrinkI) {
			intervalLength = 1;
			knownInterval = true;
		} else if (beginAndEndMasked) {
			if (dimI >= 0) {
				if (denseSpec.strides[i] < 0) intervalLength = -dimI;
				else intervalLength = dimI;
				knownInterval = true;
			}
		}
		if (knownInterval) {
			let sizeI;
			if (intervalLength === 0 || intervalLength < 0 !== denseSpec.strides[i] < 0) sizeI = 0;
			else sizeI = Math.trunc(intervalLength / denseSpec.strides[i]) + (intervalLength % denseSpec.strides[i] !== 0 ? 1 : 0);
			processingShape.push(sizeI);
		} else processingShape.push(-1);
	}
	for (let denseDim = 0; denseDim < denseSpec.finalShapeGatherIndices.length; ++denseDim) {
		const gatherIndex = denseSpec.finalShapeGatherIndices[denseDim];
		if (gatherIndex >= 0) finalShape.push(processingShape[gatherIndex]);
		else if (gatherIndex === NEW_AXIS) finalShape.push(1);
	}
	return {
		finalShapeSparse: finalShape.filter((dim, i) => denseSpec.finalShapeGatherIndices[i] !== NEW_AXIS),
		finalShape,
		isIdentity,
		sliceDim0,
		isSimpleSlice,
		begin: denseSpec.begin,
		end: denseSpec.end,
		strides: denseSpec.strides
	};
}
function buildDenseSpec(sparse$1, dense) {
	dense.beginMask = 0;
	dense.endMask = 0;
	dense.shrinkAxisMask = 0;
	let fullIndex = 0;
	dense.beginValid = sparse$1.begin != null;
	dense.endValid = sparse$1.end != null;
	dense.begin = new Array(dense.dims);
	dense.end = new Array(dense.dims);
	dense.strides = new Array(dense.dims);
	dense.finalShapeGatherIndices = [];
	dense.finalShapeGatherIndicesSparse = [];
	dense.inputShapeGatherIndicesSparse = new Array(dense.dims);
	for (let i = 0; i < sparse$1.dims; i++) if (1 << i & sparse$1.ellipsisMask) {
		const nextIndex = Math.min(dense.dims - (sparse$1.dims - i) + 1 + sparse$1.numAddAxisAfterEllipsis, dense.dims);
		for (; fullIndex < nextIndex; fullIndex++) {
			dense.begin[fullIndex] = 0;
			dense.end[fullIndex] = 0;
			dense.strides[fullIndex] = 1;
			dense.beginMask |= 1 << fullIndex;
			dense.endMask |= 1 << fullIndex;
			dense.finalShapeGatherIndices.push(fullIndex);
			dense.finalShapeGatherIndicesSparse.push(-1);
			dense.inputShapeGatherIndicesSparse[fullIndex] = i;
		}
	} else if (1 << i & sparse$1.newAxisMask) {
		dense.finalShapeGatherIndices.push(NEW_AXIS);
		dense.finalShapeGatherIndicesSparse.push(-1);
	} else {
		if (fullIndex === dense.begin.length) throw Error(`Index out of range using input dim ${fullIndex}; input has only ${dense.dims} dims, ${dense.begin.length}.`);
		if (sparse$1.begin != null) dense.begin[fullIndex] = sparse$1.begin[i];
		if (sparse$1.end != null) dense.end[fullIndex] = sparse$1.end[i];
		dense.strides[fullIndex] = sparse$1.strides[i];
		if (sparse$1.beginMask & 1 << i) dense.beginMask |= 1 << fullIndex;
		if (sparse$1.endMask & 1 << i) dense.endMask |= 1 << fullIndex;
		if (sparse$1.shrinkAxisMask & 1 << i) {
			dense.finalShapeGatherIndices.push(SHRINK_AXIS);
			dense.finalShapeGatherIndicesSparse.push(-1);
			dense.shrinkAxisMask |= 1 << fullIndex;
		} else {
			dense.finalShapeGatherIndices.push(fullIndex);
			dense.finalShapeGatherIndicesSparse.push(i);
		}
		dense.inputShapeGatherIndicesSparse[fullIndex] = i;
		fullIndex++;
	}
}
function canonical(x, c, strideI, dimI, masks, validRange) {
	if (masks[c]) return strideI > 0 ? validRange[c] : validRange[c + 1 & 1];
	else {
		const xFwd = x < 0 ? dimI + x : x;
		return xFwd < validRange[0] ? validRange[0] : xFwd > validRange[1] ? validRange[1] : xFwd;
	}
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/serialization.js
var serialization_exports = /* @__PURE__ */ __export({
	Serializable: () => Serializable,
	SerializationMap: () => SerializationMap,
	registerClass: () => registerClass
});
/**
* Serializable defines the serialization contract.
*
* TFJS requires serializable classes to return their className when asked
* to avoid issues with minification.
*/
var Serializable = class {
	/**
	* Return the class name for this class to use in serialization contexts.
	*
	* Generally speaking this will be the same thing that constructor.name
	* would have returned.  However, the class name needs to be robust
	* against minification for serialization/deserialization to work properly.
	*
	* There's also places such as initializers.VarianceScaling, where
	* implementation details between different languages led to different
	* class hierarchies and a non-leaf node is used for serialization purposes.
	*/
	getClassName() {
		return this.constructor.className;
	}
	/**
	* Creates an instance of T from a ConfigDict.
	*
	* This works for most descendants of serializable.  A few need to
	* provide special handling.
	* @param cls A Constructor for the class to instantiate.
	* @param config The Configuration for the object.
	*/
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config);
	}
};
/**
* Maps string keys to class constructors.
*
* Used during (de)serialization from the cross-language JSON format, which
* requires the class name in the serialization format matches the class
* names as used in Python, should it exist.
*/
var SerializationMap = class SerializationMap {
	constructor() {
		this.classNameMap = {};
	}
	/**
	* Returns the singleton instance of the map.
	*/
	static getMap() {
		if (SerializationMap.instance == null) SerializationMap.instance = new SerializationMap();
		return SerializationMap.instance;
	}
	/**
	* Registers the class as serializable.
	*/
	static register(cls) {
		SerializationMap.getMap().classNameMap[cls.className] = [cls, cls.fromConfig];
	}
};
/**
* Register a class with the serialization map of TensorFlow.js.
*
* This is often used for registering custom Layers, so they can be
* serialized and deserialized.
*
* Example:
*
* ```js
* class MyCustomLayer extends tf.layers.Layer {
*   static className = 'MyCustomLayer';
*
*   constructor(config) {
*     super(config);
*   }
* }
* tf.serialization.registerClass(MyCustomLayer);
* ```
*
* @param cls The class to be registered. It must have a public static member
*   called `className` defined and the value must be a non-empty string.
*
* @doc {heading: 'Models', subheading: 'Serialization', ignoreCI: true}
*/
function registerClass(cls) {
	assert(cls.className != null, () => "Class being registered does not have the static className property defined.");
	assert(typeof cls.className === "string", () => `className is required to be a string, but got type ` + typeof cls.className);
	assert(cls.className.length > 0, () => "Class being registered has an empty-string as its className, which is disallowed.");
	SerializationMap.register(cls);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/test_util.js
var test_util_exports = /* @__PURE__ */ __export({
	TEST_EPSILON_FLOAT16: () => TEST_EPSILON_FLOAT16,
	encodeStrings: () => encodeStrings,
	expectArrayBuffersEqual: () => expectArrayBuffersEqual,
	expectArraysClose: () => expectArraysClose,
	expectArraysEqual: () => expectArraysEqual,
	expectNumbersClose: () => expectNumbersClose,
	expectPromiseToFail: () => expectPromiseToFail,
	expectValuesInRange: () => expectValuesInRange,
	testEpsilon: () => testEpsilon
});
var TEST_EPSILON_FLOAT32 = .001;
const TEST_EPSILON_FLOAT16 = .1;
function expectArraysClose(actual, expected, epsilon) {
	if (epsilon == null) epsilon = testEpsilon();
	return expectArraysPredicate(actual, expected, (a, b) => areClose(a, b, epsilon));
}
function testEpsilon() {
	return ENGINE.backend.floatPrecision() === 32 ? TEST_EPSILON_FLOAT32 : TEST_EPSILON_FLOAT16;
}
function expectArraysPredicate(actual, expected, predicate) {
	let checkClassType = true;
	if (isTypedArray(actual) || isTypedArray(expected)) checkClassType = false;
	if (isTypedArray(actual) && isTypedArray(expected)) checkClassType = true;
	if (checkClassType) {
		const aType = actual.constructor.name;
		const bType = expected.constructor.name;
		if (aType !== bType) throw new Error(`Arrays are of different type. Actual: ${aType}. Expected: ${bType}`);
	}
	if (Array.isArray(actual) && Array.isArray(expected)) {
		const actualShape = inferShape(actual);
		const expectedShape = inferShape(expected);
		if (!arraysEqual(actualShape, expectedShape)) throw new Error(`Arrays have different shapes. Actual: [${actualShape}]. Expected: [${expectedShape}]`);
	}
	const actualFlat = isTypedArray(actual) ? actual : flatten(actual);
	const expectedFlat = isTypedArray(expected) ? expected : flatten(expected);
	if (actualFlat.length !== expectedFlat.length) throw new Error(`Arrays have different lengths actual: ${actualFlat.length} vs expected: ${expectedFlat.length}.\nActual:   ${actualFlat}.\nExpected: ${expectedFlat}.`);
	for (let i = 0; i < expectedFlat.length; ++i) {
		const a = actualFlat[i];
		const e = expectedFlat[i];
		if (!predicate(a, e)) throw new Error(`Arrays differ: actual[${i}] = ${a}, expected[${i}] = ${e}.\nActual:   ${actualFlat}.\nExpected: ${expectedFlat}.`);
	}
}
function expectPromiseToFail(fn, done) {
	fn().then(() => done.fail(), () => done());
}
function expectArraysEqual(actual, expected) {
	const exp$1 = typeof expected === "string" || typeof expected === "number" || typeof expected === "boolean" ? [expected] : expected;
	if (isString(actual) || isString(actual[0]) || isString(expected) || isString(expected[0])) return expectArraysPredicate(actual, exp$1, (a, b) => a == b);
	return expectArraysPredicate(actual, expected, (a, b) => areClose(a, b, 0));
}
function expectNumbersClose(a, e, epsilon) {
	if (epsilon == null) epsilon = testEpsilon();
	if (!areClose(a, e, epsilon)) throw new Error(`Numbers differ: actual === ${a}, expected === ${e}`);
}
function areClose(a, e, epsilon) {
	if (!isFinite(a) && !isFinite(e)) return true;
	if (isNaN(a) || isNaN(e) || Math.abs(a - e) > epsilon) return false;
	return true;
}
function expectValuesInRange(actual, low, high) {
	for (let i = 0; i < actual.length; i++) if (actual[i] < low || actual[i] > high) throw new Error(`Value out of range:${actual[i]} low: ${low}, high: ${high}`);
}
function expectArrayBuffersEqual(actual, expected) {
	expect(new Float32Array(actual)).toEqual(new Float32Array(expected));
}
/** Encodes strings into utf-8 bytes. */
function encodeStrings(a) {
	for (let i = 0; i < a.length; i++) {
		const val = a[i];
		if (Array.isArray(val)) encodeStrings(val);
		else a[i] = encodeString(val);
	}
	return a;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/version.js
/** @license See the LICENSE file. */
var version = "3.11.0";

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/globals.js
/**
* Enables production mode which disables correctness checks in favor of
* performance.
*
* @doc {heading: 'Environment'}
*/
function enableProdMode() {
	env().set("PROD", true);
}
/**
* Enables debug mode which will log information about all executed kernels:
* the elapsed time of the kernel execution, as well as the rank, shape, and
* size of the output tensor.
*
* Debug mode will significantly slow down your application as it will
* download the result of every operation to the CPU. This should not be used in
* production. Debug mode does not affect the timing information of the kernel
* execution as we do not measure download time in the kernel execution time.
*
* See also: `tf.profile`, `tf.memory`.
*
* @doc {heading: 'Environment'}
*/
function enableDebugMode() {
	env().set("DEBUG", true);
}
/** Globally disables deprecation warnings */
function disableDeprecationWarnings() {
	env().set("DEPRECATION_WARNINGS_ENABLED", false);
	console.warn(`TensorFlow.js deprecation warnings have been disabled.`);
}
/** Warn users about deprecated functionality. */
function deprecationWarn(msg) {
	if (env().getBool("DEPRECATION_WARNINGS_ENABLED")) console.warn(msg + " You can disable deprecation warnings with tf.disableDeprecationWarnings().");
}
/**
* Dispose all variables kept in backend engine.
*
* @doc {heading: 'Environment'}
*/
function disposeVariables() {
	ENGINE.disposeVariables();
}
/**
* It returns the global engine that keeps track of all tensors and backends.
*
* @doc {heading: 'Environment'}
*/
function engine() {
	return ENGINE;
}
/**
* Returns memory info at the current time in the program. The result is an
* object with the following properties:
*
* - `numBytes`: Number of bytes allocated (undisposed) at this time.
* - `numTensors`: Number of unique tensors allocated.
* - `numDataBuffers`: Number of unique data buffers allocated
*   (undisposed) at this time, which is ≤ the number of tensors
*   (e.g. `a.reshape(newShape)` makes a new Tensor that shares the same
*   data buffer with `a`).
* - `unreliable`: True if the memory usage is unreliable. See `reasons` when
*    `unreliable` is true.
* - `reasons`: `string[]`, reasons why the memory is unreliable, present if
*    `unreliable` is true.
*
* WebGL Properties:
* - `numBytesInGPU`: Number of bytes allocated (undisposed) in the GPU only at
*     this time.
*
* @doc {heading: 'Performance', subheading: 'Memory'}
*/
function memory() {
	return ENGINE.memory();
}
/**
* Executes the provided function `f()` and returns a promise that resolves
* with information about the function's memory use:
* - `newBytes`: the number of new bytes allocated
* - `newTensors`: the number of new tensors created
* - `peakBytes`: the peak number of bytes allocated
* - `kernels`: an array of objects for each kernel involved that reports
* their input and output shapes, number of bytes used, and number of new
* tensors created.
* - `kernelNames`: an array of unique strings with just the names of the
* kernels in the `kernels` array.
*
* ```js
* const profile = await tf.profile(() => {
*   const x = tf.tensor1d([1, 2, 3]);
*   let x2 = x.square();
*   x2.dispose();
*   x2 = x.square();
*   x2.dispose();
*   return x;
* });
*
* console.log(`newBytes: ${profile.newBytes}`);
* console.log(`newTensors: ${profile.newTensors}`);
* console.log(`byte usage over all kernels: ${profile.kernels.map(k =>
* k.totalBytesSnapshot)}`);
* ```
*
*
* @doc {heading: 'Performance', subheading: 'Profile'}
*/
function profile(f) {
	return ENGINE.profile(f);
}
/**
* Executes the provided function `fn` and after it is executed, cleans up all
* intermediate tensors allocated by `fn` except those returned by `fn`.
* `fn` must not return a Promise (async functions not allowed). The returned
* result can be a complex object.
*
* Using this method helps avoid memory leaks. In general, wrap calls to
* operations in `tf.tidy` for automatic memory cleanup.
*
* NOTE: Variables do *not* get cleaned up when inside a tidy(). If you want to
* dispose variables, please use `tf.disposeVariables` or call dispose()
* directly on variables.
*
* ```js
* // y = 2 ^ 2 + 1
* const y = tf.tidy(() => {
*   // a, b, and one will be cleaned up when the tidy ends.
*   const one = tf.scalar(1);
*   const a = tf.scalar(2);
*   const b = a.square();
*
*   console.log('numTensors (in tidy): ' + tf.memory().numTensors);
*
*   // The value returned inside the tidy function will return
*   // through the tidy, in this case to the variable y.
*   return b.add(one);
* });
*
* console.log('numTensors (outside tidy): ' + tf.memory().numTensors);
* y.print();
* ```
*
* @param nameOrFn The name of the closure, or the function to execute.
*     If a name is provided, the 2nd argument should be the function.
*     If debug mode is on, the timing and the memory usage of the function
*     will be tracked and displayed on the console using the provided name.
* @param fn The function to execute.
*
* @doc {heading: 'Performance', subheading: 'Memory'}
*/
function tidy(nameOrFn, fn) {
	return ENGINE.tidy(nameOrFn, fn);
}
/**
* Disposes any `tf.Tensor`s found within the provided object.
*
* @param container an object that may be a `tf.Tensor` or may directly
*     contain `tf.Tensor`s, such as a `Tensor[]` or `{key: Tensor, ...}`. If
*     the object is not a `tf.Tensor` or does not contain `Tensors`, nothing
*     happens. In general it is safe to pass any object here, except that
*     `Promise`s are not supported.
*
* @doc {heading: 'Performance', subheading: 'Memory'}
*/
function dispose(container) {
	getTensorsInContainer(container).forEach((tensor$1) => tensor$1.dispose());
}
/**
* Keeps a `tf.Tensor` generated inside a `tf.tidy` from being disposed
* automatically.
*
* ```js
* let b;
* const y = tf.tidy(() => {
*   const one = tf.scalar(1);
*   const a = tf.scalar(2);
*
*   // b will not be cleaned up by the tidy. a and one will be cleaned up
*   // when the tidy ends.
*   b = tf.keep(a.square());
*
*   console.log('numTensors (in tidy): ' + tf.memory().numTensors);
*
*   // The value returned inside the tidy function will return
*   // through the tidy, in this case to the variable y.
*   return b.add(one);
* });
*
* console.log('numTensors (outside tidy): ' + tf.memory().numTensors);
* console.log('y:');
* y.print();
* console.log('b:');
* b.print();
* ```
*
* @param result The tensor to keep from being disposed.
*
* @doc {heading: 'Performance', subheading: 'Memory'}
*/
function keep(result) {
	return ENGINE.keep(result);
}
/**
* Executes `f()` and returns a promise that resolves with timing
* information.
*
* The result is an object with the following properties:
*
* - `wallMs`: Wall execution time.
* - `kernelMs`: Kernel execution time, ignoring data transfer. If using the
* WebGL backend and the query timer extension is not available, this will
* return an error object.
* - On `WebGL` The following additional properties exist:
*   - `uploadWaitMs`: CPU blocking time on texture uploads.
*   - `downloadWaitMs`: CPU blocking time on texture downloads (readPixels).
*
* ```js
* const x = tf.randomNormal([20, 20]);
* const time = await tf.time(() => x.matMul(x));
*
* console.log(`kernelMs: ${time.kernelMs}, wallTimeMs: ${time.wallMs}`);
* ```
*
* @param f The function to execute and time.
*
* @doc {heading: 'Performance', subheading: 'Timing'}
*/
function time(f) {
	return ENGINE.time(f);
}
/**
* Sets the backend (cpu, webgl, wasm, etc) responsible for creating tensors and
* executing operations on those tensors. Returns a promise that resolves
* to a boolean if the backend initialization was successful.
*
* Note this disposes the current backend, if any, as well as any tensors
* associated with it. A new backend is initialized, even if it is of the
* same type as the previous one.
*
* @param backendName The name of the backend. Currently supports
*     `'webgl'|'cpu'` in the browser, `'tensorflow'` under node.js
*     (requires tfjs-node), and `'wasm'` (requires tfjs-backend-wasm).
*
* @doc {heading: 'Backends'}
*/
function setBackend(backendName) {
	return ENGINE.setBackend(backendName);
}
/**
* Returns a promise that resolves when the currently selected backend (or the
* highest priority one) has initialized. Await this promise when you are using
* a backend that has async initialization.
*
* @doc {heading: 'Backends'}
*/
function ready() {
	return ENGINE.ready();
}
/**
* Returns the current backend name (cpu, webgl, etc). The backend is
* responsible for creating tensors and executing operations on those tensors.
*
* @doc {heading: 'Backends'}
*/
function getBackend() {
	return ENGINE.backendName;
}
/**
* Removes a backend and the registered factory.
*
* @doc {heading: 'Backends'}
*/
function removeBackend(name) {
	ENGINE.removeBackend(name);
}
/**
* Finds the backend registered under the provided name. Returns null if the
* name is not in the registry, or the registration hasn't finished yet.
*/
function findBackend(name) {
	return ENGINE.findBackend(name);
}
/**
* Finds the backend factory registered under the provided name. Returns a
* function that produces a new backend when called. Returns null if the name
* is not in the registry.
*/
function findBackendFactory(name) {
	return ENGINE.findBackendFactory(name);
}
/**
* Registers a global backend. The registration should happen when importing
* a module file (e.g. when importing `backend_webgl.ts`), and is used for
* modular builds (e.g. custom tfjs bundle with only webgl support).
*
* @param factory The backend factory function. When called, it should
* return a backend instance, or a promise of an instance.
* @param priority The priority of the backend (higher = more important).
*     In case multiple backends are registered, the priority is used to find
*     the best backend. Defaults to 1.
* @return False if there is already a registered backend under this name, true
*     if not.
*
* @doc {heading: 'Backends'}
*/
function registerBackend(name, factory, priority = 1) {
	return ENGINE.registerBackend(name, factory, priority);
}
/**
* Gets the current backend. If no backends have been initialized, this will
* attempt to initialize the best backend. Will throw an error if the highest
* priority backend has async initialization, in which case, you should call
* 'await tf.ready()' before running other code.
*
* @doc {heading: 'Backends'}
*/
function backend() {
	return ENGINE.backend;
}
/**
* Sets the global platform.
*
* @param platformName The name of this platform.
* @param platform A platform implementation.
*/
function setPlatform(platformName, platform) {
	env().setPlatform(platformName, platform);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/add.js
/**
* Adds two `tf.Tensor`s element-wise, A + B. Supports broadcasting.
*
*
* ```js
* const a = tf.tensor1d([1, 2, 3, 4]);
* const b = tf.tensor1d([10, 20, 30, 40]);
*
* a.add(b).print();  // or tf.add(a, b)
* ```
*
* ```js
* // Broadcast add a with b.
* const a = tf.scalar(5);
* const b = tf.tensor1d([10, 20, 30, 40]);
*
* a.add(b).print();  // or tf.add(a, b)
* ```
* @param a The first `tf.Tensor` to add.
* @param b The second `tf.Tensor` to add. Must have the same type as `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function add_(a, b) {
	let $a = convertToTensor(a, "a", "add");
	let $b = convertToTensor(b, "b", "add");
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Add, inputs);
}
const add = op({ add_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/floorDiv.js
/**
* Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.
* The result is rounded with floor function.
*
*
* ```js
* const a = tf.tensor1d([1, 4, 9, 16]);
* const b = tf.tensor1d([1, 2, 3, 4]);
*
* a.floorDiv(b).print();  // or tf.div(a, b)
* ```
*
* ```js
* // Broadcast div a with b.
* const a = tf.tensor1d([2, 4, 6, 8]);
* const b = tf.scalar(2);
*
* a.floorDiv(b).print();  // or tf.floorDiv(a, b)
* ```
*
* @param a The first tensor as the numerator.
* @param b The second tensor as the denominator. Must have the same dtype as
* `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function floorDiv_(a, b) {
	let $a = convertToTensor(a, "a", "floorDiv");
	let $b = convertToTensor(b, "b", "floorDiv");
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(FloorDiv, inputs);
}
const floorDiv = op({ floorDiv_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/div.js
/**
* Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 4, 9, 16]);
* const b = tf.tensor1d([1, 2, 3, 4]);
*
* a.div(b).print();  // or tf.div(a, b)
* ```
*
* ```js
* // Broadcast div a with b.
* const a = tf.tensor1d([2, 4, 6, 8]);
* const b = tf.scalar(2);
*
* a.div(b).print();  // or tf.div(a, b)
* ```
*
* @param a The first tensor as the numerator.
* @param b The second tensor as the denominator. Must have the same dtype as
* `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function div_(a, b) {
	let $a = convertToTensor(a, "a", "div");
	let $b = convertToTensor(b, "b", "div");
	[$a, $b] = makeTypesMatch($a, $b);
	if ($a.dtype === "int32" && $b.dtype === "int32") return floorDiv($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(RealDiv, inputs, {});
}
const div = op({ div_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/mul.js
/**
* Multiplies two `tf.Tensor`s element-wise, A * B. Supports broadcasting.
*
* We also expose `tf.mulStrict` which has the same signature as this op and
* asserts that `a` and `b` are the same shape (does not broadcast).
*
* ```js
* const a = tf.tensor1d([1, 2, 3, 4]);
* const b = tf.tensor1d([2, 3, 4, 5]);
*
* a.mul(b).print();  // or tf.mul(a, b)
* ```
*
* ```js
* // Broadcast mul a with b.
* const a = tf.tensor1d([1, 2, 3, 4]);
* const b = tf.scalar(5);
*
* a.mul(b).print();  // or tf.mul(a, b)
* ```
* @param a The first tensor to multiply.
* @param b The second tensor to multiply. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function mul_(a, b) {
	let $a = convertToTensor(a, "a", "mul");
	let $b = convertToTensor(b, "b", "mul");
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Multiply, inputs);
}
const mul = op({ mul_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/abs.js
/**
* Computes absolute value element-wise: `abs(x)`
*
* ```js
* const x = tf.tensor1d([-1, 2, -3, 4]);
*
* x.abs().print();  // or tf.abs(x)
* ```
* @param x The input `tf.Tensor`.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function abs_(x) {
	const $x = convertToTensor(x, "x", "abs");
	if ($x.dtype === "complex64") {
		const inputs = { x: $x };
		return ENGINE.runKernel(ComplexAbs, inputs);
	} else {
		const inputs = { x: $x };
		return ENGINE.runKernel(Abs, inputs);
	}
}
const abs = op({ abs_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/acos.js
/**
* Computes acos of the input `tf.Tensor` element-wise: `acos(x)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.acos().print();  // or tf.acos(x)
* ```
* @param x The input tensor.
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function acos_(x) {
	const inputs = { x: convertToTensor(x, "x", "acos") };
	return ENGINE.runKernel(Acos, inputs);
}
const acos = op({ acos_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/acosh.js
/**
* Computes the inverse hyperbolic cos of the input `tf.Tensor` element-wise:
* `acosh(x)`
*
* ```js
* const x = tf.tensor1d([10, 1, 3, 5.7]);
*
* x.acosh().print();  // or tf.acosh(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function acosh_(x) {
	const inputs = { x: convertToTensor(x, "x", "acosh") };
	return ENGINE.runKernel(Acosh, inputs);
}
const acosh = op({ acosh_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/add_n.js
/**
* Adds a list of `tf.Tensor`s element-wise, each with the same shape and dtype.
*
* ```js
* const a = tf.tensor1d([1, 2]);
* const b = tf.tensor1d([3, 4]);
* const c = tf.tensor1d([5, 6]);
*
* tf.addN([a, b, c]).print();
* ```
* @param tensors A list of tensors with the same shape and dtype.
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function addN_(tensors) {
	assert(Array.isArray(tensors), () => "The argument passed to tf.addN() must be a list of tensors");
	assert(tensors.length >= 1, () => `Must pass at least one tensor to tf.addN(), but got ${tensors.length}`);
	const $tensors = tensors.map((t, i) => convertToTensor(t, `tensors${i}`, "addN"));
	const firstTensor = $tensors[0];
	$tensors.forEach((t) => {
		if (t.dtype !== firstTensor.dtype) throw new Error("All tensors passed to tf.addN() must have the same dtype");
	});
	$tensors.forEach((t) => {
		if (!arraysEqual(t.shape, firstTensor.shape)) throw new Error("All tensors passed to tf.addN() must have the same shape");
	});
	const inputs = $tensors;
	return ENGINE.runKernel(AddN, inputs);
}
const addN = op({ addN_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/all.js
/**
* Computes the logical and of elements across dimensions of a `tf.Tensor`.
*
* Reduces the input along the dimensions given in `axes`. Unless `keepDims`
* is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
* `axes`. If `keepDims` is true, the reduced dimensions are retained with
* length 1. If `axes` has no entries, all dimensions are reduced, and an
* `tf.Tensor` with a single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 1, 1], 'bool');
*
* x.all().print();  // or tf.all(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');
*
* const axis = 1;
* x.all(axis).print();  // or tf.all(x, axis)
* ```
*
* @param x The input tensor. Must be of dtype bool.
* @param axis The dimension(s) to reduce. By default it reduces
*     all dimensions.
* @param keepDims If true, retains reduced dimensions with size 1.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function all_(x, axis = null, keepDims = false) {
	const inputs = { x: convertToTensor(x, "x", "all", "bool") };
	const attrs = {
		axis,
		keepDims
	};
	return ENGINE.runKernel(All, inputs, attrs);
}
const all = op({ all_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/any.js
/**
* Computes the logical or of elements across dimensions of a `tf.Tensor`.
*
* Reduces the input along the dimensions given in `axes`. Unless `keepDims`
* is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
* `axes`. If `keepDims` is true, the reduced dimensions are retained with
* length 1. If `axes` has no entries, all dimensions are reduced, and an
* `tf.Tensor` with a single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 1, 1], 'bool');
*
* x.any().print();  // or tf.any(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 1, 0, 0], [2, 2], 'bool');
*
* const axis = 1;
* x.any(axis).print();  // or tf.any(x, axis)
* ```
*
* @param x The input tensor. Must be of dtype bool.
* @param axis The dimension(s) to reduce. By default it reduces
*     all dimensions.
* @param keepDims If true, retains reduced dimensions with size 1.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function any_(x, axis = null, keepDims = false) {
	const inputs = { x: convertToTensor(x, "x", "any", "bool") };
	const attrs = {
		axis,
		keepDims
	};
	return ENGINE.runKernel(Any, inputs, attrs);
}
const any = op({ any_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/arg_max.js
/**
* Returns the indices of the maximum values along an `axis`.
*
* The result has the same shape as `input` with the dimension along `axis`
* removed.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.argMax().print();  // or tf.argMax(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);
*
* const axis = 1;
* x.argMax(axis).print();  // or tf.argMax(x, axis)
* ```
*
* @param x The input tensor.
* @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function argMax_(x, axis = 0) {
	const inputs = { x: convertToTensor(x, "x", "argMax") };
	const attrs = { axis };
	return ENGINE.runKernel(ArgMax, inputs, attrs);
}
const argMax = op({ argMax_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/arg_min.js
/**
* Returns the indices of the minimum values along an `axis`.
*
* The result has the same shape as `input` with the dimension along `axis`
* removed.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.argMin().print();  // or tf.argMin(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 4, 3], [2, 2]);
*
* const axis = 1;
* x.argMin(axis).print();  // or tf.argMin(x, axis)
* ```
*
* @param x The input tensor.
* @param axis The dimension to reduce. Defaults to 0 (outer-most dimension).
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function argMin_(x, axis = 0) {
	const inputs = { x: convertToTensor(x, "x", "argMin") };
	const attrs = { axis };
	return ENGINE.runKernel(ArgMin, inputs, attrs);
}
const argMin = op({ argMin_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/asin.js
/**
* Computes asin of the input `tf.Tensor` element-wise: `asin(x)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.asin().print();  // or tf.asin(x)
* ```
* @param x The input tensor.
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function asin_(x) {
	const inputs = { x: convertToTensor(x, "x", "asin") };
	return ENGINE.runKernel(Asin, inputs);
}
const asin = op({ asin_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/asinh.js
/**
* Computes inverse hyperbolic sin of the input `tf.Tensor` element-wise:
* `asinh(x)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.asinh().print();  // or tf.asinh(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function asinh_(x) {
	const inputs = { x: convertToTensor(x, "x", "asinh") };
	return ENGINE.runKernel(Asinh, inputs);
}
const asinh = op({ asinh_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/atan.js
/**
* Computes atan of the input `tf.Tensor` element-wise: `atan(x)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.atan().print();  // or tf.atan(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function atan_(x) {
	const inputs = { x: convertToTensor(x, "x", "atan") };
	return ENGINE.runKernel(Atan, inputs);
}
const atan = op({ atan_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/atan2.js
/**
* Computes arctangent of `tf.Tensor`s a / b element-wise: `atan2(a, b)`.
* Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1.0, 1.0, -1.0, .7]);
* const b = tf.tensor1d([2.0, 13.0, 3.5, .21]);
*
* tf.atan2(a, b).print()
* ```
*
* @param a The first tensor.
* @param b The second tensor. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function atan2_(a, b) {
	let $a = convertToTensor(a, "a", "atan2");
	let $b = convertToTensor(b, "b", "atan2");
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Atan2, inputs);
}
const atan2 = op({ atan2_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/atanh.js
/**
* Computes inverse hyperbolic tan of the input `tf.Tensor` element-wise:
* `atanh(x)`
*
* ```js
* const x = tf.tensor1d([0, .1, -.1, .7]);
*
* x.atanh().print();  // or tf.atanh(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function atanh_(x) {
	const inputs = { x: convertToTensor(x, "x", "atanh") };
	return ENGINE.runKernel(Atanh, inputs);
}
const atanh = op({ atanh_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv_util.js
/**
* @license
* Copyright 2020 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
*
* @param inputShape Input tensor shape is of the following dimensions:
*     `[batch, height, width, inChannels]`.
* @param filterShape The filter shape is of the following dimensions:
*     `[filterHeight, filterWidth, depth]`.
* @param strides The strides of the sliding window for each dimension of the
*     input tensor: `[strideHeight, strideWidth]`.
*     If `strides` is a single number,
*     then `strideHeight == strideWidth`.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1*1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dataFormat The data format of the input and output data.
*     Defaults to 'NHWC'.
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`.
*     Defaults to `[1, 1]`. If `dilations` is a single number, then
*     `dilationHeight == dilationWidth`.
*/
function computeDilation2DInfo(inputShape, filterShape, strides, pad$1, dataFormat = "NHWC", dilations) {
	const inputChannels = inputShape[3];
	const $filterShape = [...filterShape, inputChannels];
	const $dataFormat = convertConv2DDataFormat(dataFormat);
	return computeConv2DInfo(inputShape, $filterShape, strides, dilations, pad$1, null, null, $dataFormat);
}
function computePool2DInfo(inShape, filterSize, strides, dilations, pad$1, roundingMode, dataFormat = "channelsLast") {
	const [filterHeight, filterWidth] = parseTupleParam(filterSize);
	let filterShape;
	if (dataFormat === "channelsLast") filterShape = [
		filterHeight,
		filterWidth,
		inShape[3],
		inShape[3]
	];
	else if (dataFormat === "channelsFirst") filterShape = [
		filterHeight,
		filterWidth,
		inShape[1],
		inShape[1]
	];
	else throw new Error(`Unknown dataFormat ${dataFormat}`);
	return computeConv2DInfo(inShape, filterShape, strides, dilations, pad$1, roundingMode, false, dataFormat);
}
/**
* Computes the information for a forward pass of a pooling3D operation.
*/
function computePool3DInfo(inShape, filterSize, strides, dilations, pad$1, roundingMode, dataFormat = "NDHWC") {
	const [filterDepth, filterHeight, filterWidth] = parse3TupleParam(filterSize);
	let filterShape;
	let $dataFormat;
	if (dataFormat === "NDHWC") {
		$dataFormat = "channelsLast";
		filterShape = [
			filterDepth,
			filterHeight,
			filterWidth,
			inShape[4],
			inShape[4]
		];
	} else if (dataFormat === "NCDHW") {
		$dataFormat = "channelsFirst";
		filterShape = [
			filterDepth,
			filterHeight,
			filterWidth,
			inShape[1],
			inShape[1]
		];
	} else throw new Error(`Unknown dataFormat ${dataFormat}`);
	return computeConv3DInfo(inShape, filterShape, strides, dilations, pad$1, false, $dataFormat, roundingMode);
}
/**
* Computes the information for a forward pass of a convolution/pooling
* operation.
*/
function computeConv2DInfo(inShape, filterShape, strides, dilations, pad$1, roundingMode, depthwise = false, dataFormat = "channelsLast") {
	let [batchSize, inHeight, inWidth, inChannels] = [
		-1,
		-1,
		-1,
		-1
	];
	if (dataFormat === "channelsLast") [batchSize, inHeight, inWidth, inChannels] = inShape;
	else if (dataFormat === "channelsFirst") [batchSize, inChannels, inHeight, inWidth] = inShape;
	else throw new Error(`Unknown dataFormat ${dataFormat}`);
	const [filterHeight, filterWidth, , filterChannels] = filterShape;
	const [strideHeight, strideWidth] = parseTupleParam(strides);
	const [dilationHeight, dilationWidth] = parseTupleParam(dilations);
	const effectiveFilterHeight = getEffectiveFilterSize(filterHeight, dilationHeight);
	const effectiveFilterWidth = getEffectiveFilterSize(filterWidth, dilationWidth);
	const { padInfo, outHeight, outWidth } = getPadAndOutInfo(pad$1, inHeight, inWidth, strideHeight, strideWidth, effectiveFilterHeight, effectiveFilterWidth, roundingMode, dataFormat);
	const outChannels = depthwise ? filterChannels * inChannels : filterChannels;
	let outShape;
	if (dataFormat === "channelsFirst") outShape = [
		batchSize,
		outChannels,
		outHeight,
		outWidth
	];
	else if (dataFormat === "channelsLast") outShape = [
		batchSize,
		outHeight,
		outWidth,
		outChannels
	];
	return {
		batchSize,
		dataFormat,
		inHeight,
		inWidth,
		inChannels,
		outHeight,
		outWidth,
		outChannels,
		padInfo,
		strideHeight,
		strideWidth,
		filterHeight,
		filterWidth,
		effectiveFilterHeight,
		effectiveFilterWidth,
		dilationHeight,
		dilationWidth,
		inShape,
		outShape,
		filterShape
	};
}
/**
* Computes the information for a forward pass of a 3D convolution/pooling
* operation.
*/
function computeConv3DInfo(inShape, filterShape, strides, dilations, pad$1, depthwise = false, dataFormat = "channelsLast", roundingMode) {
	let [batchSize, inDepth, inHeight, inWidth, inChannels] = [
		-1,
		-1,
		-1,
		-1,
		-1
	];
	if (dataFormat === "channelsLast") [batchSize, inDepth, inHeight, inWidth, inChannels] = inShape;
	else if (dataFormat === "channelsFirst") [batchSize, inChannels, inDepth, inHeight, inWidth] = inShape;
	else throw new Error(`Unknown dataFormat ${dataFormat}`);
	const [filterDepth, filterHeight, filterWidth, , filterChannels] = filterShape;
	const [strideDepth, strideHeight, strideWidth] = parse3TupleParam(strides);
	const [dilationDepth, dilationHeight, dilationWidth] = parse3TupleParam(dilations);
	const effectiveFilterDepth = getEffectiveFilterSize(filterDepth, dilationDepth);
	const effectiveFilterHeight = getEffectiveFilterSize(filterHeight, dilationHeight);
	const effectiveFilterWidth = getEffectiveFilterSize(filterWidth, dilationWidth);
	const { padInfo, outDepth, outHeight, outWidth } = get3DPadAndOutInfo(pad$1, inDepth, inHeight, inWidth, strideDepth, strideHeight, strideWidth, effectiveFilterDepth, effectiveFilterHeight, effectiveFilterWidth, roundingMode);
	const outChannels = depthwise ? filterChannels * inChannels : filterChannels;
	let outShape;
	if (dataFormat === "channelsFirst") outShape = [
		batchSize,
		outChannels,
		outDepth,
		outHeight,
		outWidth
	];
	else if (dataFormat === "channelsLast") outShape = [
		batchSize,
		outDepth,
		outHeight,
		outWidth,
		outChannels
	];
	return {
		batchSize,
		dataFormat,
		inDepth,
		inHeight,
		inWidth,
		inChannels,
		outDepth,
		outHeight,
		outWidth,
		outChannels,
		padInfo,
		strideDepth,
		strideHeight,
		strideWidth,
		filterDepth,
		filterHeight,
		filterWidth,
		effectiveFilterDepth,
		effectiveFilterHeight,
		effectiveFilterWidth,
		dilationDepth,
		dilationHeight,
		dilationWidth,
		inShape,
		outShape,
		filterShape
	};
}
function computeOutputShape2D(inShape, fieldSize, stride, zeroPad, roundingMode) {
	if (zeroPad == null) zeroPad = computeDefaultPad(inShape, fieldSize, stride);
	const inputRows = inShape[0];
	const inputCols = inShape[1];
	const outputRows = round$1((inputRows - fieldSize + 2 * zeroPad) / stride + 1, roundingMode);
	const outputCols = round$1((inputCols - fieldSize + 2 * zeroPad) / stride + 1, roundingMode);
	return [outputRows, outputCols];
}
function computeOutputShape4D(inShape, fieldSize, outChannels, stride, zeroPad, roundingMode) {
	if (zeroPad == null) zeroPad = computeDefaultPad(inShape, fieldSize, stride);
	const inputDepth = inShape[0];
	const inputRows = inShape[1];
	const inputCols = inShape[2];
	const outputDepths = round$1((inputDepth - fieldSize + 2 * zeroPad) / stride + 1, roundingMode);
	const outputRows = round$1((inputRows - fieldSize + 2 * zeroPad) / stride + 1, roundingMode);
	const outputCols = round$1((inputCols - fieldSize + 2 * zeroPad) / stride + 1, roundingMode);
	return [
		outputDepths,
		outputRows,
		outputCols,
		outChannels
	];
}
function computeDefaultPad(inputShape, fieldSize, stride, dilation = 1) {
	const effectiveFieldSize = getEffectiveFilterSize(fieldSize, dilation);
	return Math.floor((inputShape[0] * (stride - 1) - stride + effectiveFieldSize) / 2);
}
function parseTupleParam(param) {
	if (typeof param === "number") return [
		param,
		param,
		param
	];
	if (param.length === 2) return [
		param[0],
		param[1],
		1
	];
	return param;
}
function parse3TupleParam(param) {
	return typeof param === "number" ? [
		param,
		param,
		param
	] : param;
}
function getEffectiveFilterSize(filterSize, dilation) {
	if (dilation <= 1) return filterSize;
	return filterSize + (filterSize - 1) * (dilation - 1);
}
function getPadAndOutInfo(pad$1, inHeight, inWidth, strideHeight, strideWidth, filterHeight, filterWidth, roundingMode, dataFormat) {
	let padInfo;
	let outHeight;
	let outWidth;
	if (typeof pad$1 === "number") {
		padInfo = {
			top: pad$1,
			bottom: pad$1,
			left: pad$1,
			right: pad$1,
			type: pad$1 === 0 ? "VALID" : "NUMBER"
		};
		const outShape = computeOutputShape2D([inHeight, inWidth], filterHeight, strideHeight, pad$1, roundingMode);
		outHeight = outShape[0];
		outWidth = outShape[1];
	} else if (pad$1 === "same") {
		outHeight = Math.ceil(inHeight / strideHeight);
		outWidth = Math.ceil(inWidth / strideWidth);
		const padAlongHeight = Math.max(0, (outHeight - 1) * strideHeight + filterHeight - inHeight);
		const padAlongWidth = Math.max(0, (outWidth - 1) * strideWidth + filterWidth - inWidth);
		const top = Math.floor(padAlongHeight / 2);
		const bottom = padAlongHeight - top;
		const left = Math.floor(padAlongWidth / 2);
		const right = padAlongWidth - left;
		padInfo = {
			top,
			bottom,
			left,
			right,
			type: "SAME"
		};
	} else if (pad$1 === "valid") {
		padInfo = {
			top: 0,
			bottom: 0,
			left: 0,
			right: 0,
			type: "VALID"
		};
		outHeight = Math.ceil((inHeight - filterHeight + 1) / strideHeight);
		outWidth = Math.ceil((inWidth - filterWidth + 1) / strideWidth);
	} else if (typeof pad$1 === "object") {
		const top = dataFormat === "channelsLast" ? pad$1[1][0] : pad$1[2][0];
		const bottom = dataFormat === "channelsLast" ? pad$1[1][1] : pad$1[2][1];
		const left = dataFormat === "channelsLast" ? pad$1[2][0] : pad$1[3][0];
		const right = dataFormat === "channelsLast" ? pad$1[2][1] : pad$1[3][1];
		padInfo = {
			top,
			bottom,
			left,
			right,
			type: top === 0 && bottom === 0 && left === 0 && right === 0 ? "VALID" : "EXPLICIT"
		};
		outHeight = round$1((inHeight - filterHeight + top + bottom) / strideHeight + 1, roundingMode);
		outWidth = round$1((inWidth - filterWidth + left + right) / strideWidth + 1, roundingMode);
	} else throw Error(`Unknown padding parameter: ${pad$1}`);
	return {
		padInfo,
		outHeight,
		outWidth
	};
}
function get3DPadAndOutInfo(pad$1, inDepth, inHeight, inWidth, strideDepth, strideHeight, strideWidth, filterDepth, filterHeight, filterWidth, roundingMode) {
	let padInfo;
	let outDepth;
	let outHeight;
	let outWidth;
	if (typeof pad$1 === "number") {
		padInfo = {
			top: pad$1,
			bottom: pad$1,
			left: pad$1,
			right: pad$1,
			front: pad$1,
			back: pad$1,
			type: pad$1 === 0 ? "VALID" : "NUMBER"
		};
		const outShape = computeOutputShape4D([
			inDepth,
			inHeight,
			inWidth,
			1
		], filterDepth, 1, strideDepth, pad$1, roundingMode);
		outDepth = outShape[0];
		outHeight = outShape[1];
		outWidth = outShape[2];
	} else if (pad$1 === "same") {
		outDepth = Math.ceil(inDepth / strideDepth);
		outHeight = Math.ceil(inHeight / strideHeight);
		outWidth = Math.ceil(inWidth / strideWidth);
		const padAlongDepth = (outDepth - 1) * strideDepth + filterDepth - inDepth;
		const padAlongHeight = (outHeight - 1) * strideHeight + filterHeight - inHeight;
		const padAlongWidth = (outWidth - 1) * strideWidth + filterWidth - inWidth;
		const front = Math.floor(padAlongDepth / 2);
		const back = padAlongDepth - front;
		const top = Math.floor(padAlongHeight / 2);
		const bottom = padAlongHeight - top;
		const left = Math.floor(padAlongWidth / 2);
		const right = padAlongWidth - left;
		padInfo = {
			top,
			bottom,
			left,
			right,
			front,
			back,
			type: "SAME"
		};
	} else if (pad$1 === "valid") {
		padInfo = {
			top: 0,
			bottom: 0,
			left: 0,
			right: 0,
			front: 0,
			back: 0,
			type: "VALID"
		};
		outDepth = Math.ceil((inDepth - filterDepth + 1) / strideDepth);
		outHeight = Math.ceil((inHeight - filterHeight + 1) / strideHeight);
		outWidth = Math.ceil((inWidth - filterWidth + 1) / strideWidth);
	} else throw Error(`Unknown padding parameter: ${pad$1}`);
	return {
		padInfo,
		outDepth,
		outHeight,
		outWidth
	};
}
/**
* Rounds a value depending on the rounding mode
* @param value
* @param roundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*/
function round$1(value, roundingMode) {
	if (!roundingMode) return Math.trunc(value);
	switch (roundingMode) {
		case "round": return Math.round(value);
		case "ceil": return Math.ceil(value);
		case "floor": return Math.floor(value);
		default: throw new Error(`Unknown roundingMode ${roundingMode}`);
	}
}
function tupleValuesAreOne(param) {
	const [dimA, dimB, dimC] = parseTupleParam(param);
	return dimA === 1 && dimB === 1 && dimC === 1;
}
function eitherStridesOrDilationsAreOne(strides, dilations) {
	return tupleValuesAreOne(strides) || tupleValuesAreOne(dilations);
}
/**
* Convert Conv2D dataFormat from 'NHWC'|'NCHW' to
*    'channelsLast'|'channelsFirst'
* @param dataFormat in 'NHWC'|'NCHW' mode
* @return dataFormat in 'channelsLast'|'channelsFirst' mode
* @throws unknown dataFormat
*/
function convertConv2DDataFormat(dataFormat) {
	if (dataFormat === "NHWC") return "channelsLast";
	else if (dataFormat === "NCHW") return "channelsFirst";
	else throw new Error(`Unknown dataFormat ${dataFormat}`);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reshape.js
/**
* Reshapes a `tf.Tensor` to a given shape.
*
* Given an input tensor, returns a new tensor with the same values as the
* input tensor with shape `shape`.
*
* If one component of shape is the special value -1, the size of that
* dimension is computed so that the total size remains constant. In
* particular, a shape of [-1] flattens into 1-D. At most one component of
* shape can be -1.
*
* If shape is 1-D or higher, then the operation returns a tensor with shape
* shape filled with the values of tensor. In this case, the number of
* elements implied by shape must be the same as the number of elements in
* tensor.
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
* x.reshape([2, 2]).print();
* ```
*
* @param x The input tensor to be reshaped.
* @param shape An array of integers defining the output tensor shape.
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function reshape_(x, shape) {
	const inputs = { x: convertToTensor(x, "x", "reshape", "string_or_numeric") };
	const attrs = { shape };
	return ENGINE.runKernel(Reshape, inputs, attrs);
}
const reshape = op({ reshape_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/avg_pool.js
/**
* Computes the 2D average pooling of an image.
*
* @param x The input tensor, of rank 4 or rank 3 of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
* @param filterSize The filter size: `[filterHeight, filterWidth]`. If
*     `filterSize` is a single number, then `filterHeight == filterWidth`.
* @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
*     `strides` is a single number, then `strideHeight == strideWidth`.
* @param pad The type of padding algorithm:
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*         https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*/
function avgPool_(x, filterSize, strides, pad$1, dimRoundingMode) {
	const $x = convertToTensor(x, "x", "avgPool", "float32");
	const dilations = 1;
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => `Error in avgPool: Either strides or dilations must be 1. Got strides ${strides} and dilations '${dilations}'`);
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	assert(x4D.rank === 4, () => `Error in avgPool: x must be rank 4 but got rank ${x4D.rank}.`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in avgPool: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inputs = { x: x4D };
	const attrs = {
		filterSize,
		strides,
		pad: pad$1,
		dimRoundingMode
	};
	let res = ENGINE.runKernel(AvgPool, inputs, attrs);
	res = cast(res, $x.dtype);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const avgPool = op({ avgPool_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/avg_pool_3d.js
/**
* Computes the 3D average pooling.
*
* ```js
* const x = tf.tensor5d([1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 2, 2, 1]);
* const result = tf.avgPool3d(x, 2, 1, 'valid');
* result.print();
* ```
*
* @param x The input tensor, of rank 5 or rank 4 of shape
*     `[batch, depth, height, width, inChannels]`.
* @param filterSize The filter size:
*     `[filterDepth, filterHeight, filterWidth]`.
*     If `filterSize` is a single number,
*     then `filterDepth == filterHeight == filterWidth`.
* @param strides The strides of the pooling:
*     `[strideDepth, strideHeight, strideWidth]`.
*     If `strides` is a single number,
*     then `strideDepth == strideHeight == strideWidth`.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1*1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
* @param dataFormat An optional string from: "NDHWC", "NCDHW". Defaults to
*     "NDHWC". Specify the data format of the input and output data. With the
*     default format "NDHWC", the data is stored in the order of: [batch,
*     depth, height, width, channels]. Only "NDHWC" is currently supported.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function avgPool3d_(x, filterSize, strides, pad$1, dimRoundingMode, dataFormat = "NDHWC") {
	const $x = convertToTensor(x, "x", "avgPool3d", "float32");
	let x5D = $x;
	let reshapedTo5D = false;
	if ($x.rank === 4) {
		reshapedTo5D = true;
		x5D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2],
			$x.shape[3]
		]);
	}
	assert(x5D.rank === 5, () => `Error in avgPool3d: x must be rank 5 but got rank ${x5D.rank}.`);
	assert(dataFormat === "NDHWC", () => `Error in avgPool3d: Only NDHWC is currently supported, but got dataFormat of ${dataFormat}`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in avgPool3d: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inputs = { x: x5D };
	const attrs = {
		filterSize,
		strides,
		pad: pad$1,
		dimRoundingMode,
		dataFormat
	};
	let res = ENGINE.runKernel(AvgPool3D, inputs, attrs);
	res = cast(res, x5D.dtype);
	if (reshapedTo5D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3],
		res.shape[4]
	]);
	return res;
}
const avgPool3d = op({ avgPool3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/concat.js
/**
* Concatenates a list of `tf.Tensor`s along a given axis.
*
* The tensors ranks and types must match, and their sizes must match in all
* dimensions except `axis`.
*
* Also available are stricter rank-specific methods that assert that
* `tensors` are of the given rank:
*   - `tf.concat1d`
*   - `tf.concat2d`
*   - `tf.concat3d`
*   - `tf.concat4d`
*
* Except `tf.concat1d` (which does not have axis param), all methods have
* same signature as this method.
*
* ```js
* const a = tf.tensor1d([1, 2]);
* const b = tf.tensor1d([3, 4]);
* a.concat(b).print();  // or a.concat(b)
* ```
*
* ```js
* const a = tf.tensor1d([1, 2]);
* const b = tf.tensor1d([3, 4]);
* const c = tf.tensor1d([5, 6]);
* tf.concat([a, b, c]).print();
* ```
*
* ```js
* const a = tf.tensor2d([[1, 2], [10, 20]]);
* const b = tf.tensor2d([[3, 4], [30, 40]]);
* const axis = 1;
* tf.concat([a, b], axis).print();
* ```
* @param tensors A list of tensors to concatenate.
* @param axis The axis to concate along. Defaults to 0 (the first dim).
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function concat_(tensors, axis = 0) {
	assert(tensors.length >= 1, () => "Pass at least one tensor to concat");
	const $tensors = convertToTensorArray(tensors, "tensors", "concat", "string_or_numeric");
	if ($tensors[0].dtype === "complex64") $tensors.forEach((tensor$1) => {
		if (tensor$1.dtype !== "complex64") throw new Error(`Cannot concatenate complex64 tensors with a tensor
          with dtype ${tensor$1.dtype}. `);
	});
	if ($tensors.length === 1) return clone($tensors[0]);
	const inputs = $tensors;
	const attr = { axis };
	return ENGINE.runKernel(Concat, inputs, attr);
}
const concat = op({ concat_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sigmoid.js
/**
* Computes sigmoid element-wise, `1 / (1 + exp(-x))`
*
* ```js
* const x = tf.tensor1d([0, -1, 2, -3]);
*
* x.sigmoid().print();  // or tf.sigmoid(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function sigmoid_(x) {
	const inputs = { x: convertToTensor(x, "x", "sigmoid", "float32") };
	return ENGINE.runKernel(Sigmoid, inputs);
}
const sigmoid = op({ sigmoid_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/slice.js
/**
* Extracts a slice from a `tf.Tensor` starting at coordinates `begin`
* and is of size `size`.
*
* Also available are stricter rank-specific methods with the same signature
* as this method that assert that `x` is of the given rank:
*   - `tf.slice1d`
*   - `tf.slice2d`
*   - `tf.slice3d`
*   - `tf.slice4d`
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
*
* x.slice([1], [2]).print();
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* x.slice([1, 0], [1, 2]).print();
* ```
* @param x The input `tf.Tensor` to slice from.
* @param begin The coordinates to start the slice from. The length can be
*     less than the rank of x - the rest of the axes will have implicit 0 as
*     start. Can also be a single number, in which case it specifies the
*     first axis.
* @param size The size of the slice. The length can be less than the rank of
*     x - the rest of the axes will have implicit -1. A value of -1 requests
*     the rest of the dimensions in the axis. Can also be a single number,
*     in which case it specifies the size of the first axis.
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function slice_(x, begin, size) {
	const $x = convertToTensor(x, "x", "slice", "string_or_numeric");
	if ($x.rank === 0) throw new Error("Slicing scalar is not possible");
	const inputs = { x: $x };
	const attrs = {
		begin,
		size
	};
	return ENGINE.runKernel(Slice, inputs, attrs);
}
const slice = op({ slice_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tanh.js
/**
* Computes hyperbolic tangent of the input `tf.Tensor` element-wise: `tanh(x)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, 70]);
*
* x.tanh().print();  // or tf.tanh(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function tanh_(x) {
	const inputs = { x: convertToTensor(x, "x", "tanh", "float32") };
	return ENGINE.runKernel(Tanh, inputs);
}
const tanh = op({ tanh_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/basic_lstm_cell.js
/**
* Computes the next state and output of a BasicLSTMCell.
*
* Returns `[newC, newH]`.
*
* Derived from tf.contrib.rnn.BasicLSTMCell.
*
* @param forgetBias Forget bias for the cell.
* @param lstmKernel The weights for the cell.
* @param lstmBias The bias for the cell.
* @param data The input to the cell.
* @param c Previous cell state.
* @param h Previous cell output.
*
* @doc {heading: 'Operations', subheading: 'RNN'}
*/
function basicLSTMCell_(forgetBias, lstmKernel, lstmBias, data, c, h) {
	const $forgetBias = convertToTensor(forgetBias, "forgetBias", "basicLSTMCell");
	const $lstmKernel = convertToTensor(lstmKernel, "lstmKernel", "basicLSTMCell");
	const $lstmBias = convertToTensor(lstmBias, "lstmBias", "basicLSTMCell");
	const $data = convertToTensor(data, "data", "basicLSTMCell");
	const $c = convertToTensor(c, "c", "basicLSTMCell");
	const $h = convertToTensor(h, "h", "basicLSTMCell");
	const combined = concat([$data, $h], 1);
	const weighted = matMul(combined, $lstmKernel);
	const res = add(weighted, $lstmBias);
	const batchSize = res.shape[0];
	const sliceCols = res.shape[1] / 4;
	const sliceSize = [batchSize, sliceCols];
	const i = slice(res, [0, 0], sliceSize);
	const j = slice(res, [0, sliceCols], sliceSize);
	const f = slice(res, [0, sliceCols * 2], sliceSize);
	const o = slice(res, [0, sliceCols * 3], sliceSize);
	const newC = add(mul(sigmoid(i), tanh(j)), mul($c, sigmoid(add($forgetBias, f))));
	const newH = mul(tanh(newC), sigmoid(o));
	return [newC, newH];
}
const basicLSTMCell = op({ basicLSTMCell_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/batch_to_space_nd.js
/**
* This operation reshapes the "batch" dimension 0 into `M + 1` dimensions of
* shape `blockShape + [batch]`, interleaves these blocks back into the grid
* defined by the spatial dimensions `[1, ..., M]`, to obtain a result with
* the same rank as the input. The spatial dimensions of this intermediate
* result are then optionally cropped according to `crops` to produce the
* output. This is the reverse of `tf.spaceToBatchND`. See below for a precise
* description.
*
* ```js
* const x = tf.tensor4d([1, 2, 3, 4], [4, 1, 1, 1]);
* const blockShape = [2, 2];
* const crops = [[0, 0], [0, 0]];
*
* x.batchToSpaceND(blockShape, crops).print();
* ```
*
* @param x A `tf.Tensor`. N-D with `x.shape` = `[batch] + spatialShape +
* remainingShape`, where spatialShape has `M` dimensions.
* @param blockShape A 1-D array. Must have shape `[M]`, all values must
* be >= 1.
* @param crops A 2-D array.  Must have shape `[M, 2]`, all values must be >= 0.
* `crops[i] = [cropStart, cropEnd]` specifies the amount to crop from input
* dimension `i + 1`, which corresponds to spatial dimension `i`. It is required
* that `cropStart[i] + cropEnd[i] <= blockShape[i] * inputShape[i + 1]`
*
* This operation is equivalent to the following steps:
*
* 1. Reshape `x` to `reshaped` of shape: `[blockShape[0], ...,
* blockShape[M-1], batch / prod(blockShape), x.shape[1], ...,
* x.shape[N-1]]`
*
* 2. Permute dimensions of `reshaped`to produce `permuted` of shape `[batch /
* prod(blockShape),x.shape[1], blockShape[0], ..., x.shape[M],
* blockShape[M-1],x.shape[M+1], ..., x.shape[N-1]]`
*
* 3. Reshape `permuted` to produce `reshapedPermuted` of shape `[batch /
* prod(blockShape),x.shape[1] * blockShape[0], ..., x.shape[M] *
* blockShape[M-1],x.shape[M+1], ..., x.shape[N-1]]`
*
* 4. Crop the start and end of dimensions `[1, ..., M]` of `reshapedPermuted`
* according to `crops` to produce the output of shape: `[batch /
* prod(blockShape),x.shape[1] * blockShape[0] - crops[0,0] - crops[0,1],
* ..., x.shape[M] * blockShape[M-1] - crops[M-1,0] -
* crops[M-1,1],x.shape[M+1], ..., x.shape[N-1]]`
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function batchToSpaceND_(x, blockShape, crops) {
	const $x = convertToTensor(x, "x", "batchToSpaceND");
	const prod$1 = blockShape.reduce((a, b) => a * b);
	assert($x.rank >= 1 + blockShape.length, () => `input rank is ${$x.rank} but should be > than blockShape.length ${blockShape.length}`);
	assert(crops.length === blockShape.length, () => `crops.length is ${crops.length} but should be equal to blockShape.length  ${blockShape.length}`);
	assert($x.shape[0] % prod$1 === 0, () => `input tensor batch is ${$x.shape[0]} but is not divisible by the product of the elements of blockShape ${blockShape.join(" * ")} === ${prod$1}`);
	const inputs = { x: $x };
	const attrs = {
		blockShape,
		crops
	};
	return ENGINE.runKernel(BatchToSpaceND, inputs, attrs);
}
const batchToSpaceND = op({ batchToSpaceND_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm_util.js
function xAs4D(x) {
	let x4D;
	if (x.rank === 0 || x.rank === 1) x4D = reshape(x, [
		1,
		1,
		1,
		x.size
	]);
	else if (x.rank === 2) x4D = reshape(x, [
		1,
		1,
		x.shape[0],
		x.shape[1]
	]);
	else if (x.rank === 3) x4D = reshape(x, [
		1,
		x.shape[0],
		x.shape[1],
		x.shape[2]
	]);
	else x4D = x;
	return x4D;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm.js
/**
* Batch normalization.
*
* As described in
* [http://arxiv.org/abs/1502.03167](http://arxiv.org/abs/1502.03167).
*
* Mean, variance, scale, and offset can be of two shapes:
*   - The same shape as the input.
*   - In the common case, the depth dimension is the last dimension of x, so
*     the values would be an `tf.Tensor1D` of shape [depth].
*
* Also available are stricter rank-specific methods with the same signature
* as this method that assert that parameters passed are of given rank
*   - `tf.batchNorm2d`
*   - `tf.batchNorm3d`
*   - `tf.batchNorm4d`
*
* @param x The input Tensor.
* @param mean A mean Tensor.
* @param variance A variance Tensor.
* @param offset An offset Tensor.
* @param scale A scale Tensor.
* @param varianceEpsilon A small float number to avoid dividing by 0.
*
* @doc {heading: 'Operations', subheading: 'Normalization'}
*/
function batchNorm_(x, mean$1, variance, offset, scale, varianceEpsilon) {
	if (varianceEpsilon == null) varianceEpsilon = .001;
	const $x = convertToTensor(x, "x", "batchNorm");
	const $mean = convertToTensor(mean$1, "mean", "batchNorm");
	const $variance = convertToTensor(variance, "variance", "batchNorm");
	let $scale;
	if (scale != null) $scale = convertToTensor(scale, "scale", "batchNorm");
	let $offset;
	if (offset != null) $offset = convertToTensor(offset, "offset", "batchNorm");
	assert($mean.rank === $variance.rank, () => "Batch normalization gradient requires mean and variance to have equal ranks.");
	assert($offset == null || $mean.rank === $offset.rank, () => "Batch normalization gradient requires mean and offset to have equal ranks.");
	assert($scale == null || $mean.rank === $scale.rank, () => "Batch normalization gradient requires mean and scale to have equal ranks.");
	const inputs = {
		x: xAs4D($x),
		scale: $scale,
		offset: $offset,
		mean: $mean,
		variance: $variance
	};
	const attrs = { varianceEpsilon };
	const res = ENGINE.runKernel(FusedBatchNorm, inputs, attrs);
	return reshape(res, $x.shape);
}
const batchNorm = op({ batchNorm_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm2d.js
/**
* Batch normalization, strictly for 2D. For the more relaxed version, see
* `tf.batchNorm`.
*
* @param x The input Tensor.
* @param mean A mean Tensor.
* @param variance A variance Tensor.
* @param offset An offset Tensor.
* @param scale A scale Tensor.
* @param varianceEpsilon A small float number to avoid dividing by 0.
*/
function batchNorm2d_(x, mean$1, variance, offset, scale, varianceEpsilon) {
	const $x = convertToTensor(x, "x", "batchNorm");
	const $mean = convertToTensor(mean$1, "mean", "batchNorm");
	const $variance = convertToTensor(variance, "variance", "batchNorm");
	let $scale;
	if (scale != null) $scale = convertToTensor(scale, "scale", "batchNorm");
	let $offset;
	if (offset != null) $offset = convertToTensor(offset, "offset", "batchNorm");
	assert($x.rank === 2, () => `Error in batchNorm2D: x must be rank 2 but got rank ${$x.rank}.`);
	assert($mean.rank === 2 || $mean.rank === 1, () => `Error in batchNorm2D: mean must be rank 2 or rank 1 but got rank ${$mean.rank}.`);
	assert($variance.rank === 2 || $variance.rank === 1, () => `Error in batchNorm2D: variance must be rank 2 or rank 1 but got rank ${$variance.rank}.`);
	if ($scale != null) assert($scale.rank === 2 || $scale.rank === 1, () => `Error in batchNorm2D: scale must be rank 2 or rank 1 but got rank ${$scale.rank}.`);
	if ($offset != null) assert($offset.rank === 2 || $offset.rank === 1, () => `Error in batchNorm2D: offset must be rank 2 or rank 1 but got rank ${$offset.rank}.`);
	return batchNorm($x, $mean, $variance, $offset, $scale, varianceEpsilon);
}
const batchNorm2d = op({ batchNorm2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm3d.js
/**
* Batch normalization, strictly for 3D. For the more relaxed version, see
* `tf.batchNorm`.
*
* @param x The input Tensor.
* @param mean A mean Tensor.
* @param variance A variance Tensor.
* @param offset An offset Tensor.
* @param scale A scale Tensor.
* @param varianceEpsilon A small float number to avoid dividing by 0.
*/
function batchNorm3d_(x, mean$1, variance, offset, scale, varianceEpsilon) {
	const $x = convertToTensor(x, "x", "batchNorm");
	const $mean = convertToTensor(mean$1, "mean", "batchNorm");
	const $variance = convertToTensor(variance, "variance", "batchNorm");
	let $scale;
	if (scale != null) $scale = convertToTensor(scale, "scale", "batchNorm");
	let $offset;
	if (offset != null) $offset = convertToTensor(offset, "offset", "batchNorm");
	assert($x.rank === 3, () => `Error in batchNorm3D: x must be rank 3 but got rank ${$x.rank}.`);
	assert($mean.rank === 3 || $mean.rank === 1, () => `Error in batchNorm3D: mean must be rank 3 or rank 1 but got rank ${$mean.rank}.`);
	assert($variance.rank === 3 || $variance.rank === 1, () => `Error in batchNorm3D: variance must be rank 3 or rank 1 but got rank ${$variance.rank}.`);
	if ($scale != null) assert($scale.rank === 3 || $scale.rank === 1, () => `Error in batchNorm3D: scale must be rank 3 or rank 1 but got rank ${$scale.rank}.`);
	if ($offset != null) assert($offset.rank === 3 || $offset.rank === 1, () => `Error in batchNorm3D: offset must be rank 3 or rank 1 but got rank ${$offset.rank}.`);
	return batchNorm($x, $mean, $variance, $offset, $scale, varianceEpsilon);
}
const batchNorm3d = op({ batchNorm3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/batchnorm4d.js
/**
* Batch normalization, strictly for 4D. For the more relaxed version, see
* `tf.batchNorm`.
*
* @param x The input Tensor.
* @param mean A mean Tensor.
* @param variance A variance Tensor.
* @param offset An offset Tensor.
* @param scale A scale Tensor.
* @param varianceEpsilon A small float number to avoid dividing by 0.
*/
function batchNorm4d_(x, mean$1, variance, offset, scale, varianceEpsilon) {
	const $x = convertToTensor(x, "x", "batchNorm");
	const $mean = convertToTensor(mean$1, "mean", "batchNorm");
	const $variance = convertToTensor(variance, "variance", "batchNorm");
	let $scale;
	if (scale != null) $scale = convertToTensor(scale, "scale", "batchNorm");
	let $offset;
	if (offset != null) $offset = convertToTensor(offset, "offset", "batchNorm");
	assert($x.rank === 4, () => `Error in batchNorm4D: x must be rank 4 but got rank ${$x.rank}.`);
	assert($mean.rank === 4 || $mean.rank === 1, () => `Error in batchNorm4D: mean must be rank 4 or rank 1 but got rank ${$mean.rank}.`);
	assert($variance.rank === 4 || $variance.rank === 1, () => `Error in batchNorm4D: variance must be rank 4 or rank 1 but got rank ${$variance.rank}.`);
	if ($scale != null) assert($scale.rank === 4 || $scale.rank === 1, () => `Error in batchNorm4D: scale must be rank 4 or rank 1 but got rank ${$scale.rank}.`);
	if ($offset != null) assert($offset.rank === 4 || $offset.rank === 1, () => `Error in batchNorm4D: offset must be rank 4 or rank 1 but got rank ${$offset.rank}.`);
	return batchNorm($x, $mean, $variance, $offset, $scale, varianceEpsilon);
}
const batchNorm4d = op({ batchNorm4d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/bincount.js
/**
* Outputs a vector with length `size` and the same dtype as `weights`.
*
* If `weights` are empty, then index `i` stores the number of times the value
* `i` is counted in `x`. If `weights` are non-empty, then index `i` stores the
* sum of the value in `weights` at each index where the corresponding value in
* `x` is `i`.
*
* Values in `x` outside of the range [0, size) are ignored.
*
* @param x The input int tensor, rank 1.
* @param weights The weights tensor, must have the same shape as x, or a
*     length-0 Tensor, in which case it acts as all weights equal to 1.
* @param size Non-negative integer.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function bincount_(x, weights, size) {
	const $x = convertToTensor(x, "x", "bincount");
	const $weights = convertToTensor(weights, "weights", "bincount");
	assert($x.dtype === "int32", () => `Error in bincount: input dtype must be int32, but got ${$x.dtype}`);
	assert(size >= 0, () => `size must be non-negative, but got ${size}.`);
	assert($weights.size === $x.size || $weights.size === 0, () => `Error in bincount: weights must have the same size as input or0-length, but got input shape: ${$x.shape}, weights shape: ${$weights.shape}.`);
	const inputs = {
		x: $x,
		weights: $weights
	};
	const attrs = { size };
	return ENGINE.runKernel(Bincount, inputs, attrs);
}
const bincount = op({ bincount_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/broadcast_args.js
/**
* Return the shape of s0 op s1 with broadcast.
*
* compute r0, the broadcasted shape as a tensor.
* s0, s1 and r0 are all integer vectors.
*
* This function returns the shape of the result of an operation between
* two tensors of size s0 and s1 performed with broadcast.
*
* @param s0 A tensor representing a shape
* @param s1 A tensor representing a shape
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function broadcastArgs_(s0, s1) {
	const shape1Input = convertToTensor(s0, "s0", "broadcastArgs", "int32");
	const shape2Input = convertToTensor(s1, "s1", "broadcastArgs", "int32");
	if (shape1Input.rank !== 1) throw new Error(`broadcastArgs(): first input must be a vector (rank=1). Has rank ${shape1Input.rank}`);
	if (shape2Input.rank !== 1) throw new Error(`broadcastArgs(): second input must be a vector (rank=1). Has rank ${shape2Input.rank}`);
	const inputs = {
		s0: shape1Input,
		s1: shape2Input
	};
	return ENGINE.runKernel(BroadcastArgs, inputs);
}
const broadcastArgs = op({ broadcastArgs_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/broadcast_to.js
/**
* Broadcast an array to a compatible shape NumPy-style.
*
* The tensor's shape is compared to the broadcast shape from end to beginning.
* Ones are prepended to the tensor's shape until is has the same length as
* the broadcast shape. If input.shape[i]==shape[i], the (i+1)-th axis is
* already broadcast-compatible. If input.shape[i]==1 and shape[i]==N, then
* the input tensor is tiled N times along that axis (using tf.tile).
*
* @param input The tensor that is to be broadcasted.
* @param shape The input is to be broadcast to this shape.
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function broadcastTo_(x, shape) {
	let input = convertToTensor(x, "broadcastTo", "x");
	const xShape = input.shape;
	if (shape.some((d) => !(d > 0) || d % 1 !== 0)) throw new Error(`broadcastTo(): Invalid broadcast shape [${shape}].`);
	if (shape.length < input.rank) throw new Error(`broadcastTo(): shape.length=${shape.length} < input.rank=${input.rank}.`);
	if (shape.length > input.rank) {
		const newShape = input.shape.slice();
		while (newShape.length < shape.length) newShape.unshift(1);
		input = reshape(input, newShape);
	}
	const inputShape = input.shape;
	const reps = Array.from(shape);
	for (let i = shape.length - 1; i >= 0; i--) if (inputShape[i] === shape[i]) reps[i] = 1;
	else if (input.shape[i] !== 1) throw new Error(`broadcastTo(): [${xShape}] cannot be broadcast to [${shape}].`);
	if (reps.map((n, i) => n > 1 ? i : -1).filter((i) => i >= 0).length === 0) return clone(input);
	const inputs = { x: input };
	const attrs = { reps };
	return ENGINE.runKernel(Tile, inputs, attrs);
}
const broadcastTo = op({ broadcastTo_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/ceil.js
/**
* Computes ceiling of input `tf.Tensor` element-wise: `ceil(x)`
*
* ```js
* const x = tf.tensor1d([.6, 1.1, -3.3]);
*
* x.ceil().print();  // or tf.ceil(x)
* ```
* @param x The input Tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function ceil_(x) {
	const inputs = { x: convertToTensor(x, "x", "ceil", "float32") };
	return ENGINE.runKernel(Ceil, inputs);
}
const ceil = op({ ceil_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/clip_by_value.js
/**
* Clips values element-wise. `max(min(x, clipValueMax), clipValueMin)`
*
* ```js
* const x = tf.tensor1d([-1, 2, -3, 4]);
*
* x.clipByValue(-2, 3).print();  // or tf.clipByValue(x, -2, 3)
* ```
* @param x The input tensor.
* @param clipValueMin Lower-bound of range to be clipped to.
* @param clipValueMax Upper-bound of range to be clipped to.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function clipByValue_(x, clipValueMin, clipValueMax) {
	const $x = convertToTensor(x, "x", "clipByValue");
	assert(clipValueMin <= clipValueMax, () => `Error in clip: min (${clipValueMin}) must be less than or equal to max (${clipValueMax}).`);
	const inputs = { x: $x };
	const attrs = {
		clipValueMin,
		clipValueMax
	};
	return ENGINE.runKernel(ClipByValue, inputs, attrs);
}
const clipByValue = op({ clipByValue_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/concat_1d.js
/**
* Concatenates a list of`tf.Tensor1D`s along an axis. See `concat` for details.
*
* For example, if:
* A: shape(3) = |r1, g1, b1|
* B: shape(2) = |r2, g2|
* C = tf.concat1d([A, B]) == |r1, g1, b1, r2, g2|
*
* @param tensors A list of`tf.Tensor`s to concatenate.
* @return The concatenated array.
*/
function concat1d_(tensors) {
	return concat(tensors, 0);
}
const concat1d = op({ concat1d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/concat_2d.js
/**
* Concatenates a list of`tf.Tensor2D`s along an axis. See `concat` for details.
*
* For example, if:
* A: shape(2, 3) = | r1, g1, b1 |
*                  | r2, g2, b2 |
*
* B: shape(2, 3) = | r3, g3, b3 |
*                  | r4, g4, b4 |
*
* C = tf.concat2d([A, B], axis)
*
* if axis = 0:
* C: shape(4, 3) = | r1, g1, b1 |
*                  | r2, g2, b2 |
*                  | r3, g3, b3 |
*                  | r4, g4, b4 |
*
* if axis = 1:
* C = shape(2, 6) = | r1, g1, b1, r3, g3, b3 |
*                   | r2, g2, b2, r4, g4, b4 |
*
*
* @param tensors A list of `tf.Tensor`s to concatenate.
* @param axis The axis to concatenate along.
* @return The concatenated array.
*/
function concat2d_(tensors, axis) {
	return concat(tensors, axis);
}
const concat2d = op({ concat2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/concat_3d.js
/**
* Concatenates a list of `tf.Tensor3D`s along an axis.
* See `concat` for details.
*
* For example, if:
* A: shape(2, 1, 3) = | r1, g1, b1 |
*                     | r2, g2, b2 |
*
* B: shape(2, 1, 3) = | r3, g3, b3 |
*                     | r4, g4, b4 |
*
* C = tf.concat3d([A, B], axis)
*
* if axis = 0:
* C: shape(4, 1, 3) = | r1, g1, b1 |
*                     | r2, g2, b2 |
*                     | r3, g3, b3 |
*                     | r4, g4, b4 |
*
* if axis = 1:
* C: shape(2, 2, 3) = | r1, g1, b1, r3, g3, b3 |
*                     | r2, g2, b2, r4, g4, b4 |
*
* if axis = 2:
* C = shape(2, 1, 6) = | r1, g1, b1, r3, g3, b3 |
*                      | r2, g2, b2, r4, g4, b4 |
*
* @param tensors A list of`tf.Tensor`s to concatenate.
* @param axis The axis to concate along.
* @return The concatenated array.
*/
function concat3d_(tensors, axis) {
	return concat(tensors, axis);
}
const concat3d = op({ concat3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/concat_4d.js
/**
* Concatenates a list of `tf.Tensor4D`s along an axis.
* See `concat` for details.
*
* @param tensors A list of `tf.Tensor`s to concatenate.
* @param axis The axis to concate along.
* @return The concatenated array.
*/
function concat4d_(tensors, axis) {
	return concat(tensors, axis);
}
const concat4d = op({ concat4d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv2d.js
/**
* Computes a 2D convolution over the input x.
*
* @param x The input tensor, of rank 4 or rank 3, of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
* assumed.
* @param filter The filter, rank 4, of shape
*     `[filterHeight, filterWidth, inDepth, outDepth]`.
* @param strides The strides of the convolution: `[strideHeight,
* strideWidth]`.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*   - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
*     "NHWC". Specify the data format of the input and output data. With the
*     default format "NHWC", the data is stored in the order of: [batch,
*     height, width, channels].
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single
*     number, then `dilationHeight == dilationWidth`. If it is greater than
*     1, then all values of `strides` must be 1.
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function conv2d_(x, filter, strides, pad$1, dataFormat = "NHWC", dilations = [1, 1], dimRoundingMode) {
	const $x = convertToTensor(x, "x", "conv2d", "float32");
	const $filter = convertToTensor(filter, "filter", "conv2d", "float32");
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	assert(x4D.rank === 4, () => `Error in conv2d: input must be rank 4, but got rank ${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in conv2d: filter must be rank 4, but got rank ${$filter.rank}.`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in conv2d: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inDepth = dataFormat === "NHWC" ? x4D.shape[3] : x4D.shape[1];
	assert(inDepth === $filter.shape[2], () => `Error in conv2d: depth of input (${inDepth}) must match input depth for filter ${$filter.shape[2]}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => `Error in conv2D: Either strides or dilations must be 1. Got strides ${strides} and dilations '${dilations}'`);
	const inputs = {
		x: x4D,
		filter: $filter
	};
	const attrs = {
		strides,
		pad: pad$1,
		dataFormat,
		dilations,
		dimRoundingMode
	};
	const res = ENGINE.runKernel(Conv2D, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const conv2d = op({ conv2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv1d.js
/**
* Computes a 1D convolution over the input x.
*
* @param x The input tensor, of rank 3 or rank 2, of shape
*     `[batch, width, inChannels]`. If rank 2, batch of 1 is assumed.
* @param filter The filter, rank 3, of shape
*     `[filterWidth, inDepth, outDepth]`.
* @param stride The number of entries by which the filter is moved right at
*     each step.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*   - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dataFormat An optional string from "NWC", "NCW". Defaults to "NWC",
*     the data is stored in the order of [batch, in_width, in_channels]. Only
*     "NWC" is currently supported.
* @param dilation The dilation rate in which we sample input values in
*     atrous convolution. Defaults to `1`. If it is greater than 1, then
*     stride must be `1`.
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function conv1d_(x, filter, stride, pad$1, dataFormat = "NWC", dilation = 1, dimRoundingMode) {
	const $x = convertToTensor(x, "x", "conv1d");
	const $filter = convertToTensor(filter, "filter", "conv1d");
	let x3D = $x;
	let reshapedTo3D = false;
	if ($x.rank === 2) {
		reshapedTo3D = true;
		x3D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1]
		]);
	}
	assert(x3D.rank === 3, () => `Error in conv1d: input must be rank 3, but got rank ${x3D.rank}.`);
	assert($filter.rank === 3, () => `Error in conv1d: filter must be rank 3, but got rank ${$filter.rank}.`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in conv1d: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	assert(x3D.shape[2] === $filter.shape[1], () => `Error in conv1d: depth of input (${x3D.shape[2]}) must match input depth for filter ${$filter.shape[1]}.`);
	assert(eitherStridesOrDilationsAreOne(stride, dilation), () => `Error in conv1D: Either stride or dilation must be 1. Got stride ${stride} and dilation '${dilation}'`);
	assert(dataFormat === "NWC", () => `Error in conv1d: got dataFormat of ${dataFormat} but only NWC is currently supported.`);
	const filter4D = reshape($filter, [
		1,
		$filter.shape[0],
		$filter.shape[1],
		$filter.shape[2]
	]);
	const input4D = reshape(x3D, [
		x3D.shape[0],
		1,
		x3D.shape[1],
		x3D.shape[2]
	]);
	const res = conv2d(input4D, filter4D, [1, stride], pad$1, "NHWC", [1, dilation], dimRoundingMode);
	if (reshapedTo3D) return reshape(res, [res.shape[2], res.shape[3]]);
	return reshape(res, [
		res.shape[0],
		res.shape[2],
		res.shape[3]
	]);
}
const conv1d = op({ conv1d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv2d_backprop_input.js
/**
* Computes the derivative of the input of a 2D convolution.
*
* @param xShape The shape of the input: [batch, height, width, inDepth].
* If length of 3, batch of 1 is assumed.
* @param dy The derivative of the output, of rank 4 or rank 3 of shape
*   `[batch, outHeight, outWidth, outDepth]`. If rank 3, batch of 1 is
* assumed.
* @param filter The filter, rank 4, of shape
*     `[filterHeight, filterWidth, inDepth, outDepth]`.
* @param strides The strides of the convolution: `[strideHeight,
* strideWidth]`.
* @param pad The type of padding algorithm used:
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
* @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
*     "NHWC". Specify the data format of the input and output data. With the
*     default format "NHWC", the data is stored in the order of: [batch,
*     height, width, channels].
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*/
function conv2DBackpropInput_(xShape, dy, filter, strides, pad$1, dataFormat = "NHWC", dimRoundingMode) {
	assert(xShape.length === dy.rank, () => `Length of inShape (${xShape.length}) and rank of dy (${dy.rank}) must match`);
	let xShape4D = xShape;
	let dy4D = dy;
	let reshapedTo4D = false;
	if (dy.rank === 3) {
		reshapedTo4D = true;
		dy4D = reshape(dy, [
			1,
			dy.shape[0],
			dy.shape[1],
			dy.shape[2]
		]);
		xShape4D = [
			1,
			xShape[0],
			xShape[1],
			xShape[2]
		];
	}
	assert(xShape4D.length === 4, () => `Error in conv2dDerInput: inShape must be length 4, but got length ${xShape4D.length}.`);
	assert(dy4D.rank === 4, () => `Error in conv2dDerInput: dy must be rank 4, but got rank ${dy4D.rank}`);
	assert(filter.rank === 4, () => `Error in conv2dDerInput: filter must be rank 4, but got rank ${filter.rank}`);
	const inDepth = dataFormat === "NHWC" ? xShape4D[3] : xShape4D[1];
	const outDepth = dataFormat === "NHWC" ? dy4D.shape[3] : dy4D.shape[1];
	assert(inDepth === filter.shape[2], () => `Error in conv2dDerInput: depth of input (${inDepth}) must match input depth for filter ${filter.shape[2]}.`);
	assert(outDepth === filter.shape[3], () => `Error in conv2dDerInput: depth of output (${outDepth}) must match output depth for filter ${filter.shape[3]}.`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inputs = {
		dy: dy4D,
		filter
	};
	const attrs = {
		strides,
		pad: pad$1,
		dataFormat,
		dimRoundingMode,
		inputShape: xShape4D
	};
	const res = ENGINE.runKernel(Conv2DBackpropInput, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const conv2DBackpropInput = op({ conv2DBackpropInput_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv2d_transpose.js
/**
* Computes the transposed 2D convolution of an image, also known as a
* deconvolution.
*
* @param x The input image, of rank 4 or rank 3, of shape
*   `[batch, height, width, inDepth]`. If rank 3, batch of 1 is assumed.
* @param filter The filter, rank 4, of shape
*     `[filterHeight, filterWidth, outDepth, inDepth]`.
*     `inDepth` must match `inDepth` in `x`.
* @param outputShape Output shape, of rank 4 or rank 3:
*     `[batch, height, width, outDepth]`. If rank 3, batch of 1 is assumed.
* @param strides The strides of the original convolution:
*     `[strideHeight, strideWidth]`.
* @param pad  The type of padding algorithm used in the non-transpose version
*    of the op.
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function conv2dTranspose_(x, filter, outputShape, strides, pad$1, dimRoundingMode) {
	const $x = convertToTensor(x, "x", "conv2dTranspose");
	const $filter = convertToTensor(filter, "filter", "conv2dTranspose");
	return conv2DBackpropInput(outputShape, $x, $filter, strides, pad$1, "NHWC", dimRoundingMode);
}
const conv2dTranspose = op({ conv2dTranspose_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv3d.js
/**
* Computes a 3D convolution over the input x.
*
* @param x The input tensor, of rank 5 or rank 4, of shape
*     `[batch, depth, height, width, channels]`. If rank 4,
* batch of 1 is assumed.
* @param filter The filter, rank 5, of shape
*     `[filterDepth, filterHeight, filterWidth, inChannels, outChannels]`.
*      inChannels must match between input and filter.
* @param strides The strides of the convolution: `[strideDepth, strideHeight,
* strideWidth]`.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*   - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dataFormat: An optional string from: "NDHWC", "NCDHW". Defaults to
*     "NDHWC". Specify the data format of the input and output data. With the
*     default format "NDHWC", the data is stored in the order of: [batch,
*     depth, height, width, channels]. Only "NDHWC" is currently supported.
* @param dilations The dilation rates: `[dilationDepth, dilationHeight,
*     dilationWidth]` in which we sample input values across the height
*     and width dimensions in atrous convolution. Defaults to `[1, 1, 1]`.
*     If `dilations` is a single number, then
*     `dilationDepth == dilationHeight == dilationWidth`. If it is greater
*     than 1, then all values of `strides` must be 1.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function conv3d_(x, filter, strides, pad$1, dataFormat = "NDHWC", dilations = [
	1,
	1,
	1
]) {
	const $x = convertToTensor(x, "x", "conv3d");
	const $filter = convertToTensor(filter, "filter", "conv3d");
	let x5D = $x;
	let reshapedTo5D = false;
	if ($x.rank === 4) {
		reshapedTo5D = true;
		x5D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2],
			$x.shape[3]
		]);
	}
	assert(x5D.rank === 5, () => `Error in conv3d: input must be rank 5, but got rank ${x5D.rank}.`);
	assert($filter.rank === 5, () => `Error in conv3d: filter must be rank 5, but got rank ${$filter.rank}.`);
	assert(x5D.shape[4] === $filter.shape[3], () => `Error in conv3d: depth of input (${x5D.shape[4]}) must match input depth for filter ${$filter.shape[3]}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => `Error in conv3D: Either strides or dilations must be 1. Got strides ${strides} and dilations '${dilations}'`);
	assert(dataFormat === "NDHWC", () => `Error in conv3d: got dataFormat of ${dataFormat} but only NDHWC is currently supported.`);
	const inputs = {
		x: x5D,
		filter: $filter
	};
	const attrs = {
		strides,
		pad: pad$1,
		dataFormat,
		dilations
	};
	const res = ENGINE.runKernel(Conv3D, inputs, attrs);
	if (reshapedTo5D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3],
		res.shape[4]
	]);
	return res;
}
const conv3d = op({ conv3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv3d_backprop_input.js
/**
* Computes the derivative of the input of a 3D convolution.
*
* @param xShape The shape of the input: [batch, depth, height, width,
* in_channels]. If length of 4, batch of 1 is assumed.
* @param dy The derivative of the output, of rank 5 or rank 4 of shape
*   `[batch, outDepth, outHeight, outWidth, in_channels]`.
* If rank 4, batch of 1 is assumed.
* @param filter The filter, rank 5, of shape
*     `[filterDepth, filterHeight, filterWidth, inDepth, outDepth]`.
* @param strides The strides of the convolution: `[strideDepth, strideHeight,
* strideWidth]`.
* @param pad The type of padding algorithm used:
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*/
function conv3DBackpropInput_(xShape, dy, filter, strides, pad$1) {
	assert(xShape.length === dy.rank, () => `Length of inShape (${xShape.length}) and rank of dy (${dy.rank}) must match`);
	let xShape5D = xShape;
	let dy5D = dy;
	let reshapedTo5D = false;
	if (dy.rank === 4) {
		reshapedTo5D = true;
		dy5D = reshape(dy, [
			1,
			dy.shape[0],
			dy.shape[1],
			dy.shape[2],
			dy.shape[3]
		]);
		xShape5D = [
			1,
			xShape[0],
			xShape[1],
			xShape[2],
			xShape[3]
		];
	}
	const inDepth = xShape5D[4];
	const outDepth = dy5D.shape[4];
	assert(xShape5D.length === 5, () => `Error in conv3dDerInput: inShape must be length 5, but got length ${xShape5D.length}.`);
	assert(dy5D.rank === 5, () => `Error in conv3dDerInput: dy must be rank 5, but got rank ${dy5D.rank}`);
	assert(filter.rank === 5, () => `Error in conv3dDerInput: filter must be rank 5, but got rank ${filter.rank}`);
	assert(inDepth === filter.shape[3], () => `Error in conv3dDerInput: depth of input (${inDepth}) must match input depth for filter ${filter.shape[3]}.`);
	assert(outDepth === filter.shape[4], () => `Error in conv3dDerInput: depth of output (${outDepth}) must match output depth for filter ${filter.shape[4]}.`);
	const inputs = {
		dy: dy5D,
		filter
	};
	const attrs = {
		pad: pad$1,
		strides,
		inputShape: xShape5D
	};
	const res = ENGINE.runKernel(Conv3DBackpropInputV2, inputs, attrs);
	if (reshapedTo5D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3],
		res.shape[4]
	]);
	return res;
}
const conv3DBackpropInput = op({ conv3DBackpropInput_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv3d_transpose.js
/**
* Computes the transposed 3D convolution of a volume, also known as a
* deconvolution.
*
* @param x The input image, of rank 5 or rank 4, of shape
*   `[batch, depth, height, width, inDepth]`. If rank 4, batch of 1 is assumed.
* @param filter The filter, rank 4, of shape
*     `[depth, filterHeight, filterWidth, outDepth, inDepth]`.
*     `inDepth` must match `inDepth` in `x`.
* @param outputShape Output shape, of rank 5 or rank 4:
*     `[batch, depth, height, width, outDepth]`. If rank 3, batch of 1 is
*    assumed.
* @param strides The strides of the original convolution:
*     `[strideDepth, strideHeight, strideWidth]`.
* @param pad  The type of padding algorithm used in the non-transpose version
*    of the op.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function conv3dTranspose_(x, filter, outputShape, strides, pad$1) {
	const $x = convertToTensor(x, "x", "conv3dTranspose");
	const $filter = convertToTensor(filter, "filter", "conv3dTranspose");
	return conv3DBackpropInput(outputShape, $x, $filter, strides, pad$1);
}
const conv3dTranspose = op({ conv3dTranspose_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/cos.js
/**
* Computes cos of the input `tf.Tensor` element-wise: `cos(x)`
*
* ```js
* const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);
*
* x.cos().print();  // or tf.cos(x)
* ```
* @param x The input tensor. Must be float32 type.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function cos_(x) {
	const inputs = { x: convertToTensor(x, "x", "cos", "float32") };
	return ENGINE.runKernel(Cos, inputs);
}
const cos = op({ cos_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/cosh.js
/**
* Computes hyperbolic cos of the input `tf.Tensor` element-wise: `cosh(x)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.cosh().print();  // or tf.cosh(x)
* ```
* @param x The input tensor. Must be float32 type.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function cosh_(x) {
	const inputs = { x: convertToTensor(x, "x", "cosh", "float32") };
	return ENGINE.runKernel(Cosh, inputs);
}
const cosh = op({ cosh_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/cumsum.js
/**
* Computes the cumulative sum of a `tf.Tensor` along `axis`.
*
* ```js
* const x = tf.tensor([1, 2, 3, 4]);
* x.cumsum().print();
* ```
* ```js
* const x = tf.tensor([[1, 2], [3, 4]]);
* x.cumsum().print();
* ```
*
* @param x The input tensor to be summed.
* @param axis The axis along which to sum. Optional. Defaults to 0.
* @param exclusive Whether to perform exclusive cumulative sum. Optional.
*     Defaults to false. If set to true then the sum of each tensor entry
*     does not include its own value, but only the values previous to it
*     along the specified axis.
* @param reverse Whether to sum in the opposite direction. Optional.
*     Defaults to false.
*
* @doc {heading: 'Operations', subheading: 'Scan'}
*/
function cumsum_(x, axis = 0, exclusive = false, reverse$1 = false) {
	const inputs = { x: convertToTensor(x, "x", "cumsum") };
	const attrs = {
		axis,
		exclusive,
		reverse: reverse$1
	};
	return ENGINE.runKernel(Cumsum, inputs, attrs);
}
const cumsum = op({ cumsum_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/dense_bincount.js
/**
* Outputs a vector with length `size` and the same dtype as `weights`.
*
* If `weights` are empty, then index `i` stores the number of times the value
* `i` is counted in `x`. If `weights` are non-empty, then index `i` stores the
* sum of the value in `weights` at each index where the corresponding value in
* `x` is `i`.
*
* Values in `x` outside of the range [0, size) are ignored.
*
* @param x The input int tensor, rank 1 or rank 2.
* @param weights The weights tensor, must have the same shape as x, or a
*     length-0 Tensor, in which case it acts as all weights equal to 1.
* @param size Non-negative integer.
* @param binaryOutput Optional. Whether the kernel should count the appearance
*     or number of occurrences. Defaults to False.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function denseBincount_(x, weights, size, binaryOutput = false) {
	const $x = convertToTensor(x, "x", "denseBincount");
	const $weights = convertToTensor(weights, "weights", "denseBincount");
	assert($x.dtype === "int32", () => `Error in denseBincount: input dtype must be int32, but got ${$x.dtype}`);
	assert($x.rank <= 2, () => `Error in denseBincount: input must be at most rank 2, but got rank ${$x.rank}.`);
	assert(size >= 0, () => `size must be non-negative, but got ${size}.`);
	assert($weights.size === $x.size || $weights.size === 0, () => `Error in denseBincount: weights must have the same shape as x or 0-length, but got x shape: ${$x.shape}, weights shape: ${$weights.shape}.`);
	const inputs = {
		x: $x,
		weights: $weights
	};
	const attrs = {
		size,
		binaryOutput
	};
	return ENGINE.runKernel(DenseBincount, inputs, attrs);
}
const denseBincount = op({ denseBincount_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/depth_to_space.js
/**
* Rearranges data from depth into blocks of spatial data. More specifically,
* this op outputs a copy of the input tensor where values from the `depth`
* dimension are moved in spatial blocks to the `height` and `width` dimensions.
* The attr `blockSize` indicates the input block size and how the data is
* moved.
*
*  - Chunks of data of size `blockSize * blockSize` from depth are rearranged
* into non-overlapping blocks of size `blockSize x blockSize`
*
*  - The width the output tensor is `inputWidth * blockSize`, whereas the
* height is `inputHeight * blockSize`
*
*  - The Y, X coordinates within each block of the output image are determined
* by the high order component of the input channel index
*
*  - The depth of the input tensor must be divisible by `blockSize *
* blockSize`
*
* The `dataFormat` attr specifies the layout of the input and output tensors
* with the following options: "NHWC": [ `batch, height, width, channels` ]
* "NCHW": [ `batch, channels, height, width` ]
*
* ```js
* const x = tf.tensor4d([1, 2, 3, 4], [1, 1, 1, 4]);
* const blockSize = 2;
* const dataFormat = "NHWC";
*
* tf.depthToSpace(x, blockSize, dataFormat).print();
* ```
*
* @param x The input tensor of rank 4
* @param blockSIze  An `int` that is `>= 2`. The size of the spatial block
* @param dataFormat An optional string from: "NHWC", "NCHW". Defaults to "NHWC"
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function depthToSpace_(x, blockSize, dataFormat = "NHWC") {
	const $x = convertToTensor(x, "x", "depthToSpace", "float32");
	const inputHeight = dataFormat === "NHWC" ? $x.shape[1] : $x.shape[2];
	const inputWidth = dataFormat === "NHWC" ? $x.shape[2] : $x.shape[3];
	const inputDepth = dataFormat === "NHWC" ? $x.shape[3] : $x.shape[1];
	assert(blockSize > 1, () => `blockSize should be > 1 for depthToSpace, but was: ${blockSize}`);
	assert(inputHeight * blockSize >= 0, () => `Negative dimension size caused by overflow when multiplying
    ${inputHeight} and ${blockSize}  for depthToSpace with input shape
    ${$x.shape}`);
	assert(inputWidth * blockSize >= 0, () => `Negative dimension size caused by overflow when multiplying
    ${inputWidth} and ${blockSize} for depthToSpace with input shape
        ${$x.shape}`);
	assert(inputDepth % (blockSize * blockSize) === 0, () => `Dimension size must be evenly divisible by ${blockSize * blockSize} but is ${inputDepth} for depthToSpace with input shape ${$x.shape}`);
	const inputs = { x: $x };
	const attrs = {
		blockSize,
		dataFormat
	};
	return ENGINE.runKernel(DepthToSpace, inputs, attrs);
}
const depthToSpace = op({ depthToSpace_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/depthwise_conv2d.js
/**
* Depthwise 2D convolution.
*
* Given a 4D `input` array and a `filter` array of shape
* `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing
* `inChannels` convolutional filters of depth 1, this op applies a
* different filter to each input channel (expanding from 1 channel to
* `channelMultiplier` channels for each), then concatenates the results
* together. The output has `inChannels * channelMultiplier` channels.
*
* See
* [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](
*     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)
* for more details.
*
* @param x The input tensor, of rank 4 or rank 3, of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
* assumed.
* @param filter The filter tensor, rank 4, of shape
*     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.
* @param strides The strides of the convolution: `[strideHeight,
* strideWidth]`. If strides is a single number, then `strideHeight ==
* strideWidth`.
* @param pad The type of padding algorithm.
*   - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*   - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*   - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single
*     number, then `dilationHeight == dilationWidth`. If it is greater than
*     1, then all values of `strides` must be 1.
* @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
*     "NHWC". Specify the data format of the input and output data. With the
*     default format "NHWC", the data is stored in the order of: [batch,
*     height, width, channels]. Only "NHWC" is currently supported.
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function depthwiseConv2d_(x, filter, strides, pad$1, dataFormat = "NHWC", dilations = [1, 1], dimRoundingMode) {
	const $x = convertToTensor(x, "x", "depthwiseConv2d", "float32");
	const $filter = convertToTensor(filter, "filter", "depthwiseConv2d", "float32");
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	assert(x4D.rank === 4, () => `Error in depthwiseConv2d: input must be rank 4, but got rank ${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in depthwiseConv2d: filter must be rank 4, but got rank ${$filter.rank}.`);
	assert(x4D.shape[3] === $filter.shape[2], () => `Error in depthwiseConv2d: number of input channels (${x4D.shape[3]}) must match the inChannels dimension in filter ${$filter.shape[2]}.`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inputs = {
		x: x4D,
		filter: $filter
	};
	const attrs = {
		strides,
		pad: pad$1,
		dataFormat,
		dilations,
		dimRoundingMode
	};
	const res = ENGINE.runKernel(DepthwiseConv2dNative, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const depthwiseConv2d = op({ depthwiseConv2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/diag.js
/**
* Returns a diagonal tensor with a given diagonal values.
*
* Given a diagonal, this operation returns a tensor with the diagonal and
* everything else padded with zeros.
*
* Assume the input has dimensions `[D1,..., Dk]`, then the output is a tensor
* of rank 2k with dimensions `[D1,..., Dk, D1,..., Dk]`
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
*
* tf.diag(x).print()
* ```
* ```js
* const x = tf.tensor2d([1, 2, 3, 4, 5, 6, 6, 8], [4, 2])
*
* tf.diag(x).print()
* ```
* @param x The input tensor.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function diag_(x) {
	const inputs = { x: convertToTensor(x, "x", "diag") };
	return ENGINE.runKernel(Diag, inputs);
}
const diag = op({ diag_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/dilation2d.js
/**
* Computes the grayscale dilation over the input `x`.
*
* @param x The input tensor, rank 3 or rank 4 of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
* @param filter The filter tensor, rank 3, of shape
*     `[filterHeight, filterWidth, depth]`.
* @param strides The strides of the sliding window for each dimension of the
*     input tensor: `[strideHeight, strideWidth]`.
*     If `strides` is a single number,
*     then `strideHeight == strideWidth`.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1*1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dataFormat Specify the data format of the input and output data.
*      Defaults to 'NHWC'. Only 'NHWC' is currently supported. With the
*      default format "NHWC", the data is stored in the order of: [batch,
*      height, width, channels].
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     for atrous morphological dilation. Defaults to `[1, 1]`. If `dilations`
*     is a single number, then `dilationHeight == dilationWidth`. If it is
*     greater than 1, then all values of `strides` must be 1.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function dilation2d_(x, filter, strides, pad$1, dilations = [1, 1], dataFormat = "NHWC") {
	const $x = convertToTensor(x, "x", "dilation2d");
	const $filter = convertToTensor(filter, "filter", "dilation2d");
	assert($x.rank === 3 || $x.rank === 4, () => `Error in dilation2d: input must be rank 3 or 4, but got rank ${$x.rank}.`);
	assert($filter.rank === 3, () => `Error in dilation2d: filter must be rank 3, but got rank ${$filter.rank}.`);
	assert(dataFormat === "NHWC", () => `Error in dilation2d: Only NHWC is currently supported, but got dataFormat of ${dataFormat}`);
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
		reshapedTo4D = true;
	}
	const inputs = {
		x: x4D,
		filter: $filter
	};
	const attrs = {
		strides,
		pad: pad$1,
		dilations
	};
	const res = ENGINE.runKernel(Dilation2D, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const dilation2d = op({ dilation2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/broadcast_util.js
/**
* @license
* Copyright 2017 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
* Returns the dimensions in the input shape that are broadcasted to
* produce the provided output shape.
*
* The returned dimensions are 0-indexed and sorted. An example:
* inShape = [4, 1, 3]
* outShape = [5, 4, 3, 3]
* result = [1]. Dimension 1 (2nd dimension of input) gets broadcasted 1 => 3.
*/
function getBroadcastDims(inShape, outShape) {
	const inRank = inShape.length;
	const dims = [];
	for (let i = 0; i < inRank; i++) {
		const dim = inRank - 1 - i;
		const a = inShape[dim] || 1;
		if ((outShape[outShape.length - 1 - i] || 1) > 1 && a === 1) dims.unshift(dim);
	}
	return dims;
}
/**
* Returns the axes in the output space that should be reduced to produce
* the input space.
*/
function getReductionAxes(inShape, outShape) {
	const result = [];
	for (let i = 0; i < outShape.length; i++) {
		const inDim = inShape[inShape.length - i - 1];
		const outAxis = outShape.length - i - 1;
		const outDim = outShape[outAxis];
		if (inDim == null || inDim === 1 && outDim > 1) result.unshift(outAxis);
	}
	return result;
}
function assertAndGetBroadcastShape(shapeA, shapeB) {
	const result = [];
	const l = Math.max(shapeA.length, shapeB.length);
	for (let i = 0; i < l; i++) {
		let a = shapeA[shapeA.length - i - 1];
		if (a == null) a = 1;
		let b = shapeB[shapeB.length - i - 1];
		if (b == null) b = 1;
		if (a === 1) result.unshift(b);
		else if (b === 1) result.unshift(a);
		else if (a !== b) {
			const errMsg = `Operands could not be broadcast together with shapes ${shapeA} and ${shapeB}.`;
			throw Error(errMsg);
		} else result.unshift(a);
	}
	return result;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/equal.js
/**
* Returns the truth value of (a == b) element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
* const b = tf.tensor1d([2, 2, 2]);
*
* a.equal(b).print();
* ```
*
* @param a The first input tensor.
* @param b The second input tensor. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function equal_(a, b) {
	let $a = convertToTensor(a, "a", "equal", "string_or_numeric");
	let $b = convertToTensor(b, "b", "equal", "string_or_numeric");
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Equal, inputs);
}
const equal = op({ equal_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/where.js
/**
* Returns the elements, either `a` or `b` depending on the `condition`.
*
* If the condition is true, select from `a`, otherwise select from `b`.
*
* ```js
* const cond = tf.tensor1d([false, false, true], 'bool');
* const a = tf.tensor1d([1 , 2, 3]);
* const b = tf.tensor1d([-1, -2, -3]);
*
* a.where(cond, b).print();
* ```
*
* @param condition The input condition. Must be of dtype bool.
* @param a If `condition` is rank 1, `a` may have a higher rank but
*     its first dimension must match the size of `condition`.
* @param b A tensor with the same dtype as `a` and with shape that is
*     compatible with `a`.
* @return A tensor with same dtype as `a` and `b`, and shape that is
*     broadcastable from `a` and `b`.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function where_(condition, a, b) {
	const $a = convertToTensor(a, "a", "where");
	const $b = convertToTensor(b, "b", "where");
	const $condition = convertToTensor(condition, "condition", "where", "bool");
	const broadcastShape = assertAndGetBroadcastShape(assertAndGetBroadcastShape($condition.shape, $a.shape), $b.shape);
	const $broadcastedCondition = broadcastTo($condition, broadcastShape);
	const $broadcastedA = broadcastTo($a, broadcastShape);
	const $broadcastedB = broadcastTo($b, broadcastShape);
	const inputs = {
		condition: $broadcastedCondition,
		t: $broadcastedA,
		e: $broadcastedB
	};
	return ENGINE.runKernel(Select, inputs);
}
const where = op({ where_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/zeros_like.js
/**
* Creates a `tf.Tensor` with all elements set to 0 with the same shape as the
* given tensor.
*
* ```js
* const x = tf.tensor([1, 2]);
* tf.zerosLike(x).print();
* ```
*
* @param x The tensor of required shape.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function zerosLike_(x) {
	const inputs = { x: convertToTensor(x, "x", "zerosLike") };
	return ENGINE.runKernel(ZerosLike, inputs);
}
const zerosLike = op({ zerosLike_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/div_no_nan.js
/**
* Divides two `tf.Tensor`s element-wise, A / B. Supports broadcasting. Return 0
* if denominator is 0.
*
*
* ```js
* const a = tf.tensor1d([1, 4, 9, 16]);
* const b = tf.tensor1d([1, 2, 3, 4]);
* const c = tf.tensor1d([0, 0, 0, 0]);
*
* a.divNoNan(b).print();  // or tf.divNoNan(a, b)
* a.divNoNan(c).print();  // or tf.divNoNan(a, c)
* ```
*
* ```js
* // Broadcast div a with b.
* const a = tf.tensor1d([2, 4, 6, 8]);
* const b = tf.scalar(2);
* const c = tf.scalar(0);
*
* a.divNoNan(b).print();  // or tf.divNoNan(a, b)
* a.divNoNan(c).print();  // or tf.divNoNan(a, c)
* ```
*
* @param a The first tensor as the numerator.
* @param b The second tensor as the denominator. Must have the same dtype as
* `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function divNoNan_(a, b) {
	let $a = convertToTensor(a, "a", "div");
	let $b = convertToTensor(b, "b", "div");
	[$a, $b] = makeTypesMatch($a, $b);
	const divResult = div($a, $b);
	const zeros$1 = zerosLike(divResult);
	const bEqualsZero = equal($b, zeros$1);
	return where(bEqualsZero, zeros$1, divResult);
}
const divNoNan = op({ divNoNan_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/dot.js
/**
* Computes the dot product of two matrices and/or vectors, `t1` and `t2`.
*
* ```js
* const a = tf.tensor1d([1, 2]);
* const b = tf.tensor2d([[1, 2], [3, 4]]);
* const c = tf.tensor2d([[1, 2, 3], [4, 5, 6]]);
*
* a.dot(b).print();  // or tf.dot(a, b)
* b.dot(a).print();
* b.dot(c).print();
* ```
* @param t1 The first tensor in the dot operation.
* @param t2 The second tensor in the dot operation.
*
* @doc {heading: 'Operations', subheading: 'Matrices'}
*/
function dot_(t1, t2) {
	const $t1 = convertToTensor(t1, "t1", "dot");
	const $t2 = convertToTensor(t2, "t2", "dot");
	assert(($t1.rank === 1 || $t1.rank === 2) && ($t2.rank === 1 || $t2.rank === 2), () => `Error in dot: inputs must all be rank 1 or 2, but got ranks ${$t1.rank} and ${$t2.rank}.`);
	const t1Inner = $t1.rank === 1 ? $t1.size : $t1.shape[1];
	const t2Inner = $t2.rank === 1 ? $t2.size : $t2.shape[0];
	assert(t1Inner === t2Inner, () => `Error in dot: inner dimensions of inputs must match, but got ${t1Inner} and ${t2Inner}.`);
	if ($t1.rank === 1 && $t2.rank === 1) {
		const t12D = reshape($t1, [1, -1]);
		const t22D = reshape($t2, [-1, 1]);
		const t1t2 = matMul(t12D, t22D);
		return reshape(t1t2, []);
	} else if ($t1.rank === 1 && $t2.rank === 2) {
		const t12D = reshape($t1, [1, -1]);
		const t22D = reshape($t2, [$t2.shape[0], $t2.shape[1]]);
		const t1t2 = matMul(t12D, t22D);
		return reshape(t1t2, [t1t2.size]);
	} else if ($t1.rank === 2 && $t2.rank === 1) {
		const t22D = reshape($t2, [-1, 1]);
		const t1t2 = matMul($t1, t22D);
		return reshape(t1t2, [t1t2.size]);
	} else {
		const t22D = reshape($t2, [$t2.shape[0], $t2.shape[1]]);
		return matMul($t1, t22D);
	}
}
const dot = op({ dot_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/einsum.js
/**
* Tensor contraction over specified indices and outer product.
*
* `einsum` allows defining Tensors by defining their element-wise computation.
* This computation is based on
* [Einstein summation](https://en.wikipedia.org/wiki/Einstein_notation).
*
* Some special cases include:
*
* Matrix multiplication:
* ```js
* const x = tf.tensor2d([[1, 2, 3], [4, 5, 6]]);
* const y = tf.tensor2d([[0, 1], [2, 3], [4, 5]]);
* x.print();
* y.print();
* tf.einsum('ij,jk->ik', x, y).print();
* ```
*
* Dot product:
* ```js
* const x = tf.tensor1d([1, 2, 3]);
* const y = tf.tensor1d([0, 1, 2]);
* x.print();
* y.print();
* tf.einsum('i,i->', x, y).print();
* ```
*
* Batch dot product:
* ```js
* const x = tf.tensor2d([[1, 2, 3], [4, 5, 6]]);
* const y = tf.tensor2d([[0, 1, 2], [3, 4, 5]]);
* x.print();
* y.print();
* tf.einsum('bi,bi->b', x, y).print();
* ```
*
* Outer prouduct:
* ```js
* const x = tf.tensor1d([1, 3, 5]);
* const y = tf.tensor1d([2, 4, 6]);
* x.print();
* y.print();
* tf.einsum('i,j->ij', x, y).print();
* ```
*
* Matrix transpose:
* ```js
* const x = tf.tensor2d([[1, 2], [3, 4]]);
* x.print();
* tf.einsum('ij->ji', x).print();
* ```
*
* Batch matrix transpose:
* ```js
* const x = tf.tensor3d([[[1, 2], [3, 4]], [[-1, -2], [-3, -4]]]);
* x.print();
* tf.einsum('bij->bji', x).print();
* ```
*
* Limitations:
*
* This implementation of einsum has the following limitations:
*
* - Does not support >2 input tensors.
* - Does not support duplicate axes for any given input tensor. E.g., equation
*   'ii->' is not suppoted.
* - The `...` notation is not supported.
*
* @param equation a string describing the contraction, in the same format as
* [numpy.einsum](https://numpy.org/doc/stable/reference/generated/numpy.einsum.html).
* @param tensors the input(s) to contract (each one a Tensor), whose shapes
*     should be consistent with equation.
* @returns The output tensor.
*
* @doc {heading: 'Tensors', subheading: 'Matrices'}
*/
function einsum_(equation, ...tensors) {
	const $tensors = tensors.map((t, i) => convertToTensor(t, `tensors${i}`, "einsum"));
	const attrs = { equation };
	return ENGINE.runKernel(Einsum, $tensors, attrs);
}
const einsum = op({ einsum_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/elu.js
/**
* Computes exponential linear element-wise: `x > 0 ? x : (e ^ x) - 1`.
*
* ```js
* const x = tf.tensor1d([-1, 1, -3, 2]);
*
* x.elu().print();  // or tf.elu(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function elu_(x) {
	const inputs = { x: convertToTensor(x, "x", "elu", "float32") };
	return ENGINE.runKernel(Elu, inputs);
}
const elu = op({ elu_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/erf.js
/**
* Computes gause error function of the input `tf.Tensor` element-wise:
* `erf(x)`
*
* ```js
* const x = tf.tensor1d([0, .1, -.1, .7]);
*
* x.erf().print(); // or tf.erf(x);
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function erf_(x) {
	let $x = convertToTensor(x, "x", "erf");
	assert($x.dtype === "int32" || $x.dtype === "float32", () => "Input dtype must be `int32` or `float32`.");
	if ($x.dtype === "int32") $x = cast($x, "float32");
	const inputs = { x: $x };
	return ENGINE.runKernel(Erf, inputs);
}
const erf = op({ erf_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/exp.js
/**
* Computes exponential of the input `tf.Tensor` element-wise. `e ^ x`
*
* ```js
* const x = tf.tensor1d([1, 2, -3]);
*
* x.exp().print();  // or tf.exp(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function exp_(x) {
	const inputs = { x: convertToTensor(x, "x", "exp") };
	return ENGINE.runKernel(Exp, inputs);
}
const exp = op({ exp_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/expand_dims.js
/**
* Returns a `tf.Tensor` that has expanded rank, by inserting a dimension
* into the tensor's shape.
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
* const axis = 1;
* x.expandDims(axis).print();
* ```
*
* @param x The input tensor whose dimensions to be expanded.
* @param axis The dimension index at which to insert shape of `1`. Defaults
*     to 0 (the first dimension).
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function expandDims_(x, axis = 0) {
	const $x = convertToTensor(x, "x", "expandDims", "string_or_numeric");
	assert(axis <= $x.rank, () => "Axis must be <= rank of the tensor");
	const inputs = { input: $x };
	const attrs = { dim: axis };
	return ENGINE.runKernel(ExpandDims, inputs, attrs);
}
const expandDims = op({ expandDims_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/expm1.js
/**
* Computes exponential of the input `tf.Tensor` minus one element-wise.
* `e ^ x - 1`
*
* ```js
* const x = tf.tensor1d([1, 2, -3]);
*
* x.expm1().print();  // or tf.expm1(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function expm1_(x) {
	const inputs = { x: convertToTensor(x, "x", "expm1") };
	return ENGINE.runKernel(Expm1, inputs);
}
const expm1 = op({ expm1_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tile.js
/**
* Construct a tensor by repeating it the number of times given by reps.
*
* This operation creates a new tensor by replicating `input` `reps`
* times. The output tensor's i'th dimension has `input.shape[i] *
* reps[i]` elements, and the values of `input` are replicated
* `reps[i]` times along the i'th dimension. For example, tiling
* `[a, b, c, d]` by `[2]` produces `[a, b, c, d, a, b, c, d]`.
*
* ```js
* const a = tf.tensor1d([1, 2]);
*
* a.tile([2]).print();    // or a.tile([2])
* ```
*
* ```js
* const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* a.tile([1, 2]).print();  // or a.tile([1, 2])
* ```
* @param x The tensor to tile.
* @param reps Determines the number of replications per dimension.
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function tile_(x, reps) {
	const $x = convertToTensor(x, "x", "tile", "string_or_numeric");
	assert($x.rank === reps.length, () => `Error in transpose: rank of input ${$x.rank} must match length of reps ${reps}.`);
	const inputs = { x: $x };
	const attrs = { reps };
	return ENGINE.runKernel(Tile, inputs, attrs);
}
const tile = op({ tile_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/eye.js
/**
* Create an identity matrix.
*
* @param numRows Number of rows.
* @param numColumns Number of columns. Defaults to `numRows`.
* @param batchShape If provided, will add the batch shape to the beginning
*   of the shape of the returned `tf.Tensor` by repeating the identity
*   matrix.
* @param dtype Data type.
* @returns Identity matrix of the specified size and data type, possibly
*   with batch repetition if `batchShape` is specified.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function eye_(numRows, numColumns, batchShape, dtype = "float32") {
	if (numColumns == null) numColumns = numRows;
	const buff = buffer([numRows, numColumns], dtype);
	const n = numRows <= numColumns ? numRows : numColumns;
	for (let i = 0; i < n; ++i) buff.set(1, i, i);
	const out = reshape(buff.toTensor(), [numRows, numColumns]);
	if (batchShape == null) return out;
	else if (batchShape.length === 1) return tile(expandDims(out, 0), [
		batchShape[0],
		1,
		1
	]);
	else if (batchShape.length === 2) return tile(expandDims(expandDims(out, 0), 0), [
		batchShape[0],
		batchShape[1],
		1,
		1
	]);
	else if (batchShape.length === 3) return tile(expandDims(expandDims(expandDims(out, 0), 0), 0), [
		batchShape[0],
		batchShape[1],
		batchShape[2],
		1,
		1
	]);
	else throw new Error(`eye() currently supports only 1D and 2D batchShapes, but received ${batchShape.length}D.`);
}
const eye = op({ eye_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/fill.js
/**
* Creates a `tf.Tensor` filled with a scalar value.
*
* ```js
* tf.fill([2, 2], 4).print();
* ```
*
* @param shape An array of integers defining the output tensor shape.
* @param value The scalar value to fill the tensor with.
* @param dtype The type of an element in the resulting tensor. Defaults to
* 'float'.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function fill(shape, value, dtype) {
	const attrs = {
		shape,
		value,
		dtype
	};
	return ENGINE.runKernel(Fill, {}, attrs);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/floor.js
/**
* Computes floor of input `tf.Tensor` element-wise: `floor(x)`.
*
* ```js
* const x = tf.tensor1d([.6, 1.1, -3.3]);
*
* x.floor().print();  // or tf.floor(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function floor_(x) {
	const inputs = { x: convertToTensor(x, "x", "floor", "float32") };
	return ENGINE.runKernel(Floor, inputs);
}
const floor = op({ floor_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/gather.js
/**
* Gather slices from tensor `x`'s axis `axis` according to `indices`.
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
* const indices = tf.tensor1d([1, 3, 3], 'int32');
*
* x.gather(indices).print();
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
* const indices = tf.tensor1d([1, 1, 0], 'int32');
*
* x.gather(indices).print();
* ```
* @param x The input tensor whose slices to be gathered.
* @param indices The indices of the values to extract.
* @param axis The axis over which to select values. Defaults to 0.
* @param batchDims Optional. The number of batch dimensions. It must be less
*     than or equal to rank(indices). Defaults to 0.
*     The output tensor will have shape of
*     `x.shape[:axis] + indices.shape[batchDims:] + x.shape[axis + 1:]`
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function gather_(x, indices, axis = 0, batchDims = 0) {
	const $x = convertToTensor(x, "x", "gather");
	const $indices = convertToTensor(indices, "indices", "gather", "int32");
	const inputs = {
		x: $x,
		indices: $indices
	};
	const attrs = {
		axis,
		batchDims
	};
	return ENGINE.runKernel(GatherV2, inputs, attrs);
}
const gather = op({ gather_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/greater.js
/**
* Returns the truth value of (a > b) element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
* const b = tf.tensor1d([2, 2, 2]);
*
* a.greater(b).print();
* ```
*
* @param a The first input tensor.
* @param b The second input tensor. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function greater_(a, b) {
	let $a = convertToTensor(a, "a", "greater", "string_or_numeric");
	let $b = convertToTensor(b, "b", "greater", "string_or_numeric");
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Greater, inputs);
}
const greater = op({ greater_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/greater_equal.js
/**
* Returns the truth value of (a >= b) element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
* const b = tf.tensor1d([2, 2, 2]);
*
* a.greaterEqual(b).print();
* ```
*
* @param a The first input tensor.
* @param b The second input tensor. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function greaterEqual_(a, b) {
	let $a = convertToTensor(a, "a", "greaterEqual", "string_or_numeric");
	let $b = convertToTensor(b, "b", "greaterEqual", "string_or_numeric");
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(GreaterEqual, inputs);
}
const greaterEqual = op({ greaterEqual_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/imag.js
/**
* Returns the imaginary part of a complex (or real) tensor.
*
* Given a tensor input, this operation returns a tensor of type float that is
* the imaginary part of each element in input considered as a complex number.
* If input is real, a tensor of all zeros is returned.
*
* ```js
* const x = tf.complex([-2.25, 3.25], [4.75, 5.75]);
* tf.imag(x).print();
* ```
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function imag_(input) {
	const inputs = { input: convertToTensor(input, "input", "imag") };
	return ENGINE.runKernel(Imag, inputs);
}
const imag = op({ imag_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/is_finite.js
/**
* Returns which elements of x are finite.
*
* ```js
* const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);
*
* x.isFinite().print();  // or tf.isNaN(x)
* ```
* @param x The input Tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function isFinite_(x) {
	const inputs = { x: convertToTensor(x, "x", "isFinite") };
	return ENGINE.runKernel(IsFinite, inputs);
}
const isFinite$1 = op({ isFinite_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/is_inf.js
/**
* Returns which elements of x are Infinity or -Infinity.
*
* ```js
* const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);
*
* x.isInf().print();  // or tf.isNaN(x)
* ```
* @param x The input Tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function isInf_(x) {
	const inputs = { x: convertToTensor(x, "x", "isInf") };
	return ENGINE.runKernel(IsInf, inputs);
}
const isInf = op({ isInf_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/is_nan.js
/**
* RReturns which elements of x are NaN.
*
* ```js
* const x = tf.tensor1d([NaN, Infinity, -Infinity, 0, 1]);
*
* x.isNaN().print();  // or tf.isNaN(x)
* ```
* @param x The input Tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function isNaN_(x) {
	const inputs = { x: convertToTensor(x, "x", "isNaN") };
	return ENGINE.runKernel(IsNan, inputs);
}
const isNaN$1 = op({ isNaN_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/leaky_relu.js
/**
* Computes leaky rectified linear element-wise.
*
* See
* [http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf](
*     http://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)
*
* ```js
* const x = tf.tensor1d([-1, 2, -3, 4]);
*
* x.leakyRelu(0.1).print();  // or tf.leakyRelu(x, 0.1)
* ```
* @param x The input tensor.
* @param alpha The scaling factor for negative values, defaults to 0.2.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function leakyRelu_(x, alpha = .2) {
	const inputs = { x: convertToTensor(x, "x", "leakyRelu") };
	const attrs = { alpha };
	return ENGINE.runKernel(LeakyRelu, inputs, attrs);
}
const leakyRelu = op({ leakyRelu_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/less.js
/**
* Returns the truth value of (a < b) element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
* const b = tf.tensor1d([2, 2, 2]);
*
* a.less(b).print();
* ```
* @param a The first input tensor.
* @param b The second input tensor. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function less_(a, b) {
	let $a = convertToTensor(a, "a", "less", "string_or_numeric");
	let $b = convertToTensor(b, "b", "less", "string_or_numeric");
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Less, inputs);
}
const less = op({ less_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/less_equal.js
/**
* Returns the truth value of (a <= b) element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
* const b = tf.tensor1d([2, 2, 2]);
*
* a.lessEqual(b).print();
* ```
*
* @param a The first input tensor.
* @param b The second input tensor. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function lessEqual_(a, b) {
	let $a = convertToTensor(a, "a", "lessEqual", "string_or_numeric");
	let $b = convertToTensor(b, "b", "lessEqual", "string_or_numeric");
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(LessEqual, inputs);
}
const lessEqual = op({ lessEqual_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/linspace.js
/**
* Return an evenly spaced sequence of numbers over the given interval.
*
* ```js
* tf.linspace(0, 9, 10).print();
* ```
* @param start The start value of the sequence.
* @param stop The end value of the sequence.
* @param num The number of values to generate.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function linspace(start, stop, num) {
	if (num <= 0) throw new Error("The number of values should be positive.");
	const attrs = {
		start,
		stop,
		num
	};
	return ENGINE.runKernel(LinSpace, {}, attrs);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/local_response_normalization.js
/**
* Normalizes the activation of a local neighborhood across or within
* channels.
*
* @param x The input tensor. The 4-D input tensor is treated as a 3-D array
*     of 1D vectors (along the last dimension), and each vector is
*     normalized independently.
* @param depthRadius The number of adjacent channels in the 1D normalization
*     window.
* @param bias A constant bias term for the basis.
* @param alpha A scale factor, usually positive.
* @param beta An exponent.
*
* @doc {heading: 'Operations', subheading: 'Normalization'}
*/
function localResponseNormalization_(x, depthRadius = 5, bias = 1, alpha = 1, beta = .5) {
	const $x = convertToTensor(x, "x", "localResponseNormalization");
	assert($x.rank === 4 || $x.rank === 3, () => `Error in localResponseNormalization: x must be rank 3 or 4 but got
               rank ${$x.rank}.`);
	assert(isInt(depthRadius), () => `Error in localResponseNormalization: depthRadius must be an integer but got depthRadius ${depthRadius}.`);
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	const inputs = { x: x4D };
	const attrs = {
		depthRadius,
		bias,
		alpha,
		beta
	};
	const res = ENGINE.runKernel(LRN, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	else return res;
}
const localResponseNormalization = op({ localResponseNormalization_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/log.js
/**
* Computes natural logarithm of the input `tf.Tensor` element-wise: `ln(x)`
*
* ```js
* const x = tf.tensor1d([1, 2, Math.E]);
*
* x.log().print();  // or tf.log(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function log_(x) {
	const inputs = { x: convertToTensor(x, "x", "log", "float32") };
	return ENGINE.runKernel(Log, inputs);
}
const log = op({ log_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/log1p.js
/**
* Computes natural logarithm of the input `tf.Tensor` plus one
* element-wise: `ln(1 + x)`
*
* ```js
* const x = tf.tensor1d([1, 2, Math.E - 1]);
*
* x.log1p().print();  // or tf.log1p(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function log1p_(x) {
	const inputs = { x: convertToTensor(x, "x", "log1p") };
	return ENGINE.runKernel(Log1p, inputs);
}
const log1p = op({ log1p_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/gradients.js
/**
* Provided `f(x)`, returns another function `g(x, dy?)`, which gives the
* gradient of `f(x)` with respect to `x`.
*
* If `dy` is provided, the gradient of `f(x).mul(dy).sum()` with respect to
* `x` is computed instead. `f(x)` must take a single tensor `x` and return a
* single tensor `y`. If `f()` takes multiple inputs, use `tf.grads` instead.
*
* ```js
* // f(x) = x ^ 2
* const f = x => x.square();
* // f'(x) = 2x
* const g = tf.grad(f);
*
* const x = tf.tensor1d([2, 3]);
* g(x).print();
* ```
*
* ```js
* // f(x) = x ^ 3
* const f = x => x.pow(tf.scalar(3, 'int32'));
* // f'(x) = 3x ^ 2
* const g = tf.grad(f);
* // f''(x) = 6x
* const gg = tf.grad(g);
*
* const x = tf.tensor1d([2, 3]);
* gg(x).print();
* ```
*
* @param f The function f(x), to compute gradient for.
*
* @doc {heading: 'Training', subheading: 'Gradients'}
*/
function grad(f) {
	assert(isFunction(f), () => "The f passed in grad(f) must be a function");
	return (x, dy) => {
		const $x = convertToTensor(x, "x", "tf.grad", "string_or_numeric");
		const $dy = dy != null ? convertToTensor(dy, "dy", "tf.grad") : null;
		return ENGINE.tidy(() => {
			const { value, grads: grads$1 } = ENGINE.gradients(() => f($x), [$x], $dy);
			if ($dy != null) assertShapesMatch(value.shape, $dy.shape, "The shape of dy passed in grad(f)(x, dy) must match the shape returned by f(x)");
			checkGrads(grads$1);
			return grads$1[0];
		});
	};
}
/**
* Provided `f(x1, x2,...)`, returns another function `g([x1, x2,...], dy?)`,
* which gives an array of gradients of `f()` with respect to each input
* [`x1`,`x2`,...].
*
* If `dy` is passed when calling `g()`, the gradient of
* `f(x1,...).mul(dy).sum()` with respect to each input is computed instead.
* The provided `f` must take one or more tensors and return a single tensor
* `y`. If `f()` takes a single input, we recommend using `tf.grad` instead.
*
* ```js
* // f(a, b) = a * b
* const f = (a, b) => a.mul(b);
* // df / da = b, df / db = a
* const g = tf.grads(f);
*
* const a = tf.tensor1d([2, 3]);
* const b = tf.tensor1d([-2, -3]);
* const [da, db] = g([a, b]);
* console.log('da');
* da.print();
* console.log('db');
* db.print();
* ```
*
* @param f The function `f(x1, x2,...)` to compute gradients for.
*
* @doc {heading: 'Training', subheading: 'Gradients'}
*/
function grads(f) {
	assert(isFunction(f), () => "The f passed in grads(f) must be a function");
	return (args, dy) => {
		assert(Array.isArray(args), () => "The args passed in grads(f)(args) must be an array of `Tensor`s or `TensorLike`s");
		const $args = convertToTensorArray(args, "args", "tf.grads", "string_or_numeric");
		const $dy = dy != null ? convertToTensor(dy, "dy", "tf.grads") : null;
		return ENGINE.tidy(() => {
			const { value, grads: grads$1 } = ENGINE.gradients(() => f(...$args), $args, $dy);
			if ($dy != null) assertShapesMatch(value.shape, $dy.shape, "The shape of dy passed in grads(f)([x1,...], dy) must match the shape returned by f([x1,...])");
			checkGrads(grads$1);
			return grads$1;
		});
	};
}
/**
* Like `tf.grad`, but also returns the value of `f()`. Useful when `f()`
* returns a metric you want to show.
*
* The result is a rich object with the following properties:
* - grad: The gradient of `f(x)` w.r.t `x` (result of `tf.grad`).
* - value: The value returned by `f(x)`.
*
* ```js
* // f(x) = x ^ 2
* const f = x => x.square();
* // f'(x) = 2x
* const g = tf.valueAndGrad(f);
*
* const x = tf.tensor1d([2, 3]);
* const {value, grad} = g(x);
*
* console.log('value');
* value.print();
* console.log('grad');
* grad.print();
* ```
*
* @doc {heading: 'Training', subheading: 'Gradients'}
*/
function valueAndGrad(f) {
	assert(isFunction(f), () => "The f passed in valueAndGrad(f) must be a function");
	return (x, dy) => {
		assert(x instanceof Tensor, () => "The x passed in valueAndGrad(f)(x) must be a tensor");
		assert(dy == null || dy instanceof Tensor, () => "The dy passed in valueAndGrad(f)(x, dy) must be a tensor");
		const { grads: grads$1, value } = ENGINE.gradients(() => f(x), [x], dy);
		checkGrads(grads$1);
		return {
			grad: grads$1[0],
			value
		};
	};
}
/**
* Like `tf.grads`, but returns also the value of `f()`. Useful when `f()`
* returns a metric you want to show.
*
* The result is a rich object with the following properties:
* - grads: The gradients of `f()` w.r.t each input (result of `tf.grads`).
* - value: The value returned by `f(x)`.
*
* ```js
* // f(a, b) = a * b
* const f = (a, b) => a.mul(b);
* // df/da = b, df/db = a
* const g = tf.valueAndGrads(f);
*
* const a = tf.tensor1d([2, 3]);
* const b = tf.tensor1d([-2, -3]);
* const {value, grads} = g([a, b]);
*
* const [da, db] = grads;
*
* console.log('value');
* value.print();
*
* console.log('da');
* da.print();
* console.log('db');
* db.print();
* ```
*
* @doc {heading: 'Training', subheading: 'Gradients'}
*/
function valueAndGrads(f) {
	assert(isFunction(f), () => "The f passed in valueAndGrads(f) must be a function");
	return (args, dy) => {
		assert(Array.isArray(args) && args.every((arg) => arg instanceof Tensor), () => "The args passed in valueAndGrads(f)(args) must be array of tensors");
		assert(dy == null || dy instanceof Tensor, () => "The dy passed in valueAndGrads(f)(args, dy) must be a tensor");
		const res = ENGINE.gradients(() => f(...args), args, dy);
		if (dy != null) assertShapesMatch(res.value.shape, dy.shape, "The shape of dy passed in valueAndGrads(f)([x1,...], dy) must match the shape returned by f([x1,...])");
		checkGrads(res.grads);
		return res;
	};
}
/**
* Computes and returns the gradient of f(x) with respect to the list of
* trainable variables provided by `varList`. If no list is provided, it
* defaults to all trainable variables.
*
* ```js
* const a = tf.variable(tf.tensor1d([3, 4]));
* const b = tf.variable(tf.tensor1d([5, 6]));
* const x = tf.tensor1d([1, 2]);
*
* // f(a, b) = a * x ^ 2 + b * x
* const f = () => a.mul(x.square()).add(b.mul(x)).sum();
* // df/da = x ^ 2, df/db = x
* const {value, grads} = tf.variableGrads(f);
*
* Object.keys(grads).forEach(varName => grads[varName].print());
* ```
*
* @param f The function to execute. f() should return a scalar.
* @param varList The list of variables to compute the gradients with respect
*     to. Defaults to all trainable variables.
* @returns An object with the following keys and values:
*   - `value`: The value of the function `f`.
*   - `grads`: A map from the names of the variables to the gradients.
*     If the `varList` argument is provided explicitly and contains a subset of
*     non-trainable variables, this map in the return value will contain keys
*     that map the names of the non-trainable variables to `null`.
*
* @doc {heading: 'Training', subheading: 'Gradients'}
*/
function variableGrads(f, varList) {
	assert(isFunction(f), () => "The f passed in variableGrads(f) must be a function");
	assert(varList == null || Array.isArray(varList) && varList.every((v) => v instanceof Variable), () => "The varList passed in variableGrads(f, varList) must be an array of variables");
	const specifiedVarList = varList != null;
	if (!specifiedVarList) {
		varList = [];
		for (const varName in ENGINE.registeredVariables) varList.push(ENGINE.registeredVariables[varName]);
	}
	const specifiedNonTrainable = specifiedVarList ? varList.filter((variable$1) => !variable$1.trainable) : null;
	const originalVarCount = varList.length;
	varList = varList.filter((variable$1) => variable$1.trainable);
	assert(varList.length > 0, () => `variableGrads() expects at least one of the input variables to be trainable, but none of the ${originalVarCount} variables is trainable.`);
	const { value, grads: grads$1 } = ENGINE.gradients(f, varList, null, true);
	assert(grads$1.some((g) => g != null), () => "Cannot find a connection between any variable and the result of the loss function y=f(x). Please make sure the operations that use variables are inside the function f passed to minimize().");
	assert(value.rank === 0, () => `The f passed in variableGrads(f) must return a scalar, but it returned a rank-${value.rank} tensor`);
	const namedGrads = {};
	varList.forEach((v, i) => {
		if (grads$1[i] != null) namedGrads[v.name] = grads$1[i];
	});
	if (specifiedNonTrainable != null) specifiedNonTrainable.forEach((v) => namedGrads[v.name] = null);
	return {
		value,
		grads: namedGrads
	};
}
/**
* Overrides the gradient computation of a function `f`.
*
* Takes a function
* `f(...inputs, save) => {value: Tensor, gradFunc: (dy, saved) => Tensor[]}`
* and returns another function `g(...inputs)` which takes the same inputs as
* `f`. When called, `g` returns `f().value`. In backward mode, custom gradients
* with respect to each input of `f` are computed using `f().gradFunc`.
*
* The `save` function passsed to `f` should be used for saving tensors needed
* in the gradient. And the `saved` passed to the `gradFunc` is a
* `NamedTensorMap`, which contains those saved tensor.
*
* ```js
* const customOp = tf.customGrad((x, save) => {
*   // Save x to make sure it's available later for the gradient.
*   save([x]);
*   // Override gradient of our custom x ^ 2 op to be dy * abs(x);
*   return {
*     value: x.square(),
*     // Note `saved.x` which points to the `x` we saved earlier.
*     gradFunc: (dy, saved) => [dy.mul(saved[0].abs())]
*   };
* });
*
* const x = tf.tensor1d([-1, -2, 3]);
* const dx = tf.grad(x => customOp(x));
*
* console.log(`f(x):`);
* customOp(x).print();
* console.log(`f'(x):`);
* dx(x).print();
* ```
*
* @param f The function to evaluate in forward mode, which should return
*     `{value: Tensor, gradFunc: (dy, saved) => Tensor[]}`, where `gradFunc`
*     returns the custom gradients of `f` with respect to its inputs.
*
* @doc {heading: 'Training', subheading: 'Gradients'}
*/
function customGrad(f) {
	return ENGINE.customGrad(f);
}
function checkGrads(grads$1) {
	if (grads$1.filter((g) => g == null).length > 0) throw new Error(`Cannot compute gradient of y=f(x) with respect to x. Make sure that
    the f you passed encloses all operations that lead from x to y.`);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/neg.js
/**
* Computes `-1 * x` element-wise.
*
* ```js
* const x = tf.tensor2d([1, 2, -2, 0], [2, 2]);
*
* x.neg().print();  // or tf.neg(x)
* ```
*
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function neg_(x) {
	const inputs = { x: convertToTensor(x, "x", "neg") };
	return ENGINE.runKernel(Neg, inputs);
}
const neg = op({ neg_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/softplus.js
/**
* Computes softplus of the input `tf.Tensor` element-wise: `log(exp(x) + 1)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.softplus().print();  // or tf.softplus(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function softplus_(x) {
	const inputs = { x: convertToTensor(x, "x", "softplus") };
	return ENGINE.runKernel(Softplus, inputs);
}
const softplus = op({ softplus_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/log_sigmoid.js
/**
* Computes log sigmoid of the input `tf.Tensor` element-wise:
* `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.logSigmoid().print();  // or tf.logSigmoid(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function logSigmoid_(x) {
	const $x = convertToTensor(x, "x", "logSigmoid");
	return customGrad((x$1) => {
		const value = neg(softplus(neg(x$1)));
		const gradFunc = (dy) => {
			return mul(dy, sigmoid(neg(x$1)));
		};
		return {
			value,
			gradFunc
		};
	})($x);
}
const logSigmoid = op({ logSigmoid_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/max.js
/**
* Computes the maximum of elements across dimensions of a `tf.Tensor`.
*
* Reduces the input along the dimensions given in `axes`. Unless `keepDims`
* is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
* `axes`. If `keepDims` is true, the reduced dimensions are retained with
* length 1. If `axes` has no entries, all dimensions are reduced, and an
* `tf.Tensor` with a single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.max().print();  // or tf.max(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* const axis = 1;
* x.max(axis).print();  // or tf.max(x, axis)
* ```
*
* @param x The input tensor.
* @param axis The dimension(s) to reduce. By default it reduces
*     all dimensions.
* @param keepDims If true, retains reduced dimensions with size 1.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function max_(x, axis = null, keepDims = false) {
	const inputs = { x: convertToTensor(x, "x", "max") };
	const attrs = {
		reductionIndices: axis,
		keepDims
	};
	return ENGINE.runKernel(Max, inputs, attrs);
}
const max = op({ max_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sub.js
/**
* Subtracts two `tf.Tensor`s element-wise, A - B. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([10, 20, 30, 40]);
* const b = tf.tensor1d([1, 2, 3, 4]);
*
* a.sub(b).print();  // or tf.sub(a, b)
* ```
*
* ```js
* // Broadcast subtract a with b.
* const a = tf.tensor1d([10, 20, 30, 40]);
* const b = tf.scalar(5);
*
* a.sub(b).print();  // or tf.sub(a, b)
* ```
* @param a The first `tf.Tensor` to subtract from.
* @param b The second `tf.Tensor` to be subtracted. Must have the same dtype as
* `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function sub_(a, b) {
	let $a = convertToTensor(a, "a", "sub");
	let $b = convertToTensor(b, "b", "sub");
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Sub, inputs);
}
const sub = op({ sub_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sum.js
/**
* Computes the sum of elements across dimensions of a `tf.Tensor`.
*
* Reduces the input along the dimensions given in `axes`. Unless `keepDims`
* is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
* `axes`. If `keepDims` is true, the reduced dimensions are retained with
* length 1. If axes has no entries, all dimensions are reduced, and a
* `tf.Tensor` with a single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.sum().print();  // or tf.sum(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* const axis = 1;
* x.sum(axis).print();  // or tf.sum(x, axis)
* ```
*
* @param x The input tensor to compute the sum over. If the dtype is `bool`
*   it will be converted to `int32` and the output dtype will be `int32`.
* @param axis The dimension(s) to reduce. By default it reduces
*     all dimensions.
* @param keepDims If true, retains reduced dimensions with size 1.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function sum_(x, axis = null, keepDims = false) {
	let $x = convertToTensor(x, "x", "sum");
	if ($x.dtype === "bool") $x = cast($x, "int32");
	const inputs = { x: $x };
	const attrs = {
		axis,
		keepDims
	};
	return ENGINE.runKernel(Sum, inputs, attrs);
}
const sum = op({ sum_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/log_softmax.js
/**
* Computes the log softmax.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
*
* a.logSoftmax().print();  // or tf.logSoftmax(a)
* ```
*
* ```js
* const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);
*
* a.logSoftmax().print();  // or tf.logSoftmax(a)
* ```
*
* @param logits The logits array.
* @param axis The dimension softmax would be performed on. Defaults to `-1`
*     which indicates the last dimension.
*
* @doc {heading: 'Operations', subheading: 'Normalization'}
*/
function logSoftmax_(logits, axis = -1) {
	const $logits = convertToTensor(logits, "logits", "logSoftmax");
	if (axis === -1) axis = $logits.rank - 1;
	if (axis !== $logits.rank - 1) throw Error(`Log Softmax along a non-last dimension is not yet supported. Logits was rank ${$logits.rank} and axis was ${axis}`);
	return customGrad((logits$1, save) => {
		const keepDims = true;
		const xMax = max(logits$1, axis, true);
		const shifted = sub(logits$1, xMax);
		const value = sub(cast(shifted, "float32"), log(sum(exp(shifted), axis, keepDims)));
		save([value]);
		const gradFunc = (dy, saved) => {
			const [value$1] = saved;
			const keepDims$1 = true;
			const softmax$1 = exp(value$1);
			return sub(dy, mul(sum(dy, axis, keepDims$1), softmax$1));
		};
		return {
			value,
			gradFunc
		};
	})($logits);
}
const logSoftmax = op({ logSoftmax_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/axis_util.js
/**
* Returns true if the axis specifies the inner most dimensions of the
* array.
*/
function axesAreInnerMostDims(axes, rank) {
	for (let i = 0; i < axes.length; ++i) if (axes[axes.length - i - 1] !== rank - 1 - i) return false;
	return true;
}
function combineLocations(outputLoc, reduceLoc, axes) {
	const rank = outputLoc.length + reduceLoc.length;
	const loc = [];
	let outIdx = 0;
	let reduceIdx = 0;
	for (let dim = 0; dim < rank; dim++) if (axes.indexOf(dim) === -1) loc.push(outputLoc[outIdx++]);
	else loc.push(reduceLoc[reduceIdx++]);
	return loc;
}
function computeOutAndReduceShapes(aShape, axes) {
	const outShape = [];
	const rank = aShape.length;
	for (let dim = 0; dim < rank; dim++) if (axes.indexOf(dim) === -1) outShape.push(aShape[dim]);
	const reduceShape = axes.map((dim) => aShape[dim]);
	return [outShape, reduceShape];
}
function expandShapeToKeepDim(shape, axes) {
	const reduceSubShape = axes.map((x) => 1);
	return combineLocations(shape, reduceSubShape, axes);
}
function assertAxesAreInnerMostDims(msg, axes, rank) {
	assert(axesAreInnerMostDims(axes, rank), () => `${msg} supports only inner-most axes for now. Got axes ${axes} and rank-${rank} input.`);
}
/**
* Returns the axes permutation to be used with `tf.transpose`, if such
* permutation is necessary. Otherwise it returns null. This method is used by
* operations that operate only on inner-most axes.
*/
function getAxesPermutation(axes, rank) {
	if (axesAreInnerMostDims(axes, rank)) return null;
	const result = [];
	for (let i = 0; i < rank; ++i) if (axes.indexOf(i) === -1) result.push(i);
	axes.forEach((axis) => result.push(axis));
	return result;
}
/** Returns the axes permutation that undoes the original permutation. */
function getUndoAxesPermutation(axes) {
	return axes.map((axis, i) => [i, axis]).sort((a, b) => a[1] - b[1]).map((x) => x[0]);
}
function getInnerMostAxes(numAxes, rank) {
	const res = [];
	for (let i = rank - numAxes; i < rank; ++i) res.push(i);
	return res;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/log_sum_exp.js
/**
* Computes the log(sum(exp(elements across the reduction dimensions)).
*
* Reduces the input along the dimensions given in `axis`. Unless `keepDims`
* is true, the rank of the array is reduced by 1 for each entry in `axis`.
* If `keepDims` is true, the reduced dimensions are retained with length 1.
* If `axis` has no entries, all dimensions are reduced, and an array with a
* single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.logSumExp().print();  // or tf.logSumExp(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* const axis = 1;
* x.logSumExp(axis).print();  // or tf.logSumExp(a, axis)
* ```
* @param x The input tensor.
* @param axis The dimension(s) to reduce. If null (the default),
*     reduces all dimensions.
* @param keepDims If true, retains reduced dimensions with length
*     of 1. Defaults to false.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function logSumExp_(x, axis = null, keepDims = false) {
	const $x = convertToTensor(x, "x", "logSumExp");
	const axes = parseAxisParam(axis, $x.shape);
	const xMax = max($x, axes, true);
	const a = sub($x, xMax);
	const b = exp(a);
	const c = sum(b, axes);
	const d = log(c);
	const res = add(reshape(xMax, d.shape), d);
	if (keepDims) {
		const newShape = expandShapeToKeepDim(res.shape, axes);
		return reshape(res, newShape);
	}
	return res;
}
const logSumExp = op({ logSumExp_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/logical_and.js
/**
* Returns the truth value of `a AND b` element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([false, false, true, true], 'bool');
* const b = tf.tensor1d([false, true, false, true], 'bool');
*
* a.logicalAnd(b).print();
* ```
*
* @param a The first input tensor. Must be of dtype bool.
* @param b The second input tensor. Must be of dtype bool.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function logicalAnd_(a, b) {
	const $a = convertToTensor(a, "a", "logicalAnd", "bool");
	const $b = convertToTensor(b, "b", "logicalAnd", "bool");
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(LogicalAnd, inputs);
}
const logicalAnd = op({ logicalAnd_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/logical_not.js
/**
* Returns the truth value of `NOT x` element-wise.
*
* ```js
* const a = tf.tensor1d([false, true], 'bool');
*
* a.logicalNot().print();
* ```
*
* @param x The input tensor. Must be of dtype 'bool'.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function logicalNot_(x) {
	const inputs = { x: convertToTensor(x, "x", "logicalNot", "bool") };
	return ENGINE.runKernel(LogicalNot, inputs);
}
const logicalNot = op({ logicalNot_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/logical_or.js
/**
* Returns the truth value of `a OR b` element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([false, false, true, true], 'bool');
* const b = tf.tensor1d([false, true, false, true], 'bool');
*
* a.logicalOr(b).print();
* ```
* @param a The first input tensor. Must be of dtype bool.
* @param b The second input tensor. Must be of dtype bool.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function logicalOr_(a, b) {
	const $a = convertToTensor(a, "a", "logicalOr", "bool");
	const $b = convertToTensor(b, "b", "logicalOr", "bool");
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(LogicalOr, inputs);
}
const logicalOr = op({ logicalOr_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/logical_xor.js
/**
* Returns the truth value of `a XOR b` element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([false, false, true, true], 'bool');
* const b = tf.tensor1d([false, true, false, true], 'bool');
*
* a.logicalXor(b).print();
* ```
*
* @param a The first input tensor. Must be of dtype bool.
* @param b The second input tensor. Must be of dtype bool.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function logicalXor_(a, b) {
	const $a = convertToTensor(a, "a", "logicalXor", "bool");
	const $b = convertToTensor(b, "b", "logicalXor", "bool");
	assertAndGetBroadcastShape($a.shape, $b.shape);
	return logicalAnd(logicalOr(a, b), logicalNot(logicalAnd(a, b)));
}
const logicalXor = op({ logicalXor_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/max_pool.js
/**
* Computes the 2D max pooling of an image.
*
* @param x The input tensor, of rank 4 or rank 3 of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
* @param filterSize The filter size: `[filterHeight, filterWidth]`. If
*     `filterSize` is a single number, then `filterHeight == filterWidth`.
* @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
*     `strides` is a single number, then `strideHeight == strideWidth`.
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     in dilated pooling. Defaults to `[1, 1]`. If `dilations` is a single
*     number, then `dilationHeight == dilationWidth`. If it is greater than
*     1, then all values of `strides` must be 1.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*/
function maxPool_(x, filterSize, strides, pad$1, dimRoundingMode) {
	const $x = convertToTensor(x, "x", "maxPool");
	const dilations = 1;
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	assert(x4D.rank === 4, () => `Error in maxPool: input must be rank 4 but got rank ${x4D.rank}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => `Error in maxPool: Either strides or dilations must be 1. Got strides ${strides} and dilations '${dilations}'`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in maxPool: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inputs = { x: x4D };
	const attrs = {
		filterSize,
		strides,
		pad: pad$1,
		dimRoundingMode
	};
	const res = ENGINE.runKernel(MaxPool, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const maxPool = op({ maxPool_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_3d.js
/**
* Computes the 3D max pooling.
*
* ```js
* const x = tf.tensor5d([1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 2, 2, 1]);
* const result = tf.maxPool3d(x, 2, 1, 'valid');
* result.print();
* ```
*
* @param x The input tensor, of rank 5 or rank 4 of shape
*     `[batch, depth, height, width, inChannels]`.
* @param filterSize The filter size:
*     `[filterDepth, filterHeight, filterWidth]`.
*     If `filterSize` is a single number,
*     then `filterDepth == filterHeight == filterWidth`.
* @param strides The strides of the pooling:
*     `[strideDepth, strideHeight, strideWidth]`.
*     If `strides` is a single number,
*     then `strideDepth == strideHeight == strideWidth`.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1*1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
* @param dataFormat An optional string from: "NDHWC", "NCDHW". Defaults to
*     "NDHWC". Specify the data format of the input and output data. With the
*     default format "NDHWC", the data is stored in the order of: [batch,
*     depth, height, width, channels]. Only "NDHWC" is currently supported.
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function maxPool3d_(x, filterSize = [
	1,
	1,
	1
], strides, pad$1, dimRoundingMode, dataFormat = "NDHWC") {
	const $x = convertToTensor(x, "x", "maxPool3d");
	let x5D = $x;
	let reshapedTo5D = false;
	if ($x.rank === 4) {
		reshapedTo5D = true;
		x5D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2],
			$x.shape[3]
		]);
	}
	assert(x5D.rank === 5, () => `Error in maxPool3d: x must be rank 5 but got rank ${x5D.rank}.`);
	assert(dataFormat === "NDHWC", () => `Error in maxPool3d: Only NDHWC is currently supported, but got dataFormat of ${dataFormat}`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in maxPool3d: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inputs = { x: x5D };
	const attrs = {
		filterSize,
		strides,
		pad: pad$1,
		dimRoundingMode,
		dataFormat
	};
	const res = ENGINE.runKernel(MaxPool3D, inputs, attrs);
	if (reshapedTo5D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3],
		res.shape[4]
	]);
	return res;
}
const maxPool3d = op({ maxPool3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/max_pool_with_argmax.js
/**
* Computes the 2D max pooling of an image with Argmax index.
* The indices in argmax are flattened, so that a maximum value at position `[b,
* y, x, c]` becomes flattened index: `(y * width + x) * channels + c` if
* include_batch_in_index is False; `((b * height + y) * width + x) * channels
* +c` if include_batch_in_index is True.
*
* The indices returned are always in `[0, height) x [0, width)` before
* flattening.
*
* @param x The input tensor, of rank 4 or rank 3 of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
* @param filterSize The filter size: `[filterHeight, filterWidth]`. If
*     `filterSize` is a single number, then `filterHeight == filterWidth`.
* @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
*     `strides` is a single number, then `strideHeight == strideWidth`.
* @param dataFormat An optional string from: "NDHWC", "NCDHW". Defaults to
*     "NDHWC". Specify the data format of the input and output data. With the
*     default format "NDHWC", the data is stored in the order of: [batch,
*     depth, height, width, channels]. Only "NDHWC" is currently supported.
* @param pad The type of padding algorithm.
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param includeBatchIndex Defaults to False. Whether to include batch
*    dimension in flattened index of argmax.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function maxPoolWithArgmax_(x, filterSize, strides, pad$1, includeBatchInIndex = false) {
	const inputs = { x: convertToTensor(x, "x", "maxPoolWithArgmax") };
	const attrs = {
		filterSize,
		strides,
		pad: pad$1,
		includeBatchInIndex
	};
	const result = ENGINE.runKernel(MaxPoolWithArgmax, inputs, attrs);
	return {
		result: result[0],
		indexes: result[1]
	};
}
const maxPoolWithArgmax = op({ maxPoolWithArgmax_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/maximum.js
/**
* Returns the max of a and b (`a > b ? a : b`) element-wise.
* Supports broadcasting.
*
* We also expose `tf.maximumStrict` which has the same signature as this op and
* asserts that `a` and `b` are the same shape (does not broadcast).
*
* ```js
* const a = tf.tensor1d([1, 4, 3, 16]);
* const b = tf.tensor1d([1, 2, 9, 4]);
*
* a.maximum(b).print();  // or tf.maximum(a, b)
* ```
*
* ```js
* // Broadcast maximum a with b.
* const a = tf.tensor1d([2, 4, 6, 8]);
* const b = tf.scalar(5);
*
* a.maximum(b).print();  // or tf.maximum(a, b)
* ```
*
* @param a The first tensor.
* @param b The second tensor. Must have the same type as `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function maximum_(a, b) {
	let $a = convertToTensor(a, "a", "maximum");
	let $b = convertToTensor(b, "b", "maximum");
	[$a, $b] = makeTypesMatch($a, $b);
	if ($a.dtype === "bool") {
		$a = cast($a, "int32");
		$b = cast($b, "int32");
	}
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Maximum, inputs);
}
const maximum = op({ maximum_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/mean.js
/**
* Computes the mean of elements across dimensions of a `tf.Tensor`.
*
* Reduces `x` along the dimensions given in `axis`. Unless `keepDims` is
* true, the rank of the `tf.Tensor` is reduced by 1 for each entry in `axis`.
* If `keepDims` is true, the reduced dimensions are retained with length 1.
* If `axis` has no entries, all dimensions are reduced, and a `tf.Tensor` with
* a single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.mean().print();  // or tf.mean(a)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* const axis = 1;
* x.mean(axis).print();  // or tf.mean(x, axis)
* ```
*
* @param x The input tensor.
* @param axis The dimension(s) to reduce. By default it reduces
*     all dimensions.
* @param keepDims If true, retains reduced dimensions with size 1.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function mean_(x, axis = null, keepDims = false) {
	const inputs = { x: convertToTensor(x, "x", "mean") };
	const attrs = {
		axis,
		keepDims
	};
	return ENGINE.runKernel(Mean, inputs, attrs);
}
const mean = op({ mean_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/zeros.js
/**
* Creates a `tf.Tensor` with all elements set to 0.
*
* ```js
* tf.zeros([2, 2]).print();
* ```
*
* @param shape An array of integers defining the output tensor shape.
* @param dtype The type of an element in the resulting tensor. Can
*     be 'float32', 'int32' or 'bool'. Defaults to 'float'.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function zeros(shape, dtype = "float32") {
	if (dtype === "complex64") {
		const real$1 = zeros(shape, "float32");
		const imag$1 = zeros(shape, "float32");
		return complex(real$1, imag$1);
	}
	const values = makeZerosTypedArray(sizeFromShape(shape), dtype);
	return ENGINE.makeTensor(values, shape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/ones.js
/**
* Creates a `tf.Tensor` with all elements set to 1.
*
* ```js
* tf.ones([2, 2]).print();
* ```
*
* @param shape An array of integers defining the output tensor shape.
* @param dtype The type of an element in the resulting tensor. Defaults to
*     'float'.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function ones(shape, dtype = "float32") {
	if (dtype === "complex64") {
		const real$1 = ones(shape, "float32");
		const imag$1 = zeros(shape, "float32");
		return complex(real$1, imag$1);
	}
	const values = makeOnesTypedArray(sizeFromShape(shape), dtype);
	return ENGINE.makeTensor(values, shape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/meshgrid.js
/**
* Broadcasts parameters for evaluation on an N-D grid.
*
* Given N one-dimensional coordinate arrays `*args`, returns a list `outputs`
* of N-D coordinate arrays for evaluating expressions on an N-D grid.
*
* Notes:
* `meshgrid` supports cartesian ('xy') and matrix ('ij') indexing conventions.
* When the `indexing` argument is set to 'xy' (the default), the broadcasting
* instructions for the first two dimensions are swapped.
* Examples:
* Calling `const [X, Y] = meshgrid(x, y)` with the tensors
*
* ```javascript
* const x = [1, 2, 3];
* const y = [4, 5, 6];
* const [X, Y] = tf.meshgrid(x, y);
* // X = [[1, 2, 3],
* //      [1, 2, 3],
* //      [1, 2, 3]]
* // Y = [[4, 4, 4],
* //      [5, 5, 5],
* //      [6, 6, 6]]
* ```
*
* @param x Tensor with rank geq 1.
* @param y Tensor with rank geq 1.
* @param indexing
*
* @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
*/
function meshgrid(x, y, { indexing = "xy" } = {}) {
	if (indexing !== "xy" && indexing !== "ij") throw new TypeError(`${indexing} is not a valid third argument to meshgrid`);
	if (x === void 0) return [];
	let $x = convertToTensor(x, "x", "meshgrid", x instanceof Tensor ? x.dtype : "float32");
	if (y === void 0) return [$x];
	let $y = convertToTensor(y, "y", "meshgrid", y instanceof Tensor ? y.dtype : "float32");
	const w = sizeFromShape($x.shape);
	const h = sizeFromShape($y.shape);
	if (indexing === "xy") {
		$x = reshape($x, [1, -1]);
		$y = reshape($y, [-1, 1]);
		return [matMul(ones([h, 1], $x.dtype), $x), matMul($y, ones([1, w], $y.dtype))];
	}
	$x = reshape($x, [-1, 1]);
	$y = reshape($y, [1, -1]);
	return [matMul($x, ones([1, h], $x.dtype)), matMul(ones([w, 1], $y.dtype), $y)];
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/min.js
/**
* Computes the minimum value from the input.
*
* Reduces the input along the dimensions given in `axes`. Unless `keepDims`
* is true, the rank of the array is reduced by 1 for each entry in `axes`.
* If `keepDims` is true, the reduced dimensions are retained with length 1.
* If `axes` has no entries, all dimensions are reduced, and an array with a
* single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.min().print();  // or tf.min(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* const axis = 1;
* x.min(axis).print();  // or tf.min(x, axis)
* ```
*
* @param x The input Tensor.
* @param axis The dimension(s) to reduce. By default it reduces
*     all dimensions.
* @param keepDims If true, retains reduced dimensions with size 1.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function min_(x, axis = null, keepDims = false) {
	const inputs = { x: convertToTensor(x, "x", "min") };
	const attrs = {
		axis,
		keepDims
	};
	return ENGINE.runKernel(Min, inputs, attrs);
}
const min = op({ min_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/minimum.js
/**
* Returns the min of a and b (`a < b ? a : b`) element-wise.
* Supports broadcasting.
*
* We also expose `minimumStrict` which has the same signature as this op and
* asserts that `a` and `b` are the same shape (does not broadcast).
*
* ```js
* const a = tf.tensor1d([1, 4, 3, 16]);
* const b = tf.tensor1d([1, 2, 9, 4]);
*
* a.minimum(b).print();  // or tf.minimum(a, b)
* ```
*
* ```js
* // Broadcast minimum a with b.
* const a = tf.tensor1d([2, 4, 6, 8]);
* const b = tf.scalar(5);
*
* a.minimum(b).print();  // or tf.minimum(a, b)
* ```
*
* @param a The first tensor.
* @param b The second tensor. Must have the same type as `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function minimum_(a, b) {
	let $a = convertToTensor(a, "a", "minimum");
	let $b = convertToTensor(b, "b", "minimum");
	[$a, $b] = makeTypesMatch($a, $b);
	if ($a.dtype === "bool") {
		$a = cast($a, "int32");
		$b = cast($b, "int32");
	}
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Minimum, inputs);
}
const minimum = op({ minimum_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/mirror_pad.js
/**
* Pads a `tf.Tensor` using mirror padding.
*
* This operation implements the `REFLECT` and `SYMMETRIC` modes of pad.
*
* ```js
* const x = tf.range(0, 9).reshape([1, 1, 3, 3]);
* x.mirrorPad([[0, 0], [0, 0], [2, 2], [2, 2]], 'reflect').print();
* ```
* @param x The tensor to pad.
* @param paddings An array of length `R` (the rank of the tensor), where
* each element is a length-2 tuple of ints `[padBefore, padAfter]`,
* specifying how much to pad along each dimension of the tensor.
* In "reflect" mode, the padded regions do not include the borders,
* while in "symmetric" mode the padded regions do include the borders.
* For example, if the input is `[1, 2, 3]` and paddings is `[0, 2]`,
* then the output is `[1, 2, 3, 2, 1]` in "reflect" mode, and
* `[1, 2, 3, 3, 2]` in "symmetric" mode.
* If `mode` is "reflect" then both `paddings[D, 0]` and `paddings[D, 1]`
* must be no greater than `x.shape[D] - 1`. If mode is "symmetric"
* then both `paddings[D, 0]` and `paddings[D, 1]` must be no greater than
* `x.shape[D]`
* @param mode String to specify padding mode. Can be `'reflect' | 'symmetric'`
*/
/** @doc {heading: 'Tensors', subheading: 'Transformations'} */
function mirrorPad_(x, paddings, mode) {
	assert(mode === "reflect" || mode === "symmetric", () => `Invalid mode. Mode must be either reflect or symmetric. Got ${mode}.`);
	const $x = convertToTensor(x, "x", "mirrorPad");
	if ($x.rank === 0) throw new Error("mirrorPad(scalar) is not defined. Pass non-scalar to mirrorPad");
	assert(paddings.length === $x.rank, () => `Padding doesn't match input. Must be ${$x.rank}. Got ${paddings.length}.`);
	const shapeOffset = mode === "reflect" ? 1 : 0;
	for (let i = 0; i < $x.rank; i++) {
		assert(paddings[i].length === 2, () => `Invalid number of paddings. Must be length of 2 each.`);
		assert(paddings[i][0] >= 0 && paddings[i][0] <= $x.shape[i] - shapeOffset && paddings[i][1] >= 0 && paddings[i][1] <= $x.shape[i] - shapeOffset, () => `Padding in dimension ${i} cannot be greater than or equal to ${$x.shape[i] - shapeOffset} or less than 0 for input of shape ${$x.shape}`);
	}
	const attrs = {
		paddings,
		mode
	};
	const inputs = { x: $x };
	return ENGINE.runKernel(MirrorPad, inputs, attrs);
}
const mirrorPad = op({ mirrorPad_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/mod.js
/**
* Returns the mod of a and b element-wise.
* `floor(x / y) * y + mod(x, y) = x`
* Supports broadcasting.
*
* We also expose `tf.modStrict` which has the same signature as this op and
* asserts that `a` and `b` are the same shape (does not broadcast).
*
* ```js
* const a = tf.tensor1d([1, 4, 3, 16]);
* const b = tf.tensor1d([1, 2, 9, 4]);
*
* a.mod(b).print();  // or tf.mod(a, b)
* ```
*
* ```js
* // Broadcast a mod b.
* const a = tf.tensor1d([2, 4, 6, 8]);
* const b = tf.scalar(5);
*
* a.mod(b).print();  // or tf.mod(a, b)
* ```
*
* @param a The first tensor.
* @param b The second tensor. Must have the same type as `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function mod_(a, b) {
	let $a = convertToTensor(a, "a", "mod");
	let $b = convertToTensor(b, "b", "mod");
	[$a, $b] = makeTypesMatch($a, $b);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(Mod, inputs);
}
const mod = op({ mod_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/square.js
/**
* Computes square of `x` element-wise: `x ^ 2`
*
* ```js
* const x = tf.tensor1d([1, 2, Math.sqrt(2), -1]);
*
* x.square().print();  // or tf.square(x)
* ```
* @param x The input Tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function square_(x) {
	const $x = convertToTensor(x, "x", "square");
	return ENGINE.runKernel("Square", { x: $x }, {});
}
const square = op({ square_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/moments.js
/**
* Calculates the mean and variance of `x`. The mean and variance are
* calculated by aggregating the contents of `x` across `axes`. If `x` is
* 1-D and `axes = [0]` this is just the mean and variance of a vector.
*
* @param x The input tensor.
* @param axis The dimension(s) along with to compute mean and
*     variance. By default it reduces all dimensions.
* @param keepDims If true, the moments have the same dimensionality as the
*     input.
* @return An object with two keys: `mean` and `variance`.
*
* @doc {heading: 'Operations', subheading: 'Normalization'}
*/
function moments_(x, axis = null, keepDims = false) {
	x = convertToTensor(x, "x", "moments");
	const axes = parseAxisParam(axis, x.shape);
	const xMean = mean(x, axes, keepDims);
	let keepDimsShape = xMean.shape;
	if (!keepDims) keepDimsShape = expandShapeToKeepDim(xMean.shape, axes);
	const devSquared = square(sub(cast(x, "float32"), reshape(xMean, keepDimsShape)));
	const variance = mean(devSquared, axes, keepDims);
	return {
		mean: xMean,
		variance
	};
}
const moments = op({ moments_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/multi_rnn_cell.js
/**
* Computes the next states and outputs of a stack of LSTMCells.
*
* Each cell output is used as input to the next cell.
*
* Returns `[cellState, cellOutput]`.
*
* Derived from tf.contrib.rn.MultiRNNCell.
*
* @param lstmCells Array of LSTMCell functions.
* @param data The input to the cell.
* @param c Array of previous cell states.
* @param h Array of previous cell outputs.
*
* @doc {heading: 'Operations', subheading: 'RNN'}
*/
function multiRNNCell_(lstmCells, data, c, h) {
	const $data = convertToTensor(data, "data", "multiRNNCell");
	const $c = convertToTensorArray(c, "c", "multiRNNCell");
	const $h = convertToTensorArray(h, "h", "multiRNNCell");
	let input = $data;
	const newStates = [];
	for (let i = 0; i < lstmCells.length; i++) {
		const output = lstmCells[i](input, $c[i], $h[i]);
		newStates.push(output[0]);
		newStates.push(output[1]);
		input = output[1];
	}
	const newC = [];
	const newH = [];
	for (let i = 0; i < newStates.length; i += 2) {
		newC.push(newStates[i]);
		newH.push(newStates[i + 1]);
	}
	return [newC, newH];
}
const multiRNNCell = op({ multiRNNCell_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/multinomial.js
/**
* Creates a `tf.Tensor` with values drawn from a multinomial distribution.
*
* ```js
* const probs = tf.tensor([.75, .25]);
* tf.multinomial(probs, 3).print();
* ```
*
* @param logits 1D array with unnormalized log-probabilities, or
*     2D array of shape `[batchSize, numOutcomes]`. See the `normalized`
*     parameter.
* @param numSamples Number of samples to draw for each row slice.
* @param seed The seed number.
* @param normalized Whether the provided `logits` are normalized true
*     probabilities (sum to 1). Defaults to false.
* @return 1D array of shape `[numSamples]`, or 2D array of shape
*     `[batchSize, numSamples]`, depending on the rank of the input.
*
* @doc {heading: 'Tensors', subheading: 'Random'}
*/
function multinomial_(logits, numSamples, seed, normalized = false) {
	const $logits = convertToTensor(logits, "logits", "multinomial");
	const numOutcomes = $logits.size;
	const origRank = $logits.rank;
	if (numOutcomes < 2) throw new Error(`Error in multinomial: you need at least 2 outcomes, but got ${numOutcomes}.`);
	if (origRank > 2) throw new Error(`Rank of probabilities must be 1 or 2, but is ${origRank}`);
	seed = seed || Math.random();
	const inputs = { logits: origRank === 1 ? reshape($logits, [1, -1]) : $logits };
	const attrs = {
		numSamples,
		seed,
		normalized
	};
	const res = ENGINE.runKernel(Multinomial, inputs, attrs);
	return origRank === 1 ? reshape(res, [res.size]) : res;
}
const multinomial = op({ multinomial_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/not_equal.js
/**
* Returns the truth value of (a != b) element-wise. Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
* const b = tf.tensor1d([0, 2, 3]);
*
* a.notEqual(b).print();
* ```
* @param a The first input tensor.
* @param b The second input tensor. Must have the same dtype as `a`.
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
function notEqual_(a, b) {
	let $a = convertToTensor(a, "a", "notEqual", "string_or_numeric");
	let $b = convertToTensor(b, "b", "notEqual", "string_or_numeric");
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(NotEqual, inputs);
}
const notEqual = op({ notEqual_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/ones_like.js
/**
* Creates a `tf.Tensor` with all elements set to 1 with the same shape as the
* given tensor.
*
* ```js
* const x = tf.tensor([1, 2]);
* tf.onesLike(x).print();
* ```
* @param x A tensor.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function onesLike_(x) {
	const inputs = { x: convertToTensor(x, "x", "onesLike") };
	return ENGINE.runKernel(OnesLike, inputs);
}
const onesLike = op({ onesLike_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/outer_product.js
/**
* Computes the outer product of two vectors, `v1` and `v2`.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
* const b = tf.tensor1d([3, 4, 5]);
*
* tf.outerProduct(a, b).print();
* ```
* @param v1 The first vector in the outer product operation.
* @param v2 The second vector in the outer product operation.
*
* @doc {heading: 'Operations', subheading: 'Matrices'}
*/
function outerProduct_(v1, v2) {
	const $v1 = convertToTensor(v1, "v1", "outerProduct");
	const $v2 = convertToTensor(v2, "v2", "outerProduct");
	assert($v1.rank === 1 && $v2.rank === 1, () => `Error in outerProduct: inputs must be rank 1, but got ranks ${$v1.rank} and ${$v2.rank}.`);
	const v12D = reshape($v1, [-1, 1]);
	const v22D = reshape($v2, [1, -1]);
	return matMul(v12D, v22D);
}
const outerProduct = op({ outerProduct_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/pad.js
/**
* Pads a `tf.Tensor` with a given value and paddings.
*
* This operation implements `CONSTANT` mode. For `REFLECT` and `SYMMETRIC`,
* refer to `tf.mirrorPad`
*
* Also available are stricter rank-specific methods with the same signature
* as this method that assert that `paddings` is of given length.
*   - `tf.pad1d`
*   - `tf.pad2d`
*   - `tf.pad3d`
*   - `tf.pad4d`
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
* x.pad([[1, 2]]).print();
* ```
* @param x The tensor to pad.
* @param paddings An array of length `R` (the rank of the tensor), where
* each element is a length-2 tuple of ints `[padBefore, padAfter]`,
* specifying how much to pad along each dimension of the tensor.
* @param constantValue The pad value to use. Defaults to 0.
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function pad_(x, paddings, constantValue = 0) {
	const $x = convertToTensor(x, "x", "pad");
	if ($x.rank === 0) throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");
	const attrs = {
		paddings,
		constantValue
	};
	const inputs = { x: $x };
	return ENGINE.runKernel(PadV2, inputs, attrs);
}
const pad = op({ pad_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/pad1d.js
/**
* Pads a `tf.Tensor1D` with a given value and paddings. See `pad` for details.
*/
function pad1d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 2, () => "Invalid number of paddings. Must be length of 2.");
	return pad(x, [paddings], constantValue);
}
const pad1d = op({ pad1d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/pad2d.js
/**
* Pads a `tf.Tensor2D` with a given value and paddings. See `pad` for details.
*/
function pad2d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 2 && paddings[0].length === 2 && paddings[1].length === 2, () => "Invalid number of paddings. Must be length of 2 each.");
	return pad(x, paddings, constantValue);
}
const pad2d = op({ pad2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/pad3d.js
/**
* Pads a `tf.Tensor3D` with a given value and paddings. See `pad` for details.
*/
function pad3d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 3 && paddings[0].length === 2 && paddings[1].length === 2 && paddings[2].length === 2, () => "Invalid number of paddings. Must be length of 2 each.");
	return pad(x, paddings, constantValue);
}
const pad3d = op({ pad3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/pad4d.js
/**
* Pads a `tf.Tensor4D` with a given value and paddings. See `pad` for details.
*/
function pad4d_(x, paddings, constantValue = 0) {
	assert(paddings.length === 4 && paddings[0].length === 2 && paddings[1].length === 2 && paddings[2].length === 2 && paddings[3].length === 2, () => "Invalid number of paddings. Must be length of 2 each.");
	return pad(x, paddings, constantValue);
}
const pad4d = op({ pad4d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/space_to_batch_nd.js
/**
* This operation divides "spatial" dimensions `[1, ..., M]` of the input into
* a grid of blocks of shape `blockShape`, and interleaves these blocks with
* the "batch" dimension (0) such that in the output, the spatial
* dimensions `[1, ..., M]` correspond to the position within the grid,
* and the batch dimension combines both the position within a spatial block
* and the original batch position. Prior to division into blocks,
* the spatial dimensions of the input are optionally zero padded
* according to `paddings`. See below for a precise description.
*
* ```js
* const x = tf.tensor4d([1, 2, 3, 4], [1, 2, 2, 1]);
* const blockShape = [2, 2];
* const paddings = [[0, 0], [0, 0]];
*
* x.spaceToBatchND(blockShape, paddings).print();
* ```
*
* @param x A `tf.Tensor`. N-D with `x.shape` = `[batch] + spatialShape +
* remainingShape`, where spatialShape has `M` dimensions.
* @param blockShape A 1-D array. Must have shape `[M]`, all values must
* be >= 1.
* @param paddings A 2-D array. Must have shape `[M, 2]`, all values must be >=
*     0. `paddings[i] = [padStart, padEnd]` specifies the amount to zero-pad
* from input dimension `i + 1`, which corresponds to spatial dimension `i`. It
* is required that
* `(inputShape[i + 1] + padStart + padEnd) % blockShape[i] === 0`
*
* This operation is equivalent to the following steps:
*
* 1. Zero-pad the start and end of dimensions `[1, ..., M]` of the input
* according to `paddings` to produce `padded` of shape paddedShape.
*
* 2. Reshape `padded` to `reshapedPadded` of shape:
* `[batch] + [paddedShape[1] / blockShape[0], blockShape[0], ...,
* paddedShape[M] / blockShape[M-1], blockShape[M-1]] + remainingShape`
*
* 3. Permute dimensions of `reshapedPadded` to produce `permutedReshapedPadded`
* of shape: `blockShape + [batch] + [paddedShape[1] / blockShape[0], ...,
* paddedShape[M] / blockShape[M-1]] + remainingShape`
*
* 4. Reshape `permutedReshapedPadded` to flatten `blockShape` into the
* batch dimension, producing an output tensor of shape:
* `[batch * prod(blockShape)] + [paddedShape[1] / blockShape[0], ...,
* paddedShape[M] / blockShape[M-1]] + remainingShape`
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function spaceToBatchND_(x, blockShape, paddings) {
	const $x = convertToTensor(x, "x", "spaceToBatchND");
	assert($x.rank >= 1 + blockShape.length, () => `input rank ${$x.rank} should be > than [blockShape] ${blockShape.length}`);
	assert(paddings.length === blockShape.length, () => `paddings.shape[0] ${paddings.length} must be equal to [blockShape] ${blockShape.length}`);
	assert($x.shape.reduce((a, b, i) => {
		if (i > 0 && i <= blockShape.length) return a && (b + paddings[i - 1][0] + paddings[i - 1][1]) % blockShape[i - 1] === 0;
		return a;
	}, true), () => `input spatial dimensions ${$x.shape.slice(1)} with paddings ${paddings.toString()} must be divisible by blockShapes ${blockShape.toString()}`);
	const inputs = { x: $x };
	const attrs = {
		blockShape,
		paddings
	};
	return ENGINE.runKernel(SpaceToBatchND, inputs, attrs);
}
const spaceToBatchND = op({ spaceToBatchND_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/pool.js
/**
* Performs an N-D pooling operation
*
* @param input The input tensor, of rank 4 or rank 3 of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
* @param windowShape The filter size: `[filterHeight, filterWidth]`. If
*     `filterSize` is a single number, then `filterHeight == filterWidth`.
* @param poolingType The type of pooling, either 'max' or 'avg'.
* @param pad The type of padding algorithm:
*    - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*    - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*    - For more info, see this guide:
*     [https://www.tensorflow.org/api_guides/python/nn#Convolution](
*         https://www.tensorflow.org/api_guides/python/nn#Convolution)
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     in dilated pooling. Defaults to `[1, 1]`. If `dilationRate` is a single
*     number, then `dilationHeight == dilationWidth`. If it is greater than
*     1, then all values of `strides` must be 1.
* @param strides The strides of the pooling: `[strideHeight, strideWidth]`. If
*     `strides` is a single number, then `strideHeight == strideWidth`.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function pool_(input, windowShape, poolingType, pad$1, dilations, strides) {
	if (dilations == null) dilations = [1, 1];
	if (strides == null) strides = 1;
	if (pad$1 === 0) pad$1 = "valid";
	const $x = convertToTensor(input, "x", "maxPool");
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => `Error in pool: Either strides or dilations must be 1. Got strides ${strides} and dilations '${dilations}'`);
	const convInfo = computePool2DInfo(x4D.shape, windowShape, strides, dilations, pad$1);
	const dilation = [convInfo.dilationHeight, convInfo.dilationWidth];
	let basePadding;
	if (pad$1 === "same") basePadding = withSpaceToBatchBasePaddings([convInfo.filterHeight, convInfo.filterWidth], dilation);
	else basePadding = [[0, 0], [0, 0]];
	const isDilationOne = dilation[0] === 1 && dilation[1] === 1;
	const [adjustedPadding, adjustedCrops] = requiredSpaceToBatchPaddings([convInfo.inHeight, convInfo.inWidth], dilation, basePadding);
	const convertedPad = isDilationOne ? pad$1 : "valid";
	const convertedX = isDilationOne ? x4D : spaceToBatchND(x4D, dilation, adjustedPadding);
	const y = (poolingType === "avg" ? () => avgPool(convertedX, windowShape, strides, convertedPad) : () => maxPool(convertedX, windowShape, strides, convertedPad))();
	const res = isDilationOne ? y : batchToSpaceND(y, dilation, adjustedCrops);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
function requiredSpaceToBatchPaddings(inputShape, blockShape, basePadding) {
	const padStart = basePadding.map((b) => b[0]);
	const origPadEnd = basePadding.map((b) => b[1]);
	const fullInputShape = inputShape.concat(padStart, origPadEnd);
	const padEndExtra = blockShape.map((b, i) => (b - fullInputShape[i] % b) % b);
	const padEnd = origPadEnd.map((s, i) => s + padEndExtra[i]);
	const paddings = blockShape.map((_, i) => [padStart[i], padEnd[i]]);
	const crops = blockShape.map((_, i) => [0, padEndExtra[i]]);
	return [paddings, crops];
}
function withSpaceToBatchBasePaddings(filterShape, dilation) {
	const padExtraShape = filterShape.map((s, i) => {
		return s + (s - 1) * (dilation[i] - 1);
	}).map((s) => s - 1);
	const padExtraStart = padExtraShape.map((s) => Math.floor(s / 2));
	const padExtraEnd = padExtraShape.map((s, i) => s - padExtraStart[i]);
	return padExtraShape.map((_, i) => {
		return [padExtraStart[i], padExtraEnd[i]];
	});
}
const pool = op({ pool_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/pow.js
/**
* Computes the power of one `tf.Tensor` to another. Supports broadcasting.
*
* Given a `tf.Tensor` x and a `tf.Tensor` y, this operation computes x^y for
* corresponding elements in x and y. The result's dtype will be the upcasted
* type of the `base` and `exp` dtypes.
*
* ```js
* const a = tf.tensor([[2, 3], [4, 5]])
* const b = tf.tensor([[1, 2], [3, 0]]).toInt();
*
* a.pow(b).print();  // or tf.pow(a, b)
* ```
*
* ```js
* const a = tf.tensor([[1, 2], [3, 4]])
* const b = tf.tensor(2).toInt();
*
* a.pow(b).print();  // or tf.pow(a, b)
* ```
* We also expose `powStrict` which has the same signature as this op and
* asserts that `base` and `exp` are the same shape (does not broadcast).
*
* @param base The base `tf.Tensor` to pow element-wise.
* @param exp The exponent `tf.Tensor` to pow element-wise.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function pow_(base, exp$1) {
	let $base = convertToTensor(base, "base", "pow");
	let $exp = convertToTensor(exp$1, "exp", "pow");
	[$base, $exp] = makeTypesMatch($base, $exp);
	const inputs = {
		a: $base,
		b: $exp
	};
	return ENGINE.runKernel(Pow, inputs);
}
const pow = op({ pow_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/prelu.js
/**
* Computes leaky rectified linear element-wise with parametric alphas.
*
* `x < 0 ? alpha * x : f(x) = x`
*
* ```js
* const x = tf.tensor1d([-1, 2, -3, 4]);
* const alpha = tf.scalar(0.1);
*
* x.prelu(alpha).print();  // or tf.prelu(x, alpha)
* ```
* @param x The input tensor.
* @param alpha Scaling factor for negative values.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function prelu_(x, alpha) {
	const $x = convertToTensor(x, "x", "prelu");
	const $alpha = convertToTensor(alpha, "alpha", "prelu");
	const inputs = {
		x: $x,
		alpha: $alpha
	};
	return ENGINE.runKernel(Prelu, inputs);
}
const prelu = op({ prelu_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/prod.js
/**
* Computes the product of elements across dimensions of a `tf.Tensor`.
*
* Reduces the input along the dimensions given in `axes`. Unless `keepDims`
* is true, the rank of the `tf.Tensor` is reduced by 1 for each entry in
* `axes`. If `keepDims` is true, the reduced dimensions are retained with
* length 1. If `axes` has no entries, all dimensions are reduced, and a
* `tf.Tensor` with a single element is returned.
*
* ```js
* const x = tf.tensor1d([1, 2, 3]);
*
* x.prod().print();  // or tf.prod(x)
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* const axis = 1;
* x.prod(axis).print();  // or tf.prod(x, axis)
* ```
*
* @param x The input tensor to compute the product over. If the dtype is `bool`
*   it will be converted to `int32` and the output dtype will be `int32`.
* @param axis The dimension(s) to reduce. By default it reduces
*     all dimensions.
* @param keepDims If true, retains reduced dimensions with size 1.
*
* @doc {heading: 'Operations', subheading: 'Reduction'}
*/
function prod_(x, axis = null, keepDims = false) {
	let $x = convertToTensor(x, "x", "prod");
	if ($x.dtype === "bool") $x = cast($x, "int32");
	const inputs = { x: $x };
	const attrs = {
		axis,
		keepDims
	};
	return ENGINE.runKernel(Prod, inputs, attrs);
}
const prod = op({ prod_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/rand.js
/**
* Creates a `tf.Tensor` with values sampled from a random number generator
* function defined by the user.
*
* @param shape An array of integers defining the output tensor shape.
* @param randFunction A random number generator function which is called
* for each element in the output tensor.
* @param dtype The data type of the output tensor. Defaults to 'float32'.
*
* @doc {heading: 'Tensors', subheading: 'Random'}
*/
function rand_(shape, randFunction, dtype) {
	const size = sizeFromShape(shape);
	let values = null;
	if (dtype == null || dtype === "float32") values = new Float32Array(size);
	else if (dtype === "int32") values = new Int32Array(size);
	else if (dtype === "bool") values = new Uint8Array(size);
	else throw new Error(`Unknown data type ${dtype}`);
	for (let i = 0; i < size; i++) values[i] = randFunction();
	return ENGINE.makeTensor(values, shape, dtype);
}
const rand = op({ rand_ });

//#endregion
//#region node_modules/seedrandom/lib/alea.js
var require_alea = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/lib/alea.js": ((exports, module) => {
	(function(global$1, module$1, define$1) {
		function Alea(seed) {
			var me = this, mash = Mash();
			me.next = function() {
				var t = 2091639 * me.s0 + me.c * 23283064365386963e-26;
				me.s0 = me.s1;
				me.s1 = me.s2;
				return me.s2 = t - (me.c = t | 0);
			};
			me.c = 1;
			me.s0 = mash(" ");
			me.s1 = mash(" ");
			me.s2 = mash(" ");
			me.s0 -= mash(seed);
			if (me.s0 < 0) me.s0 += 1;
			me.s1 -= mash(seed);
			if (me.s1 < 0) me.s1 += 1;
			me.s2 -= mash(seed);
			if (me.s2 < 0) me.s2 += 1;
			mash = null;
		}
		function copy(f, t) {
			t.c = f.c;
			t.s0 = f.s0;
			t.s1 = f.s1;
			t.s2 = f.s2;
			return t;
		}
		function impl(seed, opts) {
			var xg = new Alea(seed), state = opts && opts.state, prng = xg.next;
			prng.int32 = function() {
				return xg.next() * 4294967296 | 0;
			};
			prng.double = function() {
				return prng() + (prng() * 2097152 | 0) * 11102230246251565e-32;
			};
			prng.quick = prng;
			if (state) {
				if (typeof state == "object") copy(state, xg);
				prng.state = function() {
					return copy(xg, {});
				};
			}
			return prng;
		}
		function Mash() {
			var n = 4022871197;
			var mash = function(data) {
				data = data.toString();
				for (var i = 0; i < data.length; i++) {
					n += data.charCodeAt(i);
					var h = .02519603282416938 * n;
					n = h >>> 0;
					h -= n;
					h *= n;
					n = h >>> 0;
					h -= n;
					n += h * 4294967296;
				}
				return (n >>> 0) * 23283064365386963e-26;
			};
			return mash;
		}
		if (module$1 && module$1.exports) module$1.exports = impl;
		else if (define$1 && define$1.amd) define$1(function() {
			return impl;
		});
		else this.alea = impl;
	})(exports, typeof module == "object" && module, typeof define == "function" && define);
}) });

//#endregion
//#region node_modules/seedrandom/lib/xor128.js
var require_xor128 = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/lib/xor128.js": ((exports, module) => {
	(function(global$1, module$1, define$1) {
		function XorGen(seed) {
			var me = this, strseed = "";
			me.x = 0;
			me.y = 0;
			me.z = 0;
			me.w = 0;
			me.next = function() {
				var t = me.x ^ me.x << 11;
				me.x = me.y;
				me.y = me.z;
				me.z = me.w;
				return me.w ^= me.w >>> 19 ^ t ^ t >>> 8;
			};
			if (seed === (seed | 0)) me.x = seed;
			else strseed += seed;
			for (var k = 0; k < strseed.length + 64; k++) {
				me.x ^= strseed.charCodeAt(k) | 0;
				me.next();
			}
		}
		function copy(f, t) {
			t.x = f.x;
			t.y = f.y;
			t.z = f.z;
			t.w = f.w;
			return t;
		}
		function impl(seed, opts) {
			var xg = new XorGen(seed), state = opts && opts.state, prng = function() {
				return (xg.next() >>> 0) / 4294967296;
			};
			prng.double = function() {
				do
					var top = xg.next() >>> 11, bot = (xg.next() >>> 0) / 4294967296, result = (top + bot) / (1 << 21);
				while (result === 0);
				return result;
			};
			prng.int32 = xg.next;
			prng.quick = prng;
			if (state) {
				if (typeof state == "object") copy(state, xg);
				prng.state = function() {
					return copy(xg, {});
				};
			}
			return prng;
		}
		if (module$1 && module$1.exports) module$1.exports = impl;
		else if (define$1 && define$1.amd) define$1(function() {
			return impl;
		});
		else this.xor128 = impl;
	})(exports, typeof module == "object" && module, typeof define == "function" && define);
}) });

//#endregion
//#region node_modules/seedrandom/lib/xorwow.js
var require_xorwow = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/lib/xorwow.js": ((exports, module) => {
	(function(global$1, module$1, define$1) {
		function XorGen(seed) {
			var me = this, strseed = "";
			me.next = function() {
				var t = me.x ^ me.x >>> 2;
				me.x = me.y;
				me.y = me.z;
				me.z = me.w;
				me.w = me.v;
				return (me.d = me.d + 362437 | 0) + (me.v = me.v ^ me.v << 4 ^ (t ^ t << 1)) | 0;
			};
			me.x = 0;
			me.y = 0;
			me.z = 0;
			me.w = 0;
			me.v = 0;
			if (seed === (seed | 0)) me.x = seed;
			else strseed += seed;
			for (var k = 0; k < strseed.length + 64; k++) {
				me.x ^= strseed.charCodeAt(k) | 0;
				if (k == strseed.length) me.d = me.x << 10 ^ me.x >>> 4;
				me.next();
			}
		}
		function copy(f, t) {
			t.x = f.x;
			t.y = f.y;
			t.z = f.z;
			t.w = f.w;
			t.v = f.v;
			t.d = f.d;
			return t;
		}
		function impl(seed, opts) {
			var xg = new XorGen(seed), state = opts && opts.state, prng = function() {
				return (xg.next() >>> 0) / 4294967296;
			};
			prng.double = function() {
				do
					var top = xg.next() >>> 11, bot = (xg.next() >>> 0) / 4294967296, result = (top + bot) / (1 << 21);
				while (result === 0);
				return result;
			};
			prng.int32 = xg.next;
			prng.quick = prng;
			if (state) {
				if (typeof state == "object") copy(state, xg);
				prng.state = function() {
					return copy(xg, {});
				};
			}
			return prng;
		}
		if (module$1 && module$1.exports) module$1.exports = impl;
		else if (define$1 && define$1.amd) define$1(function() {
			return impl;
		});
		else this.xorwow = impl;
	})(exports, typeof module == "object" && module, typeof define == "function" && define);
}) });

//#endregion
//#region node_modules/seedrandom/lib/xorshift7.js
var require_xorshift7 = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/lib/xorshift7.js": ((exports, module) => {
	(function(global$1, module$1, define$1) {
		function XorGen(seed) {
			var me = this;
			me.next = function() {
				var X = me.x, i = me.i, t = X[i], v;
				t ^= t >>> 7;
				v = t ^ t << 24;
				t = X[i + 1 & 7];
				v ^= t ^ t >>> 10;
				t = X[i + 3 & 7];
				v ^= t ^ t >>> 3;
				t = X[i + 4 & 7];
				v ^= t ^ t << 7;
				t = X[i + 7 & 7];
				t = t ^ t << 13;
				v ^= t ^ t << 9;
				X[i] = v;
				me.i = i + 1 & 7;
				return v;
			};
			function init(me$1, seed$1) {
				var j, X = [];
				if (seed$1 === (seed$1 | 0)) X[0] = seed$1;
				else {
					seed$1 = "" + seed$1;
					for (j = 0; j < seed$1.length; ++j) X[j & 7] = X[j & 7] << 15 ^ seed$1.charCodeAt(j) + X[j + 1 & 7] << 13;
				}
				while (X.length < 8) X.push(0);
				for (j = 0; j < 8 && X[j] === 0; ++j);
				if (j == 8) X[7] = -1;
				else X[j];
				me$1.x = X;
				me$1.i = 0;
				for (j = 256; j > 0; --j) me$1.next();
			}
			init(me, seed);
		}
		function copy(f, t) {
			t.x = f.x.slice();
			t.i = f.i;
			return t;
		}
		function impl(seed, opts) {
			if (seed == null) seed = +/* @__PURE__ */ new Date();
			var xg = new XorGen(seed), state = opts && opts.state, prng = function() {
				return (xg.next() >>> 0) / 4294967296;
			};
			prng.double = function() {
				do
					var top = xg.next() >>> 11, bot = (xg.next() >>> 0) / 4294967296, result = (top + bot) / (1 << 21);
				while (result === 0);
				return result;
			};
			prng.int32 = xg.next;
			prng.quick = prng;
			if (state) {
				if (state.x) copy(state, xg);
				prng.state = function() {
					return copy(xg, {});
				};
			}
			return prng;
		}
		if (module$1 && module$1.exports) module$1.exports = impl;
		else if (define$1 && define$1.amd) define$1(function() {
			return impl;
		});
		else this.xorshift7 = impl;
	})(exports, typeof module == "object" && module, typeof define == "function" && define);
}) });

//#endregion
//#region node_modules/seedrandom/lib/xor4096.js
var require_xor4096 = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/lib/xor4096.js": ((exports, module) => {
	(function(global$1, module$1, define$1) {
		function XorGen(seed) {
			var me = this;
			me.next = function() {
				var w = me.w, X = me.X, i = me.i, t, v;
				me.w = w = w + 1640531527 | 0;
				v = X[i + 34 & 127];
				t = X[i = i + 1 & 127];
				v ^= v << 13;
				t ^= t << 17;
				v ^= v >>> 15;
				t ^= t >>> 12;
				v = X[i] = v ^ t;
				me.i = i;
				return v + (w ^ w >>> 16) | 0;
			};
			function init(me$1, seed$1) {
				var t, v, i, j, w, X = [], limit = 128;
				if (seed$1 === (seed$1 | 0)) {
					v = seed$1;
					seed$1 = null;
				} else {
					seed$1 = seed$1 + "\0";
					v = 0;
					limit = Math.max(limit, seed$1.length);
				}
				for (i = 0, j = -32; j < limit; ++j) {
					if (seed$1) v ^= seed$1.charCodeAt((j + 32) % seed$1.length);
					if (j === 0) w = v;
					v ^= v << 10;
					v ^= v >>> 15;
					v ^= v << 4;
					v ^= v >>> 13;
					if (j >= 0) {
						w = w + 1640531527 | 0;
						t = X[j & 127] ^= v + w;
						i = 0 == t ? i + 1 : 0;
					}
				}
				if (i >= 128) X[(seed$1 && seed$1.length || 0) & 127] = -1;
				i = 127;
				for (j = 512; j > 0; --j) {
					v = X[i + 34 & 127];
					t = X[i = i + 1 & 127];
					v ^= v << 13;
					t ^= t << 17;
					v ^= v >>> 15;
					t ^= t >>> 12;
					X[i] = v ^ t;
				}
				me$1.w = w;
				me$1.X = X;
				me$1.i = i;
			}
			init(me, seed);
		}
		function copy(f, t) {
			t.i = f.i;
			t.w = f.w;
			t.X = f.X.slice();
			return t;
		}
		function impl(seed, opts) {
			if (seed == null) seed = +/* @__PURE__ */ new Date();
			var xg = new XorGen(seed), state = opts && opts.state, prng = function() {
				return (xg.next() >>> 0) / 4294967296;
			};
			prng.double = function() {
				do
					var top = xg.next() >>> 11, bot = (xg.next() >>> 0) / 4294967296, result = (top + bot) / (1 << 21);
				while (result === 0);
				return result;
			};
			prng.int32 = xg.next;
			prng.quick = prng;
			if (state) {
				if (state.X) copy(state, xg);
				prng.state = function() {
					return copy(xg, {});
				};
			}
			return prng;
		}
		if (module$1 && module$1.exports) module$1.exports = impl;
		else if (define$1 && define$1.amd) define$1(function() {
			return impl;
		});
		else this.xor4096 = impl;
	})(exports, typeof module == "object" && module, typeof define == "function" && define);
}) });

//#endregion
//#region node_modules/seedrandom/lib/tychei.js
var require_tychei = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/lib/tychei.js": ((exports, module) => {
	(function(global$1, module$1, define$1) {
		function XorGen(seed) {
			var me = this, strseed = "";
			me.next = function() {
				var b = me.b, c = me.c, d = me.d, a = me.a;
				b = b << 25 ^ b >>> 7 ^ c;
				c = c - d | 0;
				d = d << 24 ^ d >>> 8 ^ a;
				a = a - b | 0;
				me.b = b = b << 20 ^ b >>> 12 ^ c;
				me.c = c = c - d | 0;
				me.d = d << 16 ^ c >>> 16 ^ a;
				return me.a = a - b | 0;
			};
			me.a = 0;
			me.b = 0;
			me.c = -1640531527;
			me.d = 1367130551;
			if (seed === Math.floor(seed)) {
				me.a = seed / 4294967296 | 0;
				me.b = seed | 0;
			} else strseed += seed;
			for (var k = 0; k < strseed.length + 20; k++) {
				me.b ^= strseed.charCodeAt(k) | 0;
				me.next();
			}
		}
		function copy(f, t) {
			t.a = f.a;
			t.b = f.b;
			t.c = f.c;
			t.d = f.d;
			return t;
		}
		function impl(seed, opts) {
			var xg = new XorGen(seed), state = opts && opts.state, prng = function() {
				return (xg.next() >>> 0) / 4294967296;
			};
			prng.double = function() {
				do
					var top = xg.next() >>> 11, bot = (xg.next() >>> 0) / 4294967296, result = (top + bot) / (1 << 21);
				while (result === 0);
				return result;
			};
			prng.int32 = xg.next;
			prng.quick = prng;
			if (state) {
				if (typeof state == "object") copy(state, xg);
				prng.state = function() {
					return copy(xg, {});
				};
			}
			return prng;
		}
		if (module$1 && module$1.exports) module$1.exports = impl;
		else if (define$1 && define$1.amd) define$1(function() {
			return impl;
		});
		else this.tychei = impl;
	})(exports, typeof module == "object" && module, typeof define == "function" && define);
}) });

//#endregion
//#region browser-external:crypto
var require_browser_external_crypto = /* @__PURE__ */ __commonJS({ "browser-external:crypto": ((exports, module) => {
	module.exports = Object.create(new Proxy({}, { get(_, key) {
		if (key !== "__esModule" && key !== "__proto__" && key !== "constructor" && key !== "splice") console.warn(`Module "crypto" has been externalized for browser compatibility. Cannot access "crypto.${key}" in client code. See http://vite.dev/guide/troubleshooting.html#module-externalized-for-browser-compatibility for more details.`);
	} }));
}) });

//#endregion
//#region node_modules/seedrandom/seedrandom.js
var require_seedrandom$1 = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/seedrandom.js": ((exports, module) => {
	(function(pool$1, math) {
		var global$1 = this, width = 256, chunks = 6, digits = 52, rngname = "random", startdenom = math.pow(width, chunks), significance = math.pow(2, digits), overflow = significance * 2, mask = width - 1, nodecrypto;
		function seedrandom(seed, options, callback) {
			var key = [];
			options = options == true ? { entropy: true } : options || {};
			var shortseed = mixkey(flatten$1(options.entropy ? [seed, tostring(pool$1)] : seed == null ? autoseed() : seed, 3), key);
			var arc4 = new ARC4(key);
			var prng = function() {
				var n = arc4.g(chunks), d = startdenom, x = 0;
				while (n < significance) {
					n = (n + x) * width;
					d *= width;
					x = arc4.g(1);
				}
				while (n >= overflow) {
					n /= 2;
					d /= 2;
					x >>>= 1;
				}
				return (n + x) / d;
			};
			prng.int32 = function() {
				return arc4.g(4) | 0;
			};
			prng.quick = function() {
				return arc4.g(4) / 4294967296;
			};
			prng.double = prng;
			mixkey(tostring(arc4.S), pool$1);
			return (options.pass || callback || function(prng$1, seed$1, is_math_call, state) {
				if (state) {
					if (state.S) copy(state, arc4);
					prng$1.state = function() {
						return copy(arc4, {});
					};
				}
				if (is_math_call) {
					math[rngname] = prng$1;
					return seed$1;
				} else return prng$1;
			})(prng, shortseed, "global" in options ? options.global : this == math, options.state);
		}
		math["seed" + rngname] = seedrandom;
		function ARC4(key) {
			var t, keylen = key.length, me = this, i = 0, j = me.i = me.j = 0, s = me.S = [];
			if (!keylen) key = [keylen++];
			while (i < width) s[i] = i++;
			for (i = 0; i < width; i++) {
				s[i] = s[j = mask & j + key[i % keylen] + (t = s[i])];
				s[j] = t;
			}
			(me.g = function(count) {
				var t$1, r = 0, i$1 = me.i, j$1 = me.j, s$1 = me.S;
				while (count--) {
					t$1 = s$1[i$1 = mask & i$1 + 1];
					r = r * width + s$1[mask & (s$1[i$1] = s$1[j$1 = mask & j$1 + t$1]) + (s$1[j$1] = t$1)];
				}
				me.i = i$1;
				me.j = j$1;
				return r;
			})(width);
		}
		function copy(f, t) {
			t.i = f.i;
			t.j = f.j;
			t.S = f.S.slice();
			return t;
		}
		function flatten$1(obj, depth) {
			var result = [], typ = typeof obj, prop;
			if (depth && typ == "object") for (prop in obj) try {
				result.push(flatten$1(obj[prop], depth - 1));
			} catch (e) {}
			return result.length ? result : typ == "string" ? obj : obj + "\0";
		}
		function mixkey(seed, key) {
			var stringseed = seed + "", smear, j = 0;
			while (j < stringseed.length) key[mask & j] = mask & (smear ^= key[mask & j] * 19) + stringseed.charCodeAt(j++);
			return tostring(key);
		}
		function autoseed() {
			try {
				var out;
				if (nodecrypto && (out = nodecrypto.randomBytes)) out = out(width);
				else {
					out = new Uint8Array(width);
					(global$1.crypto || global$1.msCrypto).getRandomValues(out);
				}
				return tostring(out);
			} catch (e) {
				var browser = global$1.navigator, plugins = browser && browser.plugins;
				return [
					+/* @__PURE__ */ new Date(),
					global$1,
					plugins,
					global$1.screen,
					tostring(pool$1)
				];
			}
		}
		function tostring(a) {
			return String.fromCharCode.apply(0, a);
		}
		mixkey(math.random(), pool$1);
		if (typeof module == "object" && module.exports) {
			module.exports = seedrandom;
			try {
				nodecrypto = require_browser_external_crypto();
			} catch (ex) {}
		} else if (typeof define == "function" && define.amd) define(function() {
			return seedrandom;
		});
	})([], Math);
}) });

//#endregion
//#region node_modules/seedrandom/index.js
var require_seedrandom = /* @__PURE__ */ __commonJS({ "node_modules/seedrandom/index.js": ((exports, module) => {
	var alea = require_alea();
	var xor128 = require_xor128();
	var xorwow = require_xorwow();
	var xorshift7 = require_xorshift7();
	var xor4096 = require_xor4096();
	var tychei = require_tychei();
	var sr = require_seedrandom$1();
	sr.alea = alea;
	sr.xor128 = xor128;
	sr.xorwow = xorwow;
	sr.xorshift7 = xorshift7;
	sr.xor4096 = xor4096;
	sr.tychei = tychei;
	module.exports = sr;
}) });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/rand_util.js
var import_seedrandom = /* @__PURE__ */ __toESM(require_seedrandom());
var MPRandGauss = class {
	constructor(mean$1, stdDeviation, dtype, truncated, seed) {
		this.mean = mean$1;
		this.stdDev = stdDeviation;
		this.dtype = dtype;
		this.nextVal = NaN;
		this.truncated = truncated;
		if (this.truncated) {
			this.upper = this.mean + this.stdDev * 2;
			this.lower = this.mean - this.stdDev * 2;
		}
		const seedValue = seed ? seed : Math.random();
		this.random = import_seedrandom.alea(seedValue.toString());
	}
	/** Returns next sample from a Gaussian distribution. */
	nextValue() {
		if (!isNaN(this.nextVal)) {
			const value = this.nextVal;
			this.nextVal = NaN;
			return value;
		}
		let resultX, resultY;
		let isValid = false;
		while (!isValid) {
			let v1, v2, s;
			do {
				v1 = 2 * this.random() - 1;
				v2 = 2 * this.random() - 1;
				s = v1 * v1 + v2 * v2;
			} while (s >= 1 || s === 0);
			const mul$1 = Math.sqrt(-2 * Math.log(s) / s);
			resultX = this.mean + this.stdDev * v1 * mul$1;
			resultY = this.mean + this.stdDev * v2 * mul$1;
			if (!this.truncated || this.isValidTruncated(resultX)) isValid = true;
		}
		if (!this.truncated || this.isValidTruncated(resultY)) this.nextVal = this.convertValue(resultY);
		return this.convertValue(resultX);
	}
	/** Handles proper rounding for non-floating-point numbers. */
	convertValue(value) {
		if (this.dtype == null || this.dtype === "float32") return value;
		return Math.round(value);
	}
	/** Returns true if less than 2-standard-deviations from the mean. */
	isValidTruncated(value) {
		return value <= this.upper && value >= this.lower;
	}
};
var RandGamma = class {
	constructor(alpha, beta, dtype, seed) {
		this.alpha = alpha;
		this.beta = 1 / beta;
		this.dtype = dtype;
		const seedValue = seed ? seed : Math.random();
		this.randu = import_seedrandom.alea(seedValue.toString());
		this.randn = new MPRandGauss(0, 1, dtype, false, this.randu());
		if (alpha < 1) this.d = alpha + 2 / 3;
		else this.d = alpha - 1 / 3;
		this.c = 1 / Math.sqrt(9 * this.d);
	}
	/** Returns next sample from a gamma distribution. */
	nextValue() {
		let x2, v0, v1, x, u, v;
		while (true) {
			do {
				x = this.randn.nextValue();
				v = 1 + this.c * x;
			} while (v <= 0);
			v *= v * v;
			x2 = x * x;
			v0 = 1 - .331 * x2 * x2;
			v1 = .5 * x2 + this.d * (1 - v + Math.log(v));
			u = this.randu();
			if (u < v0 || Math.log(u) < v1) break;
		}
		v = 1 / this.beta * this.d * v;
		if (this.alpha < 1) v *= Math.pow(this.randu(), 1 / this.alpha);
		return this.convertValue(v);
	}
	/** Handles proper rounding for non-floating-point numbers. */
	convertValue(value) {
		if (this.dtype === "float32") return value;
		return Math.round(value);
	}
};
var UniformRandom = class {
	constructor(min$1 = 0, max$1 = 1, dtype, seed) {
		/** Handles proper rounding for non floating point numbers. */
		this.canReturnFloat = () => this.dtype == null || this.dtype === "float32";
		this.min = min$1;
		this.range = max$1 - min$1;
		this.dtype = dtype;
		if (seed == null) seed = Math.random();
		if (typeof seed === "number") seed = seed.toString();
		if (!this.canReturnFloat() && this.range <= 1) throw new Error(`The difference between ${min$1} - ${max$1} <= 1 and dtype is not float`);
		this.random = import_seedrandom.alea(seed);
	}
	convertValue(value) {
		if (this.canReturnFloat()) return value;
		return Math.round(value);
	}
	nextValue() {
		return this.convertValue(this.min + this.range * this.random());
	}
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/random_gamma.js
/**
* Creates a `tf.Tensor` with values sampled from a gamma distribution.
*
* ```js
* tf.randomGamma([2, 2], 1).print();
* ```
*
* @param shape An array of integers defining the output tensor shape.
* @param alpha The shape parameter of the gamma distribution.
* @param beta The inverse scale parameter of the gamma distribution. Defaults
*     to 1.
* @param dtype The data type of the output. Defaults to float32.
* @param seed The seed for the random number generator.
*
* @doc {heading: 'Tensors', subheading: 'Random'}
*/
function randomGamma_(shape, alpha, beta = 1, dtype = "float32", seed) {
	if (beta == null) beta = 1;
	if (dtype == null) dtype = "float32";
	if (dtype !== "float32" && dtype !== "int32") throw new Error(`Unsupported data type ${dtype}`);
	const rgamma = new RandGamma(alpha, beta, dtype, seed);
	const res = buffer(shape, dtype);
	for (let i = 0; i < res.values.length; i++) res.values[i] = rgamma.nextValue();
	return res.toTensor();
}
const randomGamma = op({ randomGamma_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/random_normal.js
/**
* Creates a `tf.Tensor` with values sampled from a normal distribution.
*
* ```js
* tf.randomNormal([2, 2]).print();
* ```
*
* @param shape An array of integers defining the output tensor shape.
* @param mean The mean of the normal distribution.
* @param stdDev The standard deviation of the normal distribution.
* @param dtype The data type of the output.
* @param seed The seed for the random number generator.
*
* @doc {heading: 'Tensors', subheading: 'Random'}
*/
function randomNormal_(shape, mean$1 = 0, stdDev = 1, dtype, seed) {
	if (dtype != null && dtype === "bool") throw new Error(`Unsupported data type ${dtype}`);
	const randGauss = new MPRandGauss(mean$1, stdDev, dtype, false, seed);
	const res = buffer(shape, dtype);
	for (let i = 0; i < res.values.length; i++) res.values[i] = randGauss.nextValue();
	return res.toTensor();
}
const randomNormal = op({ randomNormal_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/random_uniform.js
/**
* Creates a `tf.Tensor` with values sampled from a uniform distribution.
*
* The generated values follow a uniform distribution in the range [minval,
* maxval). The lower bound minval is included in the range, while the upper
* bound maxval is excluded.
*
* ```js
* tf.randomUniform([2, 2]).print();
* ```
*
* @param shape An array of integers defining the output tensor shape.
* @param minval The lower bound on the range of random values to generate.
*   Defaults to 0.
* @param maxval The upper bound on the range of random values to generate.
*   Defaults to 1.
* @param dtype The data type of the output tensor. Defaults to 'float32'.
*
* @doc {heading: 'Tensors', subheading: 'Random'}
*/
function randomUniform_(shape, minval = 0, maxval = 1, dtype = "float32", seed) {
	const res = buffer(shape, dtype);
	const random = new UniformRandom(minval, maxval, null, seed);
	for (let i = 0; i < res.values.length; i++) res.values[i] = random.nextValue();
	return res.toTensor();
}
const randomUniform = op({ randomUniform_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/range.js
/**
* Creates a new `tf.Tensor1D` filled with the numbers in the range provided.
*
* The tensor is a is half-open interval meaning it includes start, but
* excludes stop. Decrementing ranges and negative step values are also
* supported.sv
*
*
* ```js
* tf.range(0, 9, 2).print();
* ```
*
* @param start An integer start value
* @param stop An integer stop value
* @param step An integer increment (will default to 1 or -1)
* @param dtype The data type of the output tensor. Defaults to 'float32'.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function range(start, stop, step$1 = 1, dtype = "float32") {
	if (step$1 === 0) throw new Error("Cannot have a step of zero");
	const attrs = {
		start,
		stop,
		step: step$1,
		dtype
	};
	return ENGINE.runKernel(Range, {}, attrs);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/real.js
/**
* Returns the real part of a complex (or real) tensor.
*
* Given a tensor input, this operation returns a tensor of type float that is
* the real part of each element in input considered as a complex number.
*
* If the input is real, it simply makes a clone.
*
* ```js
* const x = tf.complex([-2.25, 3.25], [4.75, 5.75]);
* tf.real(x).print();
* ```
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function real_(input) {
	const inputs = { input: convertToTensor(input, "input", "real") };
	return ENGINE.runKernel(Real, inputs);
}
const real = op({ real_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reciprocal.js
/**
* Computes reciprocal of x element-wise: `1 / x`
*
* ```js
* const x = tf.tensor1d([0, 1, 2]);
*
* x.reciprocal().print();  // or tf.reciprocal(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function reciprocal_(x) {
	const inputs = { x: convertToTensor(x, "x", "reciprocal") };
	return ENGINE.runKernel(Reciprocal, inputs);
}
const reciprocal = op({ reciprocal_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/relu.js
/**
* Computes rectified linear element-wise: `max(x, 0)`.
*
* ```js
* const x = tf.tensor1d([-1, 2, -3, 4]);
*
* x.relu().print();  // or tf.relu(x)
* ```
* @param x The input tensor. If the dtype is `bool`, the output dtype will be
*     `int32'.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function relu_(x) {
	const inputs = { x: convertToTensor(x, "x", "relu") };
	return ENGINE.runKernel(Relu, inputs);
}
const relu = op({ relu_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/relu6.js
/**
* Computes rectified linear 6 element-wise: `min(max(x, 0), 6)`.
*
* ```js
* const x = tf.tensor1d([-1, 2, -3, 8]);
*
* x.relu6().print();  // or tf.relu6(x)
* ```
* @param x The input tensor. If the dtype is `bool`, the output dtype will be
*     `int32'.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function relu6_(x) {
	const inputs = { x: convertToTensor(x, "x", "relu6") };
	return ENGINE.runKernel(Relu6, inputs);
}
const relu6 = op({ relu6_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reverse.js
/**
* Reverses a `tf.Tensor` along a specified axis.
*
* Also available are stricter rank-specific methods that assert that `x` is
* of the given rank:
*   - `tf.reverse1d`
*   - `tf.reverse2d`
*   - `tf.reverse3d`
*   - `tf.reverse4d`
*
* Except `tf.reverse1d` (which does not have axis param), all methods have
* same signature as this method.
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
*
* x.reverse().print();
* ```
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* const axis = 1;
* x.reverse(axis).print();
* ```
* @param x The input tensor to be reversed.
* @param axis The set of dimensions to reverse. Must be in the
*     range [-rank(x), rank(x)). Defaults to all axes.
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function reverse_(x, axis) {
	const inputs = { x: convertToTensor(x, "x", "reverse") };
	const attrs = { dims: axis };
	return ENGINE.runKernel(Reverse, inputs, attrs);
}
const reverse = op({ reverse_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reverse_1d.js
/**
* Reverses a `tf.Tensor1D`.
*
* @param x The input tensor.
*/
function reverse1d_(x) {
	const $x = convertToTensor(x, "x", "reverse");
	assert($x.rank === 1, () => `Error in reverse1D: x must be rank 1 but got rank ${$x.rank}.`);
	return reverse($x, 0);
}
const reverse1d = op({ reverse1d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reverse_2d.js
/**
* Reverses a `tf.Tensor2D` along a specified axis.
*
* @param x The input tensor.
* @param axis The set of dimensions to reverse. Must be in the
*     range [-rank(x), rank(x)). Defaults to all axes.
*/
function reverse2d_(x, axis) {
	const $x = convertToTensor(x, "x", "reverse");
	assert($x.rank === 2, () => `Error in reverse2D: x must be rank 2 but got rank ${$x.rank}.`);
	return reverse($x, axis);
}
const reverse2d = op({ reverse2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reverse_3d.js
/**
* Reverses a `tf.Tensor3D` along a specified axis.
*
* @param x The input tensor.
* @param axis The set of dimensions to reverse. Must be in the
*     range [-rank(x), rank(x)). Defaults to all axes.
*/
function reverse3d_(x, axis) {
	const $x = convertToTensor(x, "x", "reverse");
	assert($x.rank === 3, () => `Error in reverse3D: x must be rank 3 but got rank ${$x.rank}.`);
	return reverse($x, axis);
}
const reverse3d = op({ reverse3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reverse_4d.js
/**
* Reverses a `tf.Tensor4D` along a specified axis.
*
* @param x The input tensor.
* @param axis The set of dimensions to reverse. Must be in the
*     range [-rank(x), rank(x)). Defaults to all axes.
*/
function reverse4d_(x, axis) {
	const $x = convertToTensor(x, "x", "reverse");
	assert($x.rank === 4, () => `Error in reverse4D: x must be rank 4 but got rank ${$x.rank}.`);
	return reverse($x, axis);
}
const reverse4d = op({ reverse4d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/round.js
/**
* Computes round of input `tf.Tensor` element-wise: `round(x)`.
* It implements banker's rounding.
*
* ```js
* const x = tf.tensor1d([.6, 1.1, -3.3]);
*
* x.round().print();  // or tf.round(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function round_(x) {
	const inputs = { x: convertToTensor(x, "x", "round") };
	return ENGINE.runKernel(Round, inputs);
}
const round = op({ round_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/rsqrt.js
/**
* Computes reciprocal of square root of the input `tf.Tensor` element-wise:
* `y = 1 / sqrt(x)`
*
* ```js
* const x = tf.tensor1d([1, 2, 4, -1]);
*
* x.rsqrt().print();  // or tf.rsqrt(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function rsqrt_(x) {
	const inputs = { x: convertToTensor(x, "x", "rsqrt", "float32") };
	return ENGINE.runKernel(Rsqrt, inputs);
}
const rsqrt = op({ rsqrt_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/scalar.js
/**
* Creates rank-0 `tf.Tensor` (scalar) with the provided value and dtype.
*
* The same functionality can be achieved with `tf.tensor`, but in general
* we recommend using `tf.scalar` as it makes the code more readable.
*
* ```js
* tf.scalar(3.14).print();
* ```
*
* @param value The value of the scalar.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function scalar(value, dtype) {
	if ((isTypedArray(value) && dtype !== "string" || Array.isArray(value)) && dtype !== "complex64") throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");
	if (dtype === "string" && isTypedArray(value) && !(value instanceof Uint8Array)) throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");
	return makeTensor(value, [], [], dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/selu.js
/**
* Computes scaled exponential linear element-wise.
*
* `x < 0 ? scale * alpha * (exp(x) - 1) : x`
*
* ```js
* const x = tf.tensor1d([-1, 2, -3, 4]);
*
* x.selu().print();  // or tf.selu(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function selu_(x) {
	const inputs = { x: convertToTensor(x, "x", "selu") };
	return ENGINE.runKernel(Selu, inputs);
}
const selu = op({ selu_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/separable_conv2d.js
/**
* 2-D convolution with separable filters.
*
* Performs a depthwise convolution that acts separately on channels followed
* by a pointwise convolution that mixes channels. Note that this is
* separability between dimensions [1, 2] and 3, not spatial separability
* between dimensions 1 and 2.
*
* See
* [https://www.tensorflow.org/api_docs/python/tf/nn/separable_conv2d](
*     https://www.tensorflow.org/api_docs/python/tf/nn/separable_conv2d)
* for more details.
*
* @param x The input tensor, of rank 4 or rank 3, of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
* assumed.
* @param depthwiseFilter The depthwise filter tensor, rank 4, of shape
*     `[filterHeight, filterWidth, inChannels, channelMultiplier]`. This is
*     the filter used in the first step.
* @param pointwiseFilter The pointwise filter tensor, rank 4, of shape
*     `[1, 1, inChannels * channelMultiplier, outChannels]`. This is
*     the filter used in the second step.
* @param strides The strides of the convolution: `[strideHeight,
* strideWidth]`. If strides is a single number, then `strideHeight ==
* strideWidth`.
* @param pad The type of padding algorithm.
*   - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*   - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*   - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single
*     number, then `dilationHeight == dilationWidth`. If it is greater than
*     1, then all values of `strides` must be 1.
* @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
*     "NHWC". Specify the data format of the input and output data. With the
*     default format "NHWC", the data is stored in the order of: [batch,
*     height, width, channels]. Only "NHWC" is currently supported.
*
* @doc {heading: 'Operations', subheading: 'Convolution'}
*/
function separableConv2d_(x, depthwiseFilter, pointwiseFilter, strides, pad$1, dilation = [1, 1], dataFormat = "NHWC") {
	const $x = convertToTensor(x, "x", "separableConv2d");
	const $depthwiseFilter = convertToTensor(depthwiseFilter, "depthwiseFilter", "separableConv2d");
	const $pointwiseFilter = convertToTensor(pointwiseFilter, "pointwiseFilter", "separableConv2d");
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	if (dataFormat === "NCHW") throw new Error("separableConv2d currently does not support dataFormat NCHW; only NHWC is supported");
	assert(x4D.rank === 4, () => `Error in separableConv2d: input must be rank 4, but got rank ${x4D.rank}.`);
	assert($depthwiseFilter.rank === 4, () => `Error in separableConv2d: depthwise filter must be rank 4, but got rank ${$depthwiseFilter.rank}.`);
	assert($pointwiseFilter.rank === 4, () => `Error in separableConv2d: pointwise filter must be rank 4, but got rank ${$depthwiseFilter.rank}.`);
	assert($pointwiseFilter.shape[0] === 1, () => `Error in separableConv2d: the first dimension of pointwise filter  must be 1, but got ${$pointwiseFilter.shape[0]}.`);
	assert($pointwiseFilter.shape[1] === 1, () => `Error in separableConv2d: the second dimension of pointwise filter must be 1, but got ${$pointwiseFilter.shape[1]}.`);
	const inChannels = $depthwiseFilter.shape[2];
	const channelMultiplier = $depthwiseFilter.shape[3];
	assert($pointwiseFilter.shape[2] === inChannels * channelMultiplier, () => `Error in separableConv2d: the third dimension of pointwise filter must be ${inChannels * channelMultiplier}, but got ${$pointwiseFilter.shape[2]}.`);
	const depthwise = depthwiseConv2d(x4D, $depthwiseFilter, strides, pad$1, dataFormat, dilation);
	const res = conv2d(depthwise, $pointwiseFilter, 1, "valid", dataFormat);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const separableConv2d = op({ separableConv2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/setdiff1d_async.js
/**
* Computes the difference between two lists of numbers.
*
* Given a Tensor `x` and a Tensor `y`, this operation returns a Tensor `out`
* that represents all values that are in `x` but not in `y`. The returned
* Tensor `out` is sorted in the same order that the numbers appear in `x`
* (duplicates are preserved). This operation also returns a Tensor indices that
* represents the position of each out element in `x`. In other words:
*
* `out[i] = x[idx[i]] for i in [0, 1, ..., out.length - 1]`
*
* ```js
* const x = [1, 2, 3, 4, 5, 6];
* const y = [1, 3, 5];
*
* const [out, indices] = await tf.setdiff1dAsync(x, y);
* out.print(); // [2, 4, 6]
* indices.print(); // [1, 3, 5]
* ```
*
* @param x 1-D Tensor. Values to keep.
* @param y 1-D Tensor. Must have the same type as x. Values to exclude in the
*     output.
* @returns Promise of Tensor tuple [out, indices].
*  out: Tensor with the same type as x.
*  indices: A Tensor of type int32.
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
async function setdiff1dAsync_(x, y) {
	const $x = convertToTensor(x, "x", "setdiff1d");
	const $y = convertToTensor(y, "y", "setdiff1d");
	assert($x.dtype === $y.dtype, () => `x and y should have the same dtype, but got x (${$x.dtype}) and y (${$y.dtype}).`);
	assert($x.rank === 1, () => `x should be 1D tensor, but got x (${$x.shape}).`);
	assert($y.rank === 1, () => `y should be 1D tensor, but got y (${$y.shape}).`);
	const xVals = await $x.data();
	const yVals = await $y.data();
	const ySet = new Set(yVals);
	let outputSize = 0;
	for (let i = 0; i < xVals.length; i++) if (!ySet.has(xVals[i])) outputSize++;
	const buffer$1 = new TensorBuffer([outputSize], $x.dtype);
	const indices = new TensorBuffer([outputSize], "int32");
	for (let i = 0, p = 0; i < xVals.length; i++) if (!ySet.has(xVals[i])) {
		buffer$1.values[p] = xVals[i];
		indices.values[p] = i;
		p++;
	}
	return [buffer$1.toTensor(), indices.toTensor()];
}
const setdiff1dAsync = setdiff1dAsync_;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sign.js
/**
* Returns an element-wise indication of the sign of a number.
*
* ```js
* const x = tf.tensor1d([.6, 1.1, -3.3, NaN, 0]);
*
* x.sign().print();  // or tf.sign(x)
* ```
* @param x The input Tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function sign_(x) {
	const inputs = { x: convertToTensor(x, "x", "sign") };
	return ENGINE.runKernel(Sign, inputs);
}
const sign = op({ sign_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sin.js
/**
* Computes sin of the input Tensor element-wise: `sin(x)`
*
* ```js
* const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);
*
* x.sin().print();  // or tf.sin(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function sin_(x) {
	const inputs = { x: convertToTensor(x, "x", "sin", "float32") };
	return ENGINE.runKernel(Sin, inputs);
}
const sin = op({ sin_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sinh.js
/**
* Computes hyperbolic sin of the input `tf.Tensor` element-wise: `sinh(x)`
*
* ```js
* const x = tf.tensor1d([0, 1, -1, .7]);
*
* x.sinh().print();  // or tf.sinh(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function sinh_(x) {
	const inputs = { x: convertToTensor(x, "x", "sinh") };
	return ENGINE.runKernel(Sinh, inputs);
}
const sinh = op({ sinh_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/slice1d.js
/**
* Extracts a 1D slice from 1D array starting at coordinates `begin` and is
* of length `size`. See `slice` for details.
*/
function slice1d_(x, begin, size) {
	const $x = convertToTensor(x, "x", "slice1d");
	assert($x.rank === 1, () => `slice1d expects a rank-1 tensor, but got a rank-${$x.rank} tensor`);
	return slice($x, [begin], [size]);
}
const slice1d = op({ slice1d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/slice2d.js
/**
* Extracts a 2D slice from a 2D array starting at coordinates `begin` and
* is of size `size`. See `slice` for details.
*/
function slice2d_(x, begin, size) {
	const $x = convertToTensor(x, "x", "slice2d");
	assert($x.rank === 2, () => `slice2d expects a rank-2 tensor, but got a rank-${$x.rank} tensor`);
	return slice($x, begin, size);
}
const slice2d = op({ slice2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/slice3d.js
/**
* Extracts a 3D slice from a 3D array starting at coordinates `begin` and
* is of size `size`. See `slice` for details.
*/
function slice3d_(x, begin, size) {
	const $x = convertToTensor(x, "x", "slice3d");
	assert($x.rank === 3, () => `slice3d expects a rank-3 tensor, but got a rank-${$x.rank} tensor`);
	return slice($x, begin, size);
}
const slice3d = op({ slice3d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/slice4d.js
/**
* Extracts a 4D slice from a 4D array starting at coordinates `begin` and
* is of size `size`. See `slice` for details.
*/
function slice4d_(x, begin, size) {
	const $x = convertToTensor(x, "x", "slice4d");
	assert($x.rank === 4, () => `slice4d expects a rank-4 tensor, but got a rank-${$x.rank} tensor`);
	return slice($x, begin, size);
}
const slice4d = op({ slice4d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/softmax.js
/**
* Computes the softmax normalized vector given the logits.
*
* ```js
* const a = tf.tensor1d([1, 2, 3]);
*
* a.softmax().print();  // or tf.softmax(a)
* ```
*
* ```js
* const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);
*
* a.softmax().print();  // or tf.softmax(a)
* ```
*
* @param logits The logits array.
* @param dim The dimension softmax would be performed on. Defaults to `-1`
*     which indicates the last dimension.
*
* @doc {heading: 'Operations', subheading: 'Normalization'}
*/
function softmax_(logits, dim = -1) {
	const $logits = convertToTensor(logits, "logits", "softmax", "float32");
	if (dim === -1) dim = $logits.rank - 1;
	if (dim !== $logits.rank - 1) throw Error(`Softmax along a non-last dimension is not yet supported. Logits was rank ${$logits.rank} and dim was ${dim}`);
	const inputs = { logits: $logits };
	const attrs = { dim };
	return ENGINE.runKernel(Softmax, inputs, attrs);
}
const softmax = op({ softmax_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/spectral/fft.js
/**
* Fast Fourier transform.
*
* Computes the 1-dimensional discrete Fourier transform over the inner-most
* dimension of input.
*
* ```js
* const real = tf.tensor1d([1, 2, 3]);
* const imag = tf.tensor1d([1, 2, 3]);
* const x = tf.complex(real, imag);
*
* x.fft().print();  // tf.spectral.fft(x).print();
* ```
* @param input The complex input to compute an fft over.
*
* @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
*/
function fft_(input) {
	assert(input.dtype === "complex64", () => `The dtype for tf.spectral.fft() must be complex64 but got ${input.dtype}.`);
	const inputs = { input };
	return ENGINE.runKernel(FFT, inputs);
}
const fft = op({ fft_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/spectral/ifft.js
/**
* Inverse fast Fourier transform.
*
* Computes the inverse 1-dimensional discrete Fourier transform over the
* inner-most dimension of input.
*
* ```js
* const real = tf.tensor1d([1, 2, 3]);
* const imag = tf.tensor1d([1, 2, 3]);
* const x = tf.complex(real, imag);
*
* x.ifft().print();  // tf.spectral.ifft(x).print();
* ```
* @param input The complex input to compute an ifft over.
*
* @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
*/
function ifft_(input) {
	assert(input.dtype === "complex64", () => `The dtype for tf.spectral.ifft() must be complex64 but got ${input.dtype}.`);
	const inputs = { input };
	return ENGINE.runKernel(IFFT, inputs);
}
const ifft = op({ ifft_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/spectral/irfft.js
/**
* Inversed real value input fast Fourier transform.
*
* Computes the 1-dimensional inversed discrete Fourier transform over the
* inner-most dimension of the real input.
*
* ```js
* const real = tf.tensor1d([1, 2, 3]);
* const imag = tf.tensor1d([0, 0, 0]);
* const x = tf.complex(real, imag);
*
* x.irfft().print();
* ```
* @param input The real value input to compute an irfft over.
*
* @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
*/
function irfft_(input) {
	const innerDimensionSize = input.shape[input.shape.length - 1];
	const batch = input.size / innerDimensionSize;
	let ret;
	if (innerDimensionSize <= 2) {
		const complexInput = reshape(input, [batch, innerDimensionSize]);
		ret = ifft(complexInput);
	} else {
		const outputShape = [batch, 2 * (innerDimensionSize - 1)];
		const realInput = reshape(real(input), [batch, innerDimensionSize]);
		const imagInput = reshape(imag(input), [batch, innerDimensionSize]);
		const realConjugate = reverse(slice(realInput, [0, 1], [batch, innerDimensionSize - 2]), 1);
		const imagConjugate = mul(reverse(slice(imagInput, [0, 1], [batch, innerDimensionSize - 2]), 1), scalar(-1));
		const r = concat([realInput, realConjugate], 1);
		const i = concat([imagInput, imagConjugate], 1);
		const complexInput = reshape(complex(r, i), [outputShape[0], outputShape[1]]);
		ret = ifft(complexInput);
	}
	ret = real(ret);
	if (input.rank === 3 && input.shape[0] !== 0) {
		const temp = ret;
		const batch$1 = input.shape[0];
		ret = reshape(ret, [
			batch$1,
			ret.shape[0] / batch$1,
			ret.shape[1]
		]);
		temp.dispose();
	}
	return ret;
}
const irfft = op({ irfft_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/split.js
/**
* Splits a `tf.Tensor` into sub tensors.
*
* If `numOrSizeSplits` is a number, splits `x` along dimension `axis`
* into `numOrSizeSplits` smaller tensors.
* Requires that `numOrSizeSplits` evenly divides `x.shape[axis]`.
*
* If `numOrSizeSplits` is a number array, splits `x` into
* `numOrSizeSplits.length` pieces. The shape of the `i`-th piece has the
* same size as `x` except along dimension `axis` where the size is
* `numOrSizeSplits[i]`.
*
* ```js
* const x = tf.tensor2d([1, 2, 3, 4, 5, 6, 7, 8], [2, 4]);
* const [a, b] = tf.split(x, 2, 1);
* a.print();
* b.print();
*
* const [c, d, e] = tf.split(x, [1, 2, 1], 1);
* c.print();
* d.print();
* e.print();
* ```
*
* @param x The input tensor to split.
* @param numOrSizeSplits Either an integer indicating the number of
* splits along the axis or an array of integers containing the sizes of
* each output tensor along the axis. If a number then it must evenly divide
* `x.shape[axis]`; otherwise the sum of sizes must match `x.shape[axis]`.
* Can contain one -1 indicating that dimension is to be inferred.
* @param axis The dimension along which to split. Defaults to 0 (the first
* dim).
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function split_(x, numOrSizeSplits, axis = 0) {
	const inputs = { x: convertToTensor(x, "x", "split") };
	const attr = {
		numOrSizeSplits,
		axis
	};
	return ENGINE.runKernel(SplitV, inputs, attr);
}
const split = op({ split_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/spectral/rfft.js
/**
* Real value input fast Fourier transform.
*
* Computes the 1-dimensional discrete Fourier transform over the
* inner-most dimension of the real input.
*
* ```js
* const real = tf.tensor1d([1, 2, 3]);
*
* real.rfft().print();
* ```
* @param input The real value input to compute an rfft over.
*
* @doc {heading: 'Operations', subheading: 'Spectral', namespace: 'spectral'}
*/
function rfft_(input, fftLength) {
	assert(input.dtype === "float32", () => `The dtype for rfft() must be real value but got ${input.dtype}`);
	let innerDimensionSize = input.shape[input.shape.length - 1];
	const batch = input.size / innerDimensionSize;
	let adjustedInput;
	if (fftLength != null && fftLength < innerDimensionSize) {
		const begin = input.shape.map((v) => 0);
		const size = input.shape.map((v) => v);
		size[input.shape.length - 1] = fftLength;
		adjustedInput = slice(input, begin, size);
		innerDimensionSize = fftLength;
	} else if (fftLength != null && fftLength > innerDimensionSize) {
		const zerosShape = input.shape.map((v) => v);
		zerosShape[input.shape.length - 1] = fftLength - innerDimensionSize;
		adjustedInput = concat([input, zeros(zerosShape)], input.shape.length - 1);
		innerDimensionSize = fftLength;
	} else adjustedInput = input;
	const zerosInput = zerosLike(adjustedInput);
	const complexInput = reshape(complex(adjustedInput, zerosInput), [batch, innerDimensionSize]);
	const ret = fft(complexInput);
	const half = Math.floor(innerDimensionSize / 2) + 1;
	const realValues = real(ret);
	const imagValues = imag(ret);
	const realComplexConjugate = split(realValues, [half, innerDimensionSize - half], realValues.shape.length - 1);
	const imagComplexConjugate = split(imagValues, [half, innerDimensionSize - half], imagValues.shape.length - 1);
	const outputShape = adjustedInput.shape.slice();
	outputShape[adjustedInput.shape.length - 1] = half;
	return reshape(complex(realComplexConjugate[0], imagComplexConjugate[0]), outputShape);
}
const rfft = op({ rfft_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sqrt.js
/**
* Computes square root of the input `tf.Tensor` element-wise: `y = sqrt(x)`
*
* ```js
* const x = tf.tensor1d([1, 2, 4, -1]);
*
* x.sqrt().print();  // or tf.sqrt(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function sqrt_(x) {
	const inputs = { x: convertToTensor(x, "x", "sqrt", "float32") };
	return ENGINE.runKernel(Sqrt, inputs);
}
const sqrt = op({ sqrt_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/squared_difference.js
/**
* Returns (a - b) * (a - b) element-wise.
* Supports broadcasting.
*
* ```js
* const a = tf.tensor1d([1, 4, 3, 16]);
* const b = tf.tensor1d([1, 2, 9, 4]);
*
* a.squaredDifference(b).print();  // or tf.squaredDifference(a, b)
* ```
*
* ```js
* // Broadcast squared difference  a with b.
* const a = tf.tensor1d([2, 4, 6, 8]);
* const b = tf.scalar(5);
*
* a.squaredDifference(b).print();  // or tf.squaredDifference(a, b)
* ```
*
* @param a The first tensor.
* @param b The second tensor. Must have the same type as `a`.
*
* @doc {heading: 'Operations', subheading: 'Arithmetic'}
*/
function squaredDifference_(a, b) {
	let $a = convertToTensor(a, "a", "squaredDifference");
	let $b = convertToTensor(b, "b", "squaredDifference");
	[$a, $b] = makeTypesMatch($a, $b);
	assertAndGetBroadcastShape($a.shape, $b.shape);
	const inputs = {
		a: $a,
		b: $b
	};
	return ENGINE.runKernel(SquaredDifference, inputs, {});
}
const squaredDifference = op({ squaredDifference_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/squeeze.js
/**
* Removes dimensions of size 1 from the shape of a `tf.Tensor`.
*
* ```js
* const x = tf.tensor([1, 2, 3, 4], [1, 1, 4]);
* x.squeeze().print();
* ```
*
* @param x The input tensor to be squeezed.
* @param axis An optional list of numbers. If specified, only
*     squeezes the dimensions listed. The dimension index starts at 0. It
* is an error to squeeze a dimension that is not 1.
*
* @doc {heading: 'Tensors', subheading: 'Transformations'}
*/
function squeeze_(x, axis) {
	const $x = convertToTensor(x, "x", "squeeze");
	return reshape($x, squeezeShape($x.shape, axis).newShape);
}
const squeeze = op({ squeeze_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/stack.js
/**
* Stacks a list of rank-`R` `tf.Tensor`s into one rank-`(R+1)` `tf.Tensor`.
*
* ```js
* const a = tf.tensor1d([1, 2]);
* const b = tf.tensor1d([3, 4]);
* const c = tf.tensor1d([5, 6]);
* tf.stack([a, b, c]).print();
* ```
*
* @param tensors A list of tensor objects with the same shape and dtype.
* @param axis The axis to stack along. Defaults to 0 (the first dim).
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function stack_(tensors, axis = 0) {
	const $tensors = convertToTensorArray(tensors, "tensors", "stack", "string_or_numeric");
	assert($tensors.length >= 1, () => "Pass at least one tensor to tf.stack");
	if ($tensors.length > 0) assert(axis <= $tensors[0].rank, () => "Axis must be <= rank of the tensor");
	const inputs = $tensors;
	const attrs = { axis };
	return ENGINE.runKernel(Pack, inputs, attrs);
}
const stack = op({ stack_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/step.js
/**
* Computes step of the input `tf.Tensor` element-wise: `x > 0 ? 1 : alpha * x`
*
* ```js
* const x = tf.tensor1d([0, 2, -1, -3]);
*
* x.step(.5).print();  // or tf.step(x, .5)
* ```
* @param x The input tensor.
* @param alpha The gradient when input is negative.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function step_(x, alpha = 0) {
	const inputs = { x: convertToTensor(x, "x", "step") };
	const attrs = { alpha };
	return ENGINE.runKernel(Step, inputs, attrs);
}
const step = op({ step_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/strided_slice.js
/**
* Extracts a strided slice of a tensor.
*
* Roughly speaking, this op extracts a slice of size (end-begin)/stride from
* the given input tensor (x). Starting at the location specified by begin the
* slice continues by adding stride to the index until all dimensions are not
* less than end. Note that a stride can be negative, which causes a reverse
* slice.
*
* ```js
* const t = tf.tensor3d([1, 1, 1 ,2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],
*    [3, 2, 3]);
* t.stridedSlice([1, 0, 0], [2, 1, 3], [1, 1, 1]).print()  // [[[3, 3, 3]]]
* t.stridedSlice([1, 0, 0], [2, 2, 3], [1, 1, 1]).print()  // [[[3, 3, 3],
*                                                     // [4, 4, 4]]]
* t.stridedSlice([1, -1, 0], [2, -3, 3], [1, -1, 1]).print() // [[[4, 4, 4],
*                                                     // [3, 3, 3]]]
* ```
*
* @param x The tensor to stride slice.
* @param begin The coordinates to start the slice from.
* @param end: The coordinates to end the slice at.
* @param strides: The size of the slice.
* @param beginMask: If the ith bit of beginMask is set, begin[i] is ignored
*      and the fullest possible range in that dimension is used instead.
* @param endMask: If the ith bit of endMask is set, end[i] is ignored
*      and the fullest possible range in that dimension is used instead.
* @param shrinkAxisMask: a bitmask where bit i implies that
* the ith specification should shrink the dimensionality. begin and end must
* imply a slice of size 1 in the dimension.
*
* @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
*/
function stridedSlice_(x, begin, end, strides, beginMask = 0, endMask = 0, ellipsisMask = 0, newAxisMask = 0, shrinkAxisMask = 0) {
	const inputs = { x: convertToTensor(x, "x", "stridedSlice", "string_or_numeric") };
	const attrs = {
		begin,
		end,
		strides,
		beginMask,
		endMask,
		ellipsisMask,
		newAxisMask,
		shrinkAxisMask
	};
	return ENGINE.runKernel(StridedSlice, inputs, attrs);
}
const stridedSlice = op({ stridedSlice_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tan.js
/**
* Computes tan of the input `tf.Tensor` element-wise, `tan(x)`
*
* ```js
* const x = tf.tensor1d([0, Math.PI / 2, Math.PI * 3 / 4]);
*
* x.tan().print();  // or tf.tan(x)
* ```
* @param x The input tensor.
*
* @doc {heading: 'Operations', subheading: 'Basic math'}
*/
function tan_(x) {
	const inputs = { x: convertToTensor(x, "x", "tan", "float32") };
	return ENGINE.runKernel(Tan, inputs);
}
const tan = op({ tan_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor1d.js
/**
* Creates rank-1 `tf.Tensor` with the provided values, shape and dtype.
*
* The same functionality can be achieved with `tf.tensor`, but in general
* we recommend using `tf.tensor1d` as it makes the code more readable.
*
* ```js
* tf.tensor1d([1, 2, 3]).print();
* ```
*
* @param values The values of the tensor. Can be array of numbers,
*     or a `TypedArray`.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function tensor1d(values, dtype) {
	assertNonNull(values);
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 1) throw new Error("tensor1d() requires values to be a flat/TypedArray");
	return makeTensor(values, null, inferredShape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor2d.js
/**
* Creates rank-2 `tf.Tensor` with the provided values, shape and dtype.
*
* The same functionality can be achieved with `tf.tensor`, but in general
* we recommend using `tf.tensor2d` as it makes the code more readable.
*
*  ```js
* // Pass a nested array.
* tf.tensor2d([[1, 2], [3, 4]]).print();
* ```
* ```js
* // Pass a flat array and specify a shape.
* tf.tensor2d([1, 2, 3, 4], [2, 2]).print();
* ```
*
* @param values The values of the tensor. Can be nested array of numbers,
*     or a flat array, or a `TypedArray`.
* @param shape The shape of the tensor. If not provided, it is inferred from
*     `values`.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function tensor2d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 2) throw new Error("tensor2d() requires shape to have two numbers");
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 2 && inferredShape.length !== 1) throw new Error("tensor2d() requires values to be number[][] or flat/TypedArray");
	if (inferredShape.length === 1 && shape == null) throw new Error("tensor2d() requires shape to be provided when `values` are a flat/TypedArray");
	return makeTensor(values, shape, inferredShape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor4d.js
/**
* Creates rank-4 `tf.Tensor` with the provided values, shape and dtype.
*
* The same functionality can be achieved with `tf.tensor`, but in general
* we recommend using `tf.tensor4d` as it makes the code more readable.
*
*  ```js
* // Pass a nested array.
* tf.tensor4d([[[[1], [2]], [[3], [4]]]]).print();
* ```
* ```js
* // Pass a flat array and specify a shape.
* tf.tensor4d([1, 2, 3, 4], [1, 2, 2, 1]).print();
* ```
*
* @param values The values of the tensor. Can be nested array of numbers,
*     or a flat array, or a `TypedArray`.
* @param shape The shape of the tensor. Optional. If not provided,
*   it is inferred from `values`.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function tensor4d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 4) throw new Error("tensor4d() requires shape to have four numbers");
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 4 && inferredShape.length !== 1) throw new Error("tensor4d() requires values to be number[][][][] or flat/TypedArray");
	if (inferredShape.length === 1 && shape == null) throw new Error("tensor4d() requires shape to be provided when `values` are a flat array");
	return makeTensor(values, shape, inferredShape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor5d.js
/**
* Creates rank-5 `tf.Tensor` with the provided values, shape and dtype.
*
* The same functionality can be achieved with `tf.tensor`, but in general
* we recommend using `tf.tensor5d` as it makes the code more readable.
*
*  ```js
* // Pass a nested array.
* tf.tensor5d([[[[[1],[2]],[[3],[4]]],[[[5],[6]],[[7],[8]]]]]).print();
* ```
* ```js
* // Pass a flat array and specify a shape.
* tf.tensor5d([1, 2, 3, 4, 5, 6, 7, 8], [1, 2, 2, 2, 1]).print();
* ```
*
* @param values The values of the tensor. Can be nested array of numbers,
*     or a flat array, or a `TypedArray`.
* @param shape The shape of the tensor. Optional. If not provided,
*   it is inferred from `values`.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function tensor5d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 5) throw new Error("tensor5d() requires shape to have five numbers");
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 5 && inferredShape.length !== 1) throw new Error("tensor5d() requires values to be number[][][][][] or flat/TypedArray");
	if (inferredShape.length === 1 && shape == null) throw new Error("tensor5d() requires shape to be provided when `values` are a flat array");
	return makeTensor(values, shape, inferredShape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/tensor6d.js
/**
* Creates rank-6 `tf.Tensor` with the provided values, shape and dtype.
*
* The same functionality can be achieved with `tf.tensor`, but in general
* we recommend using `tf.tensor6d` as it makes the code more readable.
*
*  ```js
* // Pass a nested array.
* tf.tensor6d([[[[[[1],[2]],[[3],[4]]],[[[5],[6]],[[7],[8]]]]]]).print();
* ```
* ```js
* // Pass a flat array and specify a shape.
* tf.tensor6d([1, 2, 3, 4, 5, 6, 7, 8], [1, 1, 2, 2, 2, 1]).print();
* ```
*
* @param values The values of the tensor. Can be nested array of numbers,
*     or a flat array, or a `TypedArray`.
* @param shape The shape of the tensor. Optional. If not provided,
*   it is inferred from `values`.
* @param dtype The data type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function tensor6d(values, shape, dtype) {
	assertNonNull(values);
	if (shape != null && shape.length !== 6) throw new Error("tensor6d() requires shape to have six numbers");
	const inferredShape = inferShape(values, dtype);
	if (inferredShape.length !== 6 && inferredShape.length !== 1) throw new Error("tensor6d() requires values to be number[][][][][][] or flat/TypedArray");
	if (inferredShape.length === 1 && shape == null) throw new Error("tensor6d() requires shape to be provided when `values` are a flat array");
	shape = shape || inferredShape;
	return makeTensor(values, shape, inferredShape, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/topk.js
/**
* Finds the values and indices of the `k` largest entries along the last
* dimension.
*
* If the input is a vector (rank=1), finds the k largest entries in the vector
* and outputs their values and indices as vectors. Thus values[j] is the j-th
* largest entry in input, and its index is indices[j].
* For higher rank inputs, computes the top k entries along the last dimension.
*
* If two elements are equal, the lower-index element appears first.
*
* ```js
* const a = tf.tensor2d([[1, 5], [4, 3]]);
* const {values, indices} = tf.topk(a);
* values.print();
* indices.print();
* ```
* @param x 1-D or higher `tf.Tensor` with last dimension being at least `k`.
* @param k Number of top elements to look for along the last dimension.
* @param sorted If true, the resulting `k` elements will be sorted by the
*     values in descending order.
*
* @doc {heading: 'Operations', subheading: 'Evaluation'}
*/
function topk_(x, k = 1, sorted = true) {
	const $x = convertToTensor(x, "x", "topk");
	if ($x.rank === 0) throw new Error("topk() expects the input to be of rank 1 or higher");
	const lastDim = $x.shape[$x.shape.length - 1];
	if (k < 0) throw new Error(`'k' passed to topk() must be >= 0 but got ${k}`);
	if (k > lastDim) throw new Error(`'k' passed to topk() must be <= the last dimension (${lastDim}) but got ${k}`);
	const inputs = { x: $x };
	const attrs = {
		k,
		sorted
	};
	const [values, indices] = ENGINE.runKernel(TopK, inputs, attrs);
	return {
		values,
		indices
	};
}
const topk = op({ topk_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/truncated_normal.js
/**
* Creates a `tf.Tensor` with values sampled from a truncated normal
* distribution.
*
* ```js
* tf.truncatedNormal([2, 2]).print();
* ```
*
* The generated values follow a normal distribution with specified mean and
* standard deviation, except that values whose magnitude is more than 2
* standard deviations from the mean are dropped and re-picked.
*
* @param shape An array of integers defining the output tensor shape.
* @param mean The mean of the normal distribution.
* @param stdDev The standard deviation of the normal distribution.
* @param dtype The data type of the output tensor.
* @param seed The seed for the random number generator.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function truncatedNormal_(shape, mean$1 = 0, stdDev = 1, dtype, seed) {
	if (dtype != null && dtype === "bool") throw new Error(`Unsupported data type $ { dtype }`);
	const randGauss = new MPRandGauss(mean$1, stdDev, dtype, true, seed);
	const res = buffer(shape, dtype);
	for (let i = 0; i < res.values.length; i++) res.values[i] = randGauss.nextValue();
	return res.toTensor();
}
const truncatedNormal = op({ truncatedNormal_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/unique.js
/**
* Finds unique elements along an axis of a tensor.
*
* It returns a tensor `values` containing all of the unique elements along the
* `axis` of the given tensor `x` in the same order that they occur along the
* `axis` in `x`; `x` does not need to be sorted. It also returns a tensor
* `indices` the same size as the number of the elements in `x` along the `axis`
* dimension. It contains the index in the unique output `values`.
*
* ```js
* // A 1-D tensor
* const a = tf.tensor1d([1, 1, 2, 4, 4, 4, 7, 8, 8]);
* const {values, indices} = tf.unique(a);
* values.print();   // [1, 2, 4, 7, 8,]
* indices.print();  // [0, 0, 1, 2, 2, 2, 3, 4, 4]
* ```
*
* ```js
* // A 2-D tensor with axis=0
* //
* // 'a' is: [[1, 0, 0],
* //          [1, 0, 0],
* //          [2, 0, 0]]
* const a = tf.tensor2d([[1, 0, 0], [1, 0, 0], [2, 0, 0]]);
* const {values, indices} = tf.unique(a, 0)
* values.print();   // [[1, 0, 0],
*                   //  [2, 0, 0]]
* indices.print();  // [0, 0, 1]
* ```
*
* ```js
* // A 2-D tensor with axis=1
* //
* // 'a' is: [[1, 0, 0],
* //          [1, 0, 0],
* //          [2, 0, 0]]
* const a = tf.tensor2d([[1, 0, 0], [1, 0, 0], [2, 0, 0]]);
* const {values, indices} = tf.unique(a, 1)
* values.print();   // [[1, 0],
*                   //  [1, 0],
*                   //  [2, 0]]
* indices.print();  // [0, 1, 1]
* ```
* @param x A tensor (int32, string, bool).
* @param axis The axis of the tensor to find the unique elements.
* @returns [uniqueElements, indices] (see above for details)
*
* @doc {heading: 'Operations', subheading: 'Evaluation'}
*/
function unique_(x, axis = 0) {
	const $x = convertToTensor(x, "x", "unique", "string_or_numeric");
	assert($x.rank > 0, () => "The input tensor must be at least 1D");
	const inputs = { x: $x };
	const attrs = { axis };
	const [values, indices] = ENGINE.runKernel(Unique, inputs, attrs);
	return {
		values,
		indices
	};
}
const unique = op({ unique_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/unsorted_segment_sum.js
/**
* Computes the sum along segments of a `tf.Tensor`.
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
* const segmentIds = tf.tensor1d([1, 2, 0, 1], 'int32');
* const numSegments = 3;
*
* x.unsortedSegmentSum(segmentIds, numSegments).print()
* //or tf.unsortedSegmentSum(x, segmentIds, numSegments)
* ```
* @param x The `tf.Tensor` that will be summed along its segments.
* @param segmentIds A `tf.Tensor1D` whose rank is equal to the rank of `x`'s
* dimension along the `axis`.  Maps each element of `x` to a segment.
* @param numSegments The number of distinct `segmentIds`.
*
* @doc {heading: 'Operations', subheading: 'Segment'}
*/
function unsortedSegmentSum_(x, segmentIds, numSegments) {
	const $x = convertToTensor(x, "x", "unsortedSegmentSum");
	const $segmentIds = convertToTensor(segmentIds, "segmentIds", "unsortedSegmentSum", "int32");
	assert(isInt(numSegments), () => "numSegments must be of dtype int");
	const inputs = {
		x: $x,
		segmentIds: $segmentIds
	};
	const attrs = { numSegments };
	return ENGINE.runKernel(UnsortedSegmentSum, inputs, attrs);
}
const unsortedSegmentSum = op({ unsortedSegmentSum_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/unstack.js
/**
* Unstacks a `tf.Tensor` of rank-`R` into a list of rank-`(R-1)` `tf.Tensor`s.
*
* ```js
* const a = tf.tensor2d([1, 2, 3, 4], [2, 2]);
*
* tf.unstack(a).forEach(tensor => tensor.print());
* ```
*
* @param x A tensor object.
* @param axis The axis to unstack along. Defaults to 0 (the first dim).
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
function unstack_(x, axis = 0) {
	const $x = convertToTensor(x, "x", "unstack", "string_or_numeric");
	assert(axis >= -$x.shape.length && axis < $x.shape.length, () => `Axis = ${axis} is not in [-${$x.shape.length}, ${$x.shape.length})`);
	const inputs = { value: $x };
	const attrs = { axis };
	return ENGINE.runKernel(Unpack, inputs, attrs);
}
const unstack = op({ unstack_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/variable.js
/**
* Creates a new variable with the provided initial value.
* ```js
* const x = tf.variable(tf.tensor([1, 2, 3]));
* x.assign(tf.tensor([4, 5, 6]));
*
* x.print();
* ```
*
* @param initialValue Initial value for the tensor.
* @param trainable If true, optimizers are allowed to update it.
* @param name Name of the variable. Defaults to a unique id.
* @param dtype If set, initialValue will be converted to the given type.
*
* @doc {heading: 'Tensors', subheading: 'Creation'}
*/
function variable(initialValue, trainable = true, name, dtype) {
	return ENGINE.makeVariable(initialValue, trainable, name, dtype);
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/backends/where_impl.js
function whereImpl(condShape, condVals) {
	const indices = [];
	for (let i = 0; i < condVals.length; i++) if (condVals[i]) indices.push(i);
	const inBuffer = buffer(condShape, "int32");
	const out = buffer([indices.length, condShape.length], "int32");
	for (let i = 0; i < indices.length; i++) {
		const loc = inBuffer.indexToLoc(indices[i]);
		const offset = i * condShape.length;
		out.values.set(loc, offset);
	}
	return out.toTensor();
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/where_async.js
/**
* Returns the coordinates of true elements of condition.
*
* The coordinates are returned in a 2-D tensor where the first dimension (rows)
* represents the number of true elements, and the second dimension (columns)
* represents the coordinates of the true elements. Keep in mind, the shape of
* the output tensor can vary depending on how many true values there are in
* input. Indices are output in row-major order. The resulting tensor has the
* shape `[numTrueElems, condition.rank]`.
*
* This is analogous to calling the python `tf.where(cond)` without an x or y.
*
* ```js
* const cond = tf.tensor1d([false, false, true], 'bool');
* const result = await tf.whereAsync(cond);
* result.print();
* ```
*
* @doc {heading: 'Operations', subheading: 'Logical'}
*/
async function whereAsync_(condition) {
	const $condition = convertToTensor(condition, "condition", "whereAsync", "bool");
	const vals = await $condition.data();
	const res = whereImpl($condition.shape, vals);
	if (condition !== $condition) $condition.dispose();
	return res;
}
const whereAsync = whereAsync_;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/boolean_mask.js
/**
* Apply boolean mask to tensor.
*
* ```js
* const tensor = tf.tensor2d([1, 2, 3, 4, 5, 6], [3, 2]);
* const mask = tf.tensor1d([1, 0, 1], 'bool');
* const result = await tf.booleanMaskAsync(tensor, mask);
* result.print();
* ```
*
* @param tensor N-D tensor.
* @param mask K-D boolean tensor, K <= N and K must be known statically.
* @param axis A 0-D int Tensor representing the axis in tensor to mask from.
*     By default, axis is 0 which will mask from the first dimension.
*     Otherwise K + axis <= N.
*
* @doc {heading: 'Tensors', subheading: 'Slicing and Joining'}
*/
async function booleanMaskAsync_(tensor$1, mask, axis) {
	const $tensor = convertToTensor(tensor$1, "tensor", "boolMask");
	const $mask = convertToTensor(mask, "mask", "boolMask", "bool");
	const axisFrom = axis == null ? 0 : axis;
	const maskDim = $mask.rank;
	const tensorShape = $tensor.shape;
	assert(maskDim > 0, () => "mask cannot be scalar");
	assertShapesMatch(tensorShape.slice(axisFrom, axisFrom + maskDim), $mask.shape, `mask's shape must match the first K dimensions of tensor's shape,`);
	let leadingSize = 1;
	for (let i = axisFrom; i < axisFrom + maskDim; i++) leadingSize *= tensorShape[i];
	const targetTensorShape = tensorShape.slice(0, axisFrom).concat([leadingSize], tensorShape.slice(axisFrom + maskDim));
	const reshapedTensor = reshape($tensor, targetTensorShape);
	const reshapedMask = reshape($mask, [-1]);
	const positivePositions = await whereAsync(reshapedMask);
	const indices = squeeze(positivePositions, [1]);
	const res = gather(reshapedTensor, indices, axisFrom);
	if (tensor$1 !== $tensor) $tensor.dispose();
	if (mask !== $mask) $mask.dispose();
	indices.dispose();
	reshapedTensor.dispose();
	reshapedMask.dispose();
	positivePositions.dispose();
	return res;
}
const booleanMaskAsync = booleanMaskAsync_;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/norm.js
/**
* Computes the norm of scalar, vectors, and matrices.
* This function can compute several different vector norms (the 1-norm, the
* Euclidean or 2-norm, the inf-norm, and in general the p-norm for p > 0)
* and matrix norms (Frobenius, 1-norm, and inf-norm).
*
* ```js
* const x = tf.tensor1d([1, 2, 3, 4]);
*
* x.norm().print();  // or tf.norm(x)
* ```
*
* @param x The input array.
* @param ord Optional. Order of the norm. Supported norm types are
* following:
*
*  | ord        | norm for matrices         | norm for vectors
*  |------------|---------------------------|---------------------
*  |'euclidean' |Frobenius norm             |2-norm
*  |'fro'       |Frobenius norm	           |
*  |Infinity    |max(sum(abs(x), axis=1))   |max(abs(x))
*  |-Infinity   |min(sum(abs(x), axis=1))   |min(abs(x))
*  |1           |max(sum(abs(x), axis=0))   |sum(abs(x))
*  |2           |                           |sum(abs(x)^2)^1/2*
*
* @param axis Optional. If axis is null (the default), the input is
* considered a vector and a single vector norm is computed over the entire
* set of values in the Tensor, i.e. norm(x, ord) is equivalent
* to norm(x.reshape([-1]), ord). If axis is a integer, the input
* is considered a batch of vectors, and axis determines the axis in x
* over which to compute vector norms. If axis is a 2-tuple of integer it is
* considered a batch of matrices and axis determines the axes in NDArray
* over which to compute a matrix norm.
* @param keepDims Optional. If true, the norm have the same dimensionality
* as the input.
*
* @doc {heading: 'Operations', subheading: 'Matrices'}
*/
function norm_(x, ord = "euclidean", axis = null, keepDims = false) {
	x = convertToTensor(x, "x", "norm");
	const norm$1 = normImpl(x, ord, axis);
	let keepDimsShape = norm$1.shape;
	if (keepDims) {
		const axes = parseAxisParam(axis, x.shape);
		keepDimsShape = expandShapeToKeepDim(norm$1.shape, axes);
	}
	return reshape(norm$1, keepDimsShape);
}
function normImpl(x, p, axis = null) {
	if (x.rank === 0) return abs(x);
	if (x.rank !== 1 && axis === null) return normImpl(reshape(x, [-1]), p, axis);
	if (x.rank === 1 || typeof axis === "number" || Array.isArray(axis) && axis.length === 1) {
		if (p === 1) return sum(abs(x), axis);
		if (p === Infinity) return max(abs(x), axis);
		if (p === -Infinity) return min(abs(x), axis);
		if (p === "euclidean" || p === 2) return sqrt(sum(pow(abs(x), scalar(2, "int32")), axis));
		throw new Error(`Error in norm: invalid ord value: ${p}`);
	}
	if (Array.isArray(axis) && axis.length === 2) {
		if (p === 1) return max(sum(abs(x), axis[0]), axis[1] - 1);
		if (p === Infinity) return max(sum(abs(x), axis[1]), axis[0]);
		if (p === -Infinity) return min(sum(abs(x), axis[1]), axis[0]);
		if (p === "fro" || p === "euclidean") return sqrt(sum(square(x), axis));
		throw new Error(`Error in norm: invalid ord value: ${p}`);
	}
	throw new Error(`Error in norm: invalid axis: ${axis}`);
}
const norm = op({ norm_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/moving_average.js
/**
* Compute the moving average of a variable.
*
* Without zeroDebias, the moving average operation is defined by:
*   `v += delta`
* where
*   `delta = (1 - decay) * (x - v)`
*
* With zeroDebias (default), the `delta` term is scaled to debias the
* effect of the (assumed) zero-initialization of `v`.
*   `delta /= (1 - decay ^ step)`
*
* For more details on the zero-debiasing algorithm, see:
*   https://arxiv.org/abs/1412.6980
*
* Note that this function is completely stateless and does not keep track of
* step count. The step count needs to be maintained by the caller and passed
* in as `step`.
*
* @param v The current moving average value.
* @param x New input value, must have the same shape and dtype as `v`.
* @param decay The decay factor. Typical values are 0.95 and 0.99.
* @param step Step count.
* @param zeroDebias: Whether zeroDebias is to be performed (default: `true`).
* @returns The new moving average value.
*
* @doc {heading: 'Operations', subheading: 'Moving Average'}
*/
function movingAverage_(v, x, decay, step$1, zeroDebias = true) {
	const $v = convertToTensor(v, "v", "movingAverage");
	const $x = convertToTensor(x, "x", "movingAverage");
	const $decay = convertToTensor(decay, "decay", "movingAverage");
	assertTypesMatch($v, $x);
	assert(arraysEqual($v.shape, $x.shape), () => "Shape mismatch in v and x");
	const one = scalar(1);
	const oneMinusDecay = sub(one, $decay);
	let update = mul(sub($x, $v), oneMinusDecay);
	if (zeroDebias) {
		assert(step$1 != null, () => "When using zeroDebias: true, step is required.");
		const $step = convertToTensor(step$1, "step", "movingAverage");
		update = div(update, sub(one, pow($decay, $step)));
	}
	return add($v, update);
}
const movingAverage = op({ movingAverage_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/scatter_nd.js
/**
* Creates a new tensor by applying sparse updates to individual
* values or slices within a zero tensor of the given shape tensor according to
* indices. This operator is the inverse of the `tf.gatherND` operator which
* extracts values or slices from a given tensor.
*
* ```js
* const indices = tf.tensor2d([4, 3, 1, 7], [4, 1], 'int32');
* const updates = tf.tensor1d([9, 10, 11, 12]);
* const shape = [8];
* tf.scatterND(indices, updates, shape).print() //[0, 11, 0, 10, 9, 0, 0, 12]
* ```
*
* @param indices The tensor contains the indices into the output tensor.
* @param updates The tensor contains the value for the indices.
* @param shape: The shape of the output tensor.
*
* @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
*/
function scatterND_(indices, updates, shape) {
	const $indices = convertToTensor(indices, "indices", "scatterND", "int32");
	const $updates = convertToTensor(updates, "updates", "scatterND");
	validateInput$1($updates, $indices, shape);
	const inputs = {
		indices: $indices,
		updates: $updates
	};
	const attrs = { shape };
	return ENGINE.runKernel(ScatterNd, inputs, attrs);
}
const scatterND = op({ scatterND_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sparse_to_dense_util.js
/**
* Validate sparseToDense inputs.
*
* @param sparseIndices A 0-D, 1-D, or 2-D Tensor of type int32.
* sparseIndices[i] contains the complete index where sparseValues[i] will be
* placed.
* @param sparseValues A 0-D or 1-D Tensor. Values
* corresponding to each row of sparseIndices, or a scalar value to be used for
* all sparse indices.
* @param outputShape number[]. Shape of the dense output tensor.
* @param validateIndices boolean. indice validation is not supported, error
* will be thrown if it is set.
*/
function validateInput(sparseIndices, sparseValues, outputShape, defaultValues) {
	if (sparseIndices.dtype !== "int32") throw new Error(`tf.sparseToDense() expects the indices to be int32 type, but the dtype was ${sparseIndices.dtype}.`);
	if (sparseIndices.rank > 2) throw new Error(`sparseIndices should be a scalar, vector, or matrix, but got shape ${sparseIndices.shape}.`);
	const numElems = sparseIndices.rank > 0 ? sparseIndices.shape[0] : 1;
	const numDims = sparseIndices.rank > 1 ? sparseIndices.shape[1] : 1;
	if (outputShape.length !== numDims) throw new Error(`outputShape has incorrect number of elements:, ${outputShape.length}, should be: ${numDims}.`);
	const numValues = sparseValues.size;
	if (!(sparseValues.rank === 0 || sparseValues.rank === 1 && numValues === numElems)) throw new Error(`sparseValues has incorrect shape ${sparseValues.shape}, should be [] or [${numElems}]`);
	if (sparseValues.dtype !== defaultValues.dtype) throw new Error("sparseValues.dtype must match defaultValues.dtype");
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sparse_to_dense.js
/**
* Converts a sparse representation into a dense tensor.
*
* Builds an array dense with shape outputShape such that:
*
* // If sparseIndices is scalar
* dense[i] = (i == sparseIndices ? sparseValues : defaultValue)
*
* // If sparseIndices is a vector, then for each i
* dense[sparseIndices[i]] = sparseValues[i]
*
* // If sparseIndices is an n by d matrix, then for each i in [0, n)
* dense[sparseIndices[i][0], ..., sparseIndices[i][d-1]] = sparseValues[i]
* All other values in dense are set to defaultValue. If sparseValues is a
* scalar, all sparse indices are set to this single value.
*
* If indices are repeated the final value is summed over all values for those
* indices.
*
* ```js
* const indices = tf.tensor1d([4, 5, 6, 1, 2, 3], 'int32');
* const values = tf.tensor1d([10, 11, 12, 13, 14, 15], 'float32');
* const shape = [8];
* tf.sparseToDense(indices, values, shape).print();
* ```
*
* @param sparseIndices A 0-D, 1-D, or 2-D Tensor of type int32.
* sparseIndices[i] contains the complete index where sparseValues[i] will be
* placed.
* @param sparseValues A 0-D or 1-D Tensor. Values
* corresponding to each row of sparseIndices, or a scalar value to be used for
* all sparse indices.
* @param outputShape Shape of the dense output tensor. the type is inferred.
* @param defaultValue Scalar. Value to set for indices not specified in
* sparseIndices. Defaults to zero.
*
* @doc {heading: 'Operations', subheading: 'Normalization'}
*/
function sparseToDense_(sparseIndices, sparseValues, outputShape, defaultValue = 0) {
	const $sparseIndices = convertToTensor(sparseIndices, "sparseIndices", "sparseToDense", "int32");
	const $sparseValues = convertToTensor(sparseValues, "sparseValues", "sparseToDense");
	const $defaultValue = convertToTensor(defaultValue, "defaultValue", "sparseToDense", $sparseValues.dtype);
	validateInput($sparseIndices, $sparseValues, outputShape, $defaultValue);
	const inputs = {
		sparseIndices: $sparseIndices,
		sparseValues: $sparseValues,
		defaultValue: $defaultValue
	};
	const attrs = { outputShape };
	return ENGINE.runKernel(SparseToDense, inputs, attrs);
}
const sparseToDense = op({ sparseToDense_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/gather_nd.js
/**
* Gather slices from input tensor into a Tensor with shape specified by
* `indices`.
*
* `indices` is an K-dimensional integer tensor, best thought of as a
* (K-1)-dimensional tensor of indices into input, where each element defines a
* slice of input:
* output[\\(i_0, ..., i_{K-2}\\)] = input[indices[\\(i_0, ..., i_{K-2}\\)]]
*
* Whereas in `tf.gather`, `indices` defines slices into the first dimension of
* input, in `tf.gatherND`, `indices` defines slices into the first N dimensions
* of input, where N = indices.shape[-1].
*
* The last dimension of indices can be at most the rank of input:
* indices.shape[-1] <= input.rank
*
* The last dimension of `indices` corresponds to elements
* (if indices.shape[-1] == input.rank) or slices
* (if indices.shape[-1] < input.rank) along dimension indices.shape[-1] of
* input.
* The output tensor has shape
* indices.shape[:-1] + input.shape[indices.shape[-1]:]
*
* Note that on CPU, if an out of bound index is found, an error is returned. On
* GPU, if an out of bound index is found, a 0 is stored in the corresponding
* output value.
*
* ```js
* const indices = tf.tensor2d([0, 1, 1, 0], [2,2], 'int32');
* const input = tf.tensor2d([9, 10, 11, 12], [2, 2]);
* tf.gatherND(input, indices).print() // [10, 11]
* ```
*
* @param x The tensor from which to gather values.
* @param indices Index tensor, must be of type int32.
*
* @doc {heading: 'Operations', subheading: 'Slicing and Joining'}
*/
function gatherND_(x, indices) {
	const $indices = convertToTensor(indices, "indices", "gatherND", "int32");
	const inputs = {
		params: convertToTensor(x, "x", "gatherND", "string_or_numeric"),
		indices: $indices
	};
	return ENGINE.runKernel(GatherNd, inputs);
}
const gatherND = op({ gatherND_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/dropout_util.js
/**
* Normalize noise shape based on provided tensor and noise shape.
*
* @param x Tensor.
* @param noiseShape The shape for the randomly generated keep/drop flags, as
*   an array of numbers. Optional.
* @returns Normalized noise shape.
*/
function getNoiseShape(x, noiseShape) {
	if (noiseShape == null) return x.shape.slice();
	if (arraysEqual(x.shape, noiseShape)) return noiseShape;
	if (x.shape.length === noiseShape.length) {
		const newDimension = [];
		for (let i = 0; i < x.shape.length; i++) if (noiseShape[i] == null && x.shape[i] != null) newDimension.push(x.shape[i]);
		else newDimension.push(noiseShape[i]);
		return newDimension;
	}
	return noiseShape;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/dropout.js
/**
* Computes dropout.
*
* ```js
* const x = tf.tensor1d([1, 2, 2, 1]);
* const rate = 0.75;
* const output = tf.dropout(x, rate);
* output.print();
* ```
*
* @param x A floating point Tensor or TensorLike.
* @param rate A float in the range [0, 1). The probability that each element
*   of x is discarded.
* @param noiseShape An array of numbers of type int32, representing the
* shape for randomly generated keep/drop flags. If the noiseShape has null
* value, it will be automatically replaced with the x's relative dimension
* size. Optional.
* @param seed Used to create random seeds. Optional.
* @returns A Tensor of the same shape of x.
*
* @doc {heading: 'Operations', subheading: 'Dropout'}
*/
function dropout_(x, rate, noiseShape, seed) {
	const $x = convertToTensor(x, "x", "dropout");
	assert($x.dtype === "float32", () => `x has to be a floating point tensor since it's going to be scaled, but got a ${$x.dtype} tensor instead.`);
	assert(rate >= 0 && rate < 1, () => `rate must be a float in the range [0, 1), but got ${rate}.`);
	if (rate === 0) return x instanceof Tensor ? $x.clone() : $x;
	const $noiseShape = getNoiseShape($x, noiseShape);
	const keepProb = 1 - rate;
	const multiplier = div(floor(add(randomUniform($noiseShape, 0, 1, "float32", seed), keepProb)), keepProb);
	return mul($x, multiplier);
}
const dropout = op({ dropout_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/signal_ops_util.js
function enclosingPowerOfTwo(value) {
	return Math.floor(Math.pow(2, Math.ceil(Math.log(value) / Math.log(2))));
}
function cosineWindow(windowLength, a, b) {
	const even = 1 - windowLength % 2;
	const newValues = new Float32Array(windowLength);
	for (let i = 0; i < windowLength; ++i) {
		const cosArg = 2 * Math.PI * i / (windowLength + even - 1);
		newValues[i] = a - b * Math.cos(cosArg);
	}
	return tensor1d(newValues, "float32");
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/in_top_k.js
/**
* Returns whether the targets are in the top K predictions.
*
* ```js
* const predictions = tf.tensor2d([[20, 10, 40, 30], [30, 50, -20, 10]]);
* const targets = tf.tensor1d([2, 0]);
* const precision = await tf.inTopKAsync(predictions, targets);
* precision.print();
* ```
* @param predictions 2-D or higher `tf.Tensor` with last dimension being
*     at least `k`.
* @param targets 1-D or higher `tf.Tensor`.
* @param k Optional Number of top elements to look at for computing precision,
*     default to 1.
*
* @doc {heading: 'Operations', subheading: 'Evaluation'}
*/
async function inTopKAsync_(predictions, targets, k = 1) {
	const $predictions = convertToTensor(predictions, "predictions", "inTopK");
	const $targets = convertToTensor(targets, "targets", "inTopK");
	assert($predictions.rank > 1, () => `inTopK() expects the predictions to be of rank 2 or higher, but got ${$predictions.rank}`);
	assert($predictions.rank - 1 === $targets.rank, () => `predictions rank should be 1 larger than targets rank, but got predictions rank ${$predictions.rank} and targets rank ${$targets.rank}`);
	assertShapesMatch($predictions.shape.slice(0, $predictions.shape.length - 1), $targets.shape, "predictions's shape should be align with the targets' shape, except the last dimension.");
	const lastDim = $predictions.shape[$predictions.shape.length - 1];
	assert(k > 0 && k <= lastDim, () => `'k' passed to inTopK() must be > 0 && <= the predictions last dimension (${lastDim}), but got ${k}`);
	const predictionsVals = await $predictions.data();
	const targetsVals = await $targets.data();
	const [batch, size] = [predictionsVals.length / lastDim, lastDim];
	const precision = getTypedArrayFromDType("bool", batch);
	for (let b = 0; b < batch; b++) {
		const offset = b * size;
		const vals = predictionsVals.subarray(offset, offset + size);
		const valAndInd = [];
		for (let i = 0; i < vals.length; i++) valAndInd.push({
			value: vals[i],
			index: i
		});
		valAndInd.sort((a, b$1) => b$1.value - a.value);
		precision[b] = 0;
		for (let i = 0; i < k; i++) if (valAndInd[i].index === targetsVals[b]) {
			precision[b] = 1;
			break;
		}
	}
	if (predictions !== $predictions) $predictions.dispose();
	if (targets !== $targets) $targets.dispose();
	return tensor(precision, $targets.shape, "bool");
}
const inTopKAsync = inTopKAsync_;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/conv2d_backprop_filter.js
/**
* Computes the derivative of the filter of a 2D convolution.
*
* @param x The input tensor, of rank 4 or rank 3 of shape
*     [batch, height, width, inChannels]. If rank 3, batch of 1 is assumed.
* @param dy The dy image, of rank 4 or rank 3, of shape
*     [batch, height, width, outDepth]. If rank 3, batch of 1 is assumed.
* @param filterShape The shape of the filter, length 4,
*     [filterHeight, filterWidth, inDepth, outDepth].
* @param strides The strides of the convolution: [strideHeight,
* strideWidth].
* @param pad A string from: 'same', 'valid'. The type of padding algorithm
*     used in the forward prop of the op.
* @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
*     "NHWC". Specify the data format of the input and output data. With the
*     default format "NHWC", the data is stored in the order of: [batch,
*     height, width, channels].
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
*/
function conv2DBackpropFilter_(x, dy, filterShape, strides, pad$1, dataFormat = "NHWC", dimRoundingMode) {
	let x4D = x;
	if (x.rank === 3) x4D = reshape(x, [
		1,
		x.shape[0],
		x.shape[1],
		x.shape[2]
	]);
	let dy4D = dy;
	if (dy4D.rank === 3) dy4D = reshape(dy, [
		1,
		dy.shape[0],
		dy.shape[1],
		dy.shape[2]
	]);
	assert(x4D.rank === 4, () => `Error in conv2dDerFilter: input must be rank 4, but got shape ${x4D.shape}.`);
	assert(dy4D.rank === 4, () => `Error in conv2dDerFilter: dy must be rank 4, but got shape ${dy4D.shape}.`);
	assert(filterShape.length === 4, () => `Error in conv2dDerFilter: filterShape must be length 4, but got ${filterShape}.`);
	const inDepth = dataFormat === "NHWC" ? x4D.shape[3] : x4D.shape[1];
	const outDepth = dataFormat === "NHWC" ? dy4D.shape[3] : dy4D.shape[1];
	assert(inDepth === filterShape[2], () => `Error in conv2dDerFilter: depth of input ${inDepth}) must match input depth in filter (${filterShape[2]}.`);
	assert(outDepth === filterShape[3], () => `Error in conv2dDerFilter: depth of dy (${outDepth}) must match output depth for filter (${filterShape[3]}).`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const inputs = {
		x: x4D,
		dy: dy4D
	};
	const attrs = {
		strides,
		pad: pad$1,
		dataFormat,
		dimRoundingMode,
		filterShape
	};
	return ENGINE.runKernel(Conv2DBackpropFilter, inputs, attrs);
}
const conv2DBackpropFilter = op({ conv2DBackpropFilter_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/fused_util.js
function getFusedDyActivation(dy, y, activation) {
	if (activation == null || activation === "linear") return dy;
	if (activation === "relu") return mul(dy, step(y));
	throw new Error(`Cannot compute gradient for fused activation ${activation}.`);
}
function getFusedBiasGradient(bias, dyActivation) {
	let res = dyActivation;
	const reduceAxes = getReductionAxes(bias.shape, dyActivation.shape);
	if (reduceAxes.length > 0) res = sum(res, reduceAxes);
	return reshape(res, bias.shape);
}
function applyActivation(x, activation, preluActivationWeights, leakyreluAlpha) {
	if (activation === "linear") return x;
	else if (activation === "relu") return relu(x);
	else if (activation === "elu") return elu(x);
	else if (activation === "relu6") return relu6(x);
	else if (activation === "prelu") return prelu(x, preluActivationWeights);
	else if (activation === "leakyrelu") return leakyRelu(x, leakyreluAlpha);
	else if (activation === "sigmoid") return sigmoid(x);
	throw new Error(`Unknown fused activation ${activation}.`);
}
const shouldFuse = (gradientDepth, activation) => {
	return !(gradientDepth > 0) || activation === "linear";
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/fused/conv2d.js
/**
* Computes a 2D convolution over the input x, optionally fused with adding a
* bias and applying an activation.
*
* ```js
* const inputDepth = 2;
* const inShape = [2, 2, 2, inputDepth];
* const outputDepth = 2;
* const fSize = 1;
* const pad = 0;
* const strides = 1;
*
* const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,
* 16], inShape);
* const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,
* outputDepth]);
*
* tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',
* dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();
* ```
*
* @param obj An object with the following properties:
* @param x The input tensor, of rank 4 or rank 3, of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
* assumed.
* @param filter The filter, rank 4, of shape
*     `[filterHeight, filterWidth, inDepth, outDepth]`.
* @param strides The strides of the convolution: `[strideHeight,
* strideWidth]`.
* @param pad The type of padding algorithm.
*   - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*   - `valid` output will be smaller than input if filter is larger
*       than 1x1.
*   - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dataFormat An optional string from: "NHWC", "NCHW". Defaults to
*     "NHWC". Specify the data format of the input and output data. With the
*     default format "NHWC", the data is stored in the order of: [batch,
*     height, width, channels]. Only "NHWC" is currently supported.
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single
*     number, then `dilationHeight == dilationWidth`. If it is greater than
*     1, then all values of `strides` must be 1.
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
* @param bias Tensor to be added to the result.
* @param activation Name of activation kernel (defaults to `linear`) to be
*     applied
*      after biasAdd.
* @param preluActivationWeights Tensor of prelu weights to be applied as part
*     of a `prelu` activation, typically the same shape as `x`.
* @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`
*     activation.
*/
function fusedConv2d_({ x, filter, strides, pad: pad$1, dataFormat = "NHWC", dilations = [1, 1], dimRoundingMode, bias, activation = "linear", preluActivationWeights, leakyreluAlpha }) {
	activation = activation || "linear";
	if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {
		let result = conv2d(x, filter, strides, pad$1, dataFormat, dilations, dimRoundingMode);
		if (bias != null) result = add(result, bias);
		return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);
	}
	const $x = convertToTensor(x, "x", "conv2d", "float32");
	const $filter = convertToTensor(filter, "filter", "conv2d", "float32");
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ${$filter.rank}.`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in fused conv2d: pad must be an integer when using, dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match input depth for filter ${$filter.shape[2]}.`);
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => `Error in conv2D: Either strides or dilations must be 1. Got strides ${strides} and dilations '${dilations}'`);
	assert(dataFormat === "NHWC", () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);
	const convInfo = computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad$1, dimRoundingMode);
	let $bias;
	if (bias != null) {
		$bias = convertToTensor(bias, "bias", "fused conv2d");
		[$bias] = makeTypesMatch($bias, $x);
		assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);
	}
	let $preluActivationWeights;
	if (preluActivationWeights != null) $preluActivationWeights = convertToTensor(preluActivationWeights, "prelu weights", "fused conv2d");
	const grad$1 = (dy, saved) => {
		const [$filter$1, x4D$1, y, $bias$1] = saved;
		const dyActivation = getFusedDyActivation(dy, y, activation);
		assert(tupleValuesAreOne(dilations), () => `Error in gradient of fused conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${dilations}'`);
		const xDer = conv2DBackpropInput(x4D$1.shape, dyActivation, $filter$1, strides, pad$1);
		const filterDer = conv2DBackpropFilter(x4D$1, dyActivation, $filter$1.shape, strides, pad$1);
		const der = [xDer, filterDer];
		if ($bias$1 != null) {
			const biasDer = getFusedBiasGradient($bias$1, dyActivation);
			der.push(biasDer);
		}
		return der;
	};
	const inputs = {
		x: x4D,
		filter: $filter,
		bias: $bias,
		preluActivationWeights: $preluActivationWeights
	};
	const attrs = {
		strides,
		pad: pad$1,
		dataFormat,
		dilations,
		dimRoundingMode,
		activation,
		leakyreluAlpha
	};
	if (bias == null) return customGrad((x4D$1, filter$1, save) => {
		let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);
		save([
			filter$1,
			x4D$1,
			res
		]);
		if (reshapedTo4D) res = reshape(res, [
			res.shape[1],
			res.shape[2],
			res.shape[3]
		]);
		return {
			value: res,
			gradFunc: grad$1
		};
	})(x4D, $filter);
	else return customGrad((x4D$1, filter$1, bias$1, save) => {
		let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);
		save([
			filter$1,
			x4D$1,
			res,
			bias$1
		]);
		if (reshapedTo4D) res = reshape(res, [
			res.shape[1],
			res.shape[2],
			res.shape[3]
		]);
		return {
			value: res,
			gradFunc: grad$1
		};
	})(x4D, $filter, $bias);
}
const conv2d$1 = op({ fusedConv2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/depthwise_conv2d_native_backprop_filter.js
function depthwiseConv2dNativeBackpropFilter_(x, dy, filterShape, strides, pad$1, dilations = [1, 1], dimRoundingMode) {
	let x4D = x;
	if (x.rank === 3) x4D = reshape(x, [
		1,
		x.shape[0],
		x.shape[1],
		x.shape[2]
	]);
	let dy4D = dy;
	if (dy4D.rank === 3) dy4D = reshape(dy, [
		1,
		dy.shape[0],
		dy.shape[1],
		dy.shape[2]
	]);
	const inputs = {
		x: x4D,
		dy: dy4D
	};
	const attrs = {
		strides,
		pad: pad$1,
		dimRoundingMode,
		dilations,
		filterShape
	};
	return ENGINE.runKernel(DepthwiseConv2dNativeBackpropFilter, inputs, attrs);
}
const depthwiseConv2dNativeBackpropFilter = op({ depthwiseConv2dNativeBackpropFilter_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/depthwise_conv2d_native_backprop_input.js
function depthwiseConv2dNativeBackpropInput_(xShape, dy, filter, strides, pad$1, dilations = [1, 1], dimRoundingMode) {
	let dy4D = dy;
	let reshapedTo4D = false;
	if (dy.rank === 3) {
		reshapedTo4D = true;
		dy4D = reshape(dy, [
			1,
			dy.shape[0],
			dy.shape[1],
			dy.shape[2]
		]);
	}
	const inputs = {
		dy: dy4D,
		filter
	};
	const attrs = {
		strides,
		pad: pad$1,
		dimRoundingMode,
		dilations,
		inputShape: xShape
	};
	const res = ENGINE.runKernel(DepthwiseConv2dNativeBackpropInput, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const depthwiseConv2dNativeBackpropInput = op({ depthwiseConv2dNativeBackpropInput_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/fused/depthwise_conv2d.js
/**
* Computes depthwise 2D convolution, optionally fused with adding a
* bias and applying an activation.
*
* Given a 4D `input` array and a `filter` array of shape
* `[filterHeight, filterWidth, inChannels, channelMultiplier]` containing
* `inChannels` convolutional filters of depth 1, this op applies a
* different filter to each input channel (expanding from 1 channel to
* `channelMultiplier` channels for each), then concatenates the results
* together. The output has `inChannels * channelMultiplier` channels.
*
* See
* [https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d](
*     https://www.tensorflow.org/api_docs/python/tf/nn/depthwise_conv2d)
* for more details.
*
* @param obj An object with the following properties:
* @param x The input tensor, of rank 4 or rank 3, of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is
* assumed.
* @param filter The filter tensor, rank 4, of shape
*     `[filterHeight, filterWidth, inChannels, channelMultiplier]`.
* @param strides The strides of the convolution: `[strideHeight,
* strideWidth]`. If strides is a single number, then `strideHeight ==
* strideWidth`.
* @param pad The type of padding algorithm.
*   - `same` and stride 1: output will be of same size as input,
*       regardless of filter size.
*   - `valid`: output will be smaller than input if filter is larger
*       than 1x1.
*   - For more info, see this guide:
*     [https://www.tensorflow.org/api_docs/python/tf/nn/convolution](
*          https://www.tensorflow.org/api_docs/python/tf/nn/convolution)
* @param dilations The dilation rates: `[dilationHeight, dilationWidth]`
*     in which we sample input values across the height and width dimensions
*     in atrous convolution. Defaults to `[1, 1]`. If `rate` is a single
*     number, then `dilationHeight == dilationWidth`. If it is greater than
*     1, then all values of `strides` must be 1.
* @param dataFormat: An optional string from: "NHWC", "NCHW". Defaults to
*     "NHWC". Specify the data format of the input and output data. With the
*     default format "NHWC", the data is stored in the order of: [batch,
*     height, width, channels]. Only "NHWC" is currently supported.
* @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is
*     provided, it will default to truncate.
* @param bias Tensor to be added to the result.
* @param activation Name of activation kernel (defaults to `linear`).
* @param preluActivationWeights Tensor of prelu weights to be applied as part
*     of a `prelu` activation, typically the same shape as `x`.
* @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`
*     activation.
*/
function fusedDepthwiseConv2d_({ x, filter, strides, pad: pad$1, dataFormat = "NHWC", dilations = [1, 1], dimRoundingMode, bias, activation = "linear", preluActivationWeights, leakyreluAlpha }) {
	if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {
		let result = depthwiseConv2d(x, filter, strides, pad$1, dataFormat, dilations, dimRoundingMode);
		if (bias != null) result = add(result, bias);
		return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);
	}
	const $x = convertToTensor(x, "x", "depthwiseConv2d", "float32");
	const $filter = convertToTensor(filter, "filter", "depthwiseConv2d", "float32");
	let x4D = $x;
	let reshapedTo4D = false;
	if ($x.rank === 3) {
		reshapedTo4D = true;
		x4D = reshape($x, [
			1,
			$x.shape[0],
			$x.shape[1],
			$x.shape[2]
		]);
	}
	assert(x4D.rank === 4, () => `Error in fused depthwiseConv2d: input must be rank 4, but got rank ${x4D.rank}.`);
	assert($filter.rank === 4, () => `Error in fused depthwiseConv2d: filter must be rank 4, but got rank ${$filter.rank}.`);
	assert(x4D.shape[3] === $filter.shape[2], () => `Error in fused depthwiseConv2d: number of input channels (${x4D.shape[3]}) must match the inChannels dimension in filter ${$filter.shape[2]}.`);
	if (dilations == null) dilations = [1, 1];
	assert(eitherStridesOrDilationsAreOne(strides, dilations), () => `Error in fused depthwiseConv2d: Either strides or dilations must be 1. Got strides ${strides} and dilations '${dilations}'`);
	if (dimRoundingMode != null) assert(isInt(pad$1), () => `Error in fused depthwiseConv2d: pad must be an integer when using dimRoundingMode ${dimRoundingMode} but got pad ${pad$1}.`);
	const convInfo = computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad$1, dimRoundingMode, true);
	let $bias;
	if (bias != null) {
		$bias = convertToTensor(bias, "bias", "fused conv2d");
		[$bias] = makeTypesMatch($bias, $x);
		assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);
	}
	let $preluActivationWeights;
	if (preluActivationWeights != null) $preluActivationWeights = convertToTensor(preluActivationWeights, "prelu weights", "fused depthwiseConv2d");
	const grad$1 = (dy, saved) => {
		assert(tupleValuesAreOne(dilations), () => `Error in gradient of fused depthwiseConv2d: dilation rates greater than 1 are not yet supported. Got dilations '${dilations}'`);
		const [$filter$1, x4D$1, y, bias$1] = saved;
		const dyActivation = getFusedDyActivation(dy, y, activation);
		const xDer = depthwiseConv2dNativeBackpropInput(x4D$1.shape, dyActivation, $filter$1, strides, pad$1, dilations, dimRoundingMode);
		const filterDer = depthwiseConv2dNativeBackpropFilter(x4D$1, dyActivation, $filter$1.shape, strides, pad$1, dilations, dimRoundingMode);
		if (bias$1 != null) {
			const biasDer = getFusedBiasGradient($bias, dyActivation);
			return [
				xDer,
				filterDer,
				biasDer
			];
		}
		return [xDer, filterDer];
	};
	const inputs = {
		x: x4D,
		filter: $filter,
		bias: $bias,
		preluActivationWeights: $preluActivationWeights
	};
	const attrs = {
		strides,
		pad: pad$1,
		dataFormat,
		dilations,
		dimRoundingMode,
		activation,
		leakyreluAlpha
	};
	if (bias == null) return customGrad((x4D$1, filter$1, save) => {
		let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);
		save([
			filter$1,
			x4D$1,
			res
		]);
		if (reshapedTo4D) res = reshape(res, [
			res.shape[1],
			res.shape[2],
			res.shape[3]
		]);
		return {
			value: res,
			gradFunc: grad$1
		};
	})(x4D, $filter);
	else return customGrad((x4D$1, filter$1, bias$1, save) => {
		let res = ENGINE.runKernel(FusedDepthwiseConv2D, inputs, attrs);
		save([
			filter$1,
			x4D$1,
			res,
			bias$1
		]);
		if (reshapedTo4D) res = reshape(res, [
			res.shape[1],
			res.shape[2],
			res.shape[3]
		]);
		return {
			value: res,
			gradFunc: grad$1
		};
	})(x4D, $filter, $bias);
}
const depthwiseConv2d$1 = op({ fusedDepthwiseConv2d_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/fused/mat_mul.js
/**
* Computes the dot product of two matrices with optional activation and bias.
*
* ```js
* const a = tf.tensor2d([-1, -2], [1, 2]);
* const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);
* const bias = tf.tensor2d([1, 2], [1, 2]);
*
* tf.fused.matMul({a, b, bias, activation: 'relu'}).print();
* ```
*
* @param obj An object with the following properties:
* - `a` First matrix in dot product operation.
* - `b` Second matrix in dot product operation.
* - `transposeA` If true, `a` is transposed before multiplication.
* - `transposeB` If true, `b` is transposed before multiplication.
* - `bias` Matrix to be added to the result.
* - `activation` Name of activation kernel (defaults to `linear`).
* - `preluActivationWeights` Tensor of prelu weights.
* - `leakyreluAlpha` Alpha of leakyrelu.
*/
function fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = "linear", preluActivationWeights, leakyreluAlpha }) {
	if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {
		let result = matMul(a, b, transposeA, transposeB);
		if (bias != null) result = add(result, bias);
		return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);
	}
	let $a = convertToTensor(a, "a", "fused matMul");
	let $b = convertToTensor(b, "b", "fused matMul");
	[$a, $b] = makeTypesMatch($a, $b);
	const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];
	const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];
	const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];
	const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];
	const outerDimsA = $a.shape.slice(0, -2);
	const outerDimsB = $b.shape.slice(0, -2);
	const batchDimA = sizeFromShape(outerDimsA);
	const batchDimB = sizeFromShape(outerDimsB);
	assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at least 2, got ranks ${$a.rank} and ${$b.rank}.`);
	assert(arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (${outerDimsB}) of Tensors with shapes ${$a.shape} and ${$b.shape} must match.`);
	assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (${innerShapeB}) of Tensors with shapes ${$a.shape} and ${$b.shape} and transposeA=${transposeA} and transposeB=${transposeB} must match.`);
	const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);
	const a3D = transposeA ? reshape($a, [
		batchDimA,
		innerShapeA,
		outerShapeA
	]) : reshape($a, [
		batchDimA,
		outerShapeA,
		innerShapeA
	]);
	const b3D = transposeB ? reshape($b, [
		batchDimB,
		outerShapeB,
		innerShapeB
	]) : reshape($b, [
		batchDimB,
		innerShapeB,
		outerShapeB
	]);
	let $bias;
	if (bias != null) {
		$bias = convertToTensor(bias, "bias", "fused matMul");
		[$bias] = makeTypesMatch($bias, $a);
		assertAndGetBroadcastShape(outShape, $bias.shape);
	}
	let $preluActivationWeights;
	if (preluActivationWeights != null) $preluActivationWeights = convertToTensor(preluActivationWeights, "prelu weights", "fused matMul");
	const grad$1 = (dy, saved) => {
		const [a3D$1, b3D$1, y, $bias$1] = saved;
		const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);
		let aDer;
		let bDer;
		if (!transposeA && !transposeB) {
			aDer = matMul(dyActivation, b3D$1, false, true);
			bDer = matMul(a3D$1, dyActivation, true, false);
		} else if (!transposeA && transposeB) {
			aDer = matMul(dyActivation, b3D$1, false, false);
			bDer = matMul(dyActivation, a3D$1, true, false);
		} else if (transposeA && !transposeB) {
			aDer = matMul(b3D$1, dyActivation, false, true);
			bDer = matMul(a3D$1, dyActivation, false, false);
		} else {
			aDer = matMul(b3D$1, dyActivation, true, true);
			bDer = matMul(dyActivation, a3D$1, true, true);
		}
		if (bias != null) {
			const biasDer = getFusedBiasGradient($bias$1, dyActivation);
			return [
				aDer,
				bDer,
				biasDer
			];
		} else return [aDer, bDer];
	};
	const inputs = {
		a: a3D,
		b: b3D,
		bias: $bias,
		preluActivationWeights: $preluActivationWeights
	};
	const attrs = {
		transposeA,
		transposeB,
		activation,
		leakyreluAlpha
	};
	if (bias == null) return customGrad((a3D$1, b3D$1, save) => {
		const res = ENGINE.runKernel(_FusedMatMul, inputs, attrs);
		save([
			a3D$1,
			b3D$1,
			res
		]);
		return {
			value: reshape(res, outShape),
			gradFunc: grad$1
		};
	})(a3D, b3D);
	else return customGrad((a3D$1, b3D$1, $bias$1, save) => {
		const res = ENGINE.runKernel(_FusedMatMul, inputs, attrs);
		save([
			a3D$1,
			b3D$1,
			res,
			$bias$1
		]);
		return {
			value: reshape(res, outShape),
			gradFunc: grad$1
		};
	})(a3D, b3D, $bias);
}
const matMul$1 = op({ fusedMatMul_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/fused_ops.js
var fused_ops_exports = /* @__PURE__ */ __export({
	conv2d: () => conv2d$1,
	depthwiseConv2d: () => depthwiseConv2d$1,
	matMul: () => matMul$1
});

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/signal/hamming_window.js
/**
* Generate a hamming window.
*
* See: https://en.wikipedia.org/wiki/Window_function#Hann_and_Hamming_windows
*
* ```js
* tf.signal.hammingWindow(10).print();
* ```
* @param The length of window
*
* @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
*/
function hammingWindow_(windowLength) {
	return cosineWindow(windowLength, .54, .46);
}
const hammingWindow = op({ hammingWindow_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/signal/hann_window.js
/**
* Generate a Hann window.
*
* See: https://en.wikipedia.org/wiki/Window_function#Hann_and_Hamming_windows
*
* ```js
* tf.signal.hannWindow(10).print();
* ```
* @param The length of window
*
* @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
*/
function hannWindow_(windowLength) {
	return cosineWindow(windowLength, .5, .5);
}
const hannWindow = op({ hannWindow_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/signal/frame.js
/**
* Expands input into frames of frameLength.
* Slides a window size with frameStep.
*
* ```js
* tf.signal.frame([1, 2, 3], 2, 1).print();
* ```
* @param signal The input tensor to be expanded
* @param frameLength Length of each frame
* @param frameStep The frame hop size in samples.
* @param padEnd Whether to pad the end of signal with padValue.
* @param padValue An number to use where the input signal does
*     not exist when padEnd is True.
*
* @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
*/
function frame_(signal$1, frameLength, frameStep, padEnd = false, padValue = 0) {
	let start = 0;
	const output = [];
	while (start + frameLength <= signal$1.size) {
		output.push(slice(signal$1, start, frameLength));
		start += frameStep;
	}
	if (padEnd) while (start < signal$1.size) {
		const padLen = start + frameLength - signal$1.size;
		const pad$1 = concat([slice(signal$1, start, frameLength - padLen), fill([padLen], padValue)]);
		output.push(pad$1);
		start += frameStep;
	}
	if (output.length === 0) return tensor2d([], [0, frameLength]);
	return reshape(concat(output), [output.length, frameLength]);
}
const frame = op({ frame_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/signal/stft.js
/**
* Computes the Short-time Fourier Transform of signals
* See: https://en.wikipedia.org/wiki/Short-time_Fourier_transform
*
* ```js
* const input = tf.tensor1d([1, 1, 1, 1, 1])
* tf.signal.stft(input, 3, 1).print();
* ```
* @param signal 1-dimensional real value tensor.
* @param frameLength The window length of samples.
* @param frameStep The number of samples to step.
* @param fftLength The size of the FFT to apply.
* @param windowFn A callable that takes a window length and returns 1-d tensor.
*
* @doc {heading: 'Operations', subheading: 'Signal', namespace: 'signal'}
*/
function stft_(signal$1, frameLength, frameStep, fftLength, windowFn = hannWindow) {
	if (fftLength == null) fftLength = enclosingPowerOfTwo(frameLength);
	const framedSignal = frame(signal$1, frameLength, frameStep);
	const windowedSignal = mul(framedSignal, windowFn(frameLength));
	return rfft(windowedSignal, fftLength);
}
const stft = op({ stft_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/crop_and_resize.js
/**
* Extracts crops from the input image tensor and resizes them using bilinear
* sampling or nearest neighbor sampling (possibly with aspect ratio change)
* to a common output size specified by cropSize.
*
* @param image 4d tensor of shape `[batch,imageHeight,imageWidth, depth]`,
*     where imageHeight and imageWidth must be positive, specifying the
*     batch of images from which to take crops
* @param boxes 2d float32 tensor of shape `[numBoxes, 4]`. Each entry is
*     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the normalized
*     coordinates of the box in the boxInd[i]'th image in the batch
* @param boxInd 1d int32 tensor of shape `[numBoxes]` with values in range
*     `[0, batch)` that specifies the image that the `i`-th box refers to.
* @param cropSize 1d int32 tensor of 2 elements `[cropHeigh, cropWidth]`
*     specifying the size to which all crops are resized to.
* @param method Optional string from `'bilinear' | 'nearest'`,
*     defaults to bilinear, which specifies the sampling method for resizing
* @param extrapolationValue A threshold for deciding when to remove boxes based
*     on score. Defaults to 0.
* @return A 4D tensor of the shape `[numBoxes,cropHeight,cropWidth,depth]`
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function cropAndResize_(image$1, boxes, boxInd, cropSize, method = "bilinear", extrapolationValue = 0) {
	const $image = convertToTensor(image$1, "image", "cropAndResize");
	const $boxes = convertToTensor(boxes, "boxes", "cropAndResize", "float32");
	const $boxInd = convertToTensor(boxInd, "boxInd", "cropAndResize", "int32");
	const numBoxes = $boxes.shape[0];
	assert($image.rank === 4, () => `Error in cropAndResize: image must be rank 4,but got rank ${$image.rank}.`);
	assert($boxes.rank === 2 && $boxes.shape[1] === 4, () => `Error in cropAndResize: boxes must be have size [${numBoxes},4] but had shape ${$boxes.shape}.`);
	assert($boxInd.rank === 1 && $boxInd.shape[0] === numBoxes, () => `Error in cropAndResize: boxInd must be have size [${numBoxes}] but had shape ${$boxes.shape}.`);
	assert(cropSize.length === 2, () => `Error in cropAndResize: cropSize must be of length 2, but got length ${cropSize.length}.`);
	assert(cropSize[0] >= 1 && cropSize[1] >= 1, () => `cropSize must be atleast [1,1], but was ${cropSize}`);
	assert(method === "bilinear" || method === "nearest", () => `method must be bilinear or nearest, but was ${method}`);
	const inputs = {
		image: $image,
		boxes: $boxes,
		boxInd: $boxInd
	};
	const attrs = {
		method,
		extrapolationValue,
		cropSize
	};
	return ENGINE.runKernel(CropAndResize, inputs, attrs);
}
const cropAndResize = op({ cropAndResize_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/flip_left_right.js
/**
* Flips the image left to right. Currently available in the CPU, WebGL, and
* WASM backends.
*
* @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.
*/
/** @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'} */
function flipLeftRight_(image$1) {
	const $image = convertToTensor(image$1, "image", "flipLeftRight", "float32");
	assert($image.rank === 4, () => `Error in flipLeftRight: image must be rank 4,but got rank ${$image.rank}.`);
	const inputs = { image: $image };
	return ENGINE.runKernel(FlipLeftRight, inputs, {});
}
const flipLeftRight = op({ flipLeftRight_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/grayscale_to_rgb.js
/**
* Converts images from grayscale to RGB format.
*
* @param image A grayscale tensor to convert. The `image`'s last dimension must
*     be size 1 with at least a two-dimensional shape.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function grayscaleToRGB_(image$1) {
	const $image = convertToTensor(image$1, "image", "grayscaleToRGB");
	const lastDimsIdx = $image.rank - 1;
	const lastDims = $image.shape[lastDimsIdx];
	assert($image.rank >= 2, () => `Error in grayscaleToRGB: images must be at least rank 2, but got rank ${$image.rank}.`);
	assert(lastDims === 1, () => `Error in grayscaleToRGB: last dimension of a grayscale image should be size 1, but got size ${lastDims}.`);
	const reps = new Array($image.rank);
	reps.fill(1, 0, lastDimsIdx);
	reps[lastDimsIdx] = 3;
	return tile($image, reps);
}
const grayscaleToRGB = op({ grayscaleToRGB_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/rotate_with_offset.js
/**
* Rotates the input image tensor counter-clockwise with an optional offset
* center of rotation. Currently available in the CPU, WebGL, and WASM backends.
*
* @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.
* @param radians The amount of rotation.
* @param fillValue The value to fill in the empty space leftover
*     after rotation. Can be either a single grayscale value (0-255), or an
*     array of three numbers `[red, green, blue]` specifying the red, green,
*     and blue channels. Defaults to `0` (black).
* @param center The center of rotation. Can be either a single value (0-1), or
*     an array of two numbers `[centerX, centerY]`. Defaults to `0.5` (rotates
*     the image around its center).
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function rotateWithOffset_(image$1, radians, fillValue = 0, center = .5) {
	const $image = convertToTensor(image$1, "image", "rotateWithOffset", "float32");
	assert($image.rank === 4, () => `Error in rotateWithOffset: image must be rank 4,but got rank ${$image.rank}.`);
	const inputs = { image: $image };
	const attrs = {
		radians,
		fillValue,
		center
	};
	return ENGINE.runKernel(RotateWithOffset, inputs, attrs);
}
const rotateWithOffset = op({ rotateWithOffset_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/nonmax_util.js
function nonMaxSuppSanityCheck(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma) {
	if (iouThreshold == null) iouThreshold = .5;
	if (scoreThreshold == null) scoreThreshold = Number.NEGATIVE_INFINITY;
	if (softNmsSigma == null) softNmsSigma = 0;
	const numBoxes = boxes.shape[0];
	maxOutputSize = Math.min(maxOutputSize, numBoxes);
	assert(0 <= iouThreshold && iouThreshold <= 1, () => `iouThreshold must be in [0, 1], but was '${iouThreshold}'`);
	assert(boxes.rank === 2, () => `boxes must be a 2D tensor, but was of rank '${boxes.rank}'`);
	assert(boxes.shape[1] === 4, () => `boxes must have 4 columns, but 2nd dimension was ${boxes.shape[1]}`);
	assert(scores.rank === 1, () => "scores must be a 1D tensor");
	assert(scores.shape[0] === numBoxes, () => `scores has incompatible shape with boxes. Expected ${numBoxes}, but was ${scores.shape[0]}`);
	assert(0 <= softNmsSigma && softNmsSigma <= 1, () => `softNmsSigma must be in [0, 1], but was '${softNmsSigma}'`);
	return {
		maxOutputSize,
		iouThreshold,
		scoreThreshold,
		softNmsSigma
	};
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression.js
/**
* Performs non maximum suppression of bounding boxes based on
* iou (intersection over union).
*
* @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
*     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
*     the bounding box.
* @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
* @param maxOutputSize The maximum number of boxes to be selected.
* @param iouThreshold A float representing the threshold for deciding whether
*     boxes overlap too much with respect to IOU. Must be between [0, 1].
*     Defaults to 0.5 (50% box overlap).
* @param scoreThreshold A threshold for deciding when to remove boxes based
*     on score. Defaults to -inf, which means any score is accepted.
* @return A 1D tensor with the selected box indices.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function nonMaxSuppression_(boxes, scores, maxOutputSize, iouThreshold = .5, scoreThreshold = Number.NEGATIVE_INFINITY) {
	const $boxes = convertToTensor(boxes, "boxes", "nonMaxSuppression", "float32");
	const $scores = convertToTensor(scores, "scores", "nonMaxSuppression", "float32");
	const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);
	maxOutputSize = inputs.maxOutputSize;
	iouThreshold = inputs.iouThreshold;
	scoreThreshold = inputs.scoreThreshold;
	const attrs = {
		maxOutputSize,
		iouThreshold,
		scoreThreshold
	};
	return ENGINE.runKernel(NonMaxSuppressionV3, {
		boxes: $boxes,
		scores: $scores
	}, attrs);
}
const nonMaxSuppression = op({ nonMaxSuppression_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/backends/non_max_suppression_util.js
/**
* @license
* Copyright 2019 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
* Inserts a value into a sorted array. This method allows duplicate, meaning it
* allows inserting duplicate value, in which case, the element will be inserted
* at the lowest index of the value.
* @param arr The array to modify.
* @param element The element to insert.
* @param comparator Optional. If no comparator is specified, elements are
* compared using array_util.defaultComparator, which is suitable for Strings
* and Numbers in ascending arrays. If the array contains multiple instances of
* the target value, the left-most instance will be returned. To provide a
* comparator, it should take 2 arguments to compare and return a negative,
* zero, or a positive number.
*/
function binaryInsert(arr, element, comparator) {
	const index = binarySearch(arr, element, comparator);
	const insertionPoint = index < 0 ? -(index + 1) : index;
	arr.splice(insertionPoint, 0, element);
}
/**
* Searches the array for the target using binary search, returns the index
* of the found element, or position to insert if element not found. If no
* comparator is specified, elements are compared using array_
* util.defaultComparator, which is suitable for Strings and Numbers in
* ascending arrays. If the array contains multiple instances of the target
* value, the left-most instance will be returned.
* @param arr The array to be searched in.
* @param target The target to be searched for.
* @param comparator Should take 2 arguments to compare and return a negative,
*    zero, or a positive number.
* @return Lowest index of the target value if found, otherwise the insertion
*    point where the target should be inserted, in the form of
*    (-insertionPoint - 1).
*/
function binarySearch(arr, target, comparator) {
	return binarySearch_(arr, target, comparator || defaultComparator);
}
/**
* Compares its two arguments for order.
* @param a The first element to be compared.
* @param b The second element to be compared.
* @return A negative number, zero, or a positive number as the first
*     argument is less than, equal to, or greater than the second.
*/
function defaultComparator(a, b) {
	return a > b ? 1 : a < b ? -1 : 0;
}
function binarySearch_(arr, target, comparator) {
	let left = 0;
	let right = arr.length;
	let middle = 0;
	let found = false;
	while (left < right) {
		middle = left + (right - left >>> 1);
		const compareResult = comparator(target, arr[middle]);
		if (compareResult > 0) left = middle + 1;
		else {
			right = middle;
			found = !compareResult;
		}
	}
	return found ? left : -left - 1;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/backends/non_max_suppression_impl.js
function nonMaxSuppressionV3Impl(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold) {
	return nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, 0);
}
function nonMaxSuppressionV4Impl(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, padToMaxOutputSize) {
	return nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, 0, false, padToMaxOutputSize, true);
}
function nonMaxSuppressionV5Impl(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma) {
	return nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma, true);
}
function nonMaxSuppressionImpl_(boxes, scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma, returnScoresTensor = false, padToMaxOutputSize = false, returnValidOutputs = false) {
	const candidates = [];
	for (let i = 0; i < scores.length; i++) if (scores[i] > scoreThreshold) candidates.push({
		score: scores[i],
		boxIndex: i,
		suppressBeginIndex: 0
	});
	candidates.sort(ascendingComparator);
	const scale = softNmsSigma > 0 ? -.5 / softNmsSigma : 0;
	const selectedIndices = [];
	const selectedScores = [];
	while (selectedIndices.length < maxOutputSize && candidates.length > 0) {
		const candidate = candidates.pop();
		const { score: originalScore, boxIndex, suppressBeginIndex } = candidate;
		if (originalScore < scoreThreshold) break;
		let ignoreCandidate = false;
		for (let j = selectedIndices.length - 1; j >= suppressBeginIndex; --j) {
			const iou = intersectionOverUnion(boxes, boxIndex, selectedIndices[j]);
			if (iou >= iouThreshold) {
				ignoreCandidate = true;
				break;
			}
			candidate.score = candidate.score * suppressWeight(iouThreshold, scale, iou);
			if (candidate.score <= scoreThreshold) break;
		}
		candidate.suppressBeginIndex = selectedIndices.length;
		if (!ignoreCandidate) {
			if (candidate.score === originalScore) {
				selectedIndices.push(boxIndex);
				selectedScores.push(candidate.score);
			} else if (candidate.score > scoreThreshold) binaryInsert(candidates, candidate, ascendingComparator);
		}
	}
	const validOutputs = selectedIndices.length;
	const elemsToPad = maxOutputSize - validOutputs;
	if (padToMaxOutputSize && elemsToPad > 0) {
		selectedIndices.push(...new Array(elemsToPad).fill(0));
		selectedScores.push(...new Array(elemsToPad).fill(0));
	}
	const result = { selectedIndices };
	if (returnScoresTensor) result["selectedScores"] = selectedScores;
	if (returnValidOutputs) result["validOutputs"] = validOutputs;
	return result;
}
function intersectionOverUnion(boxes, i, j) {
	const iCoord = boxes.subarray(i * 4, i * 4 + 4);
	const jCoord = boxes.subarray(j * 4, j * 4 + 4);
	const yminI = Math.min(iCoord[0], iCoord[2]);
	const xminI = Math.min(iCoord[1], iCoord[3]);
	const ymaxI = Math.max(iCoord[0], iCoord[2]);
	const xmaxI = Math.max(iCoord[1], iCoord[3]);
	const yminJ = Math.min(jCoord[0], jCoord[2]);
	const xminJ = Math.min(jCoord[1], jCoord[3]);
	const ymaxJ = Math.max(jCoord[0], jCoord[2]);
	const xmaxJ = Math.max(jCoord[1], jCoord[3]);
	const areaI = (ymaxI - yminI) * (xmaxI - xminI);
	const areaJ = (ymaxJ - yminJ) * (xmaxJ - xminJ);
	if (areaI <= 0 || areaJ <= 0) return 0;
	const intersectionYmin = Math.max(yminI, yminJ);
	const intersectionXmin = Math.max(xminI, xminJ);
	const intersectionYmax = Math.min(ymaxI, ymaxJ);
	const intersectionXmax = Math.min(xmaxI, xmaxJ);
	const intersectionArea = Math.max(intersectionYmax - intersectionYmin, 0) * Math.max(intersectionXmax - intersectionXmin, 0);
	return intersectionArea / (areaI + areaJ - intersectionArea);
}
function suppressWeight(iouThreshold, scale, iou) {
	const weight = Math.exp(scale * iou * iou);
	return iou <= iouThreshold ? weight : 0;
}
function ascendingComparator(c1, c2) {
	return c1.score - c2.score || c1.score === c2.score && c2.boxIndex - c1.boxIndex;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_async.js
/**
* Performs non maximum suppression of bounding boxes based on
* iou (intersection over union).
*
* This is the async version of `nonMaxSuppression`
*
* @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
*     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
*     the bounding box.
* @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
* @param maxOutputSize The maximum number of boxes to be selected.
* @param iouThreshold A float representing the threshold for deciding whether
*     boxes overlap too much with respect to IOU. Must be between [0, 1].
*     Defaults to 0.5 (50% box overlap).
* @param scoreThreshold A threshold for deciding when to remove boxes based
*     on score. Defaults to -inf, which means any score is accepted.
* @return A 1D tensor with the selected box indices.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
async function nonMaxSuppressionAsync_(boxes, scores, maxOutputSize, iouThreshold = .5, scoreThreshold = Number.NEGATIVE_INFINITY) {
	const $boxes = convertToTensor(boxes, "boxes", "nonMaxSuppressionAsync");
	const $scores = convertToTensor(scores, "scores", "nonMaxSuppressionAsync");
	const inputs = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold);
	maxOutputSize = inputs.maxOutputSize;
	iouThreshold = inputs.iouThreshold;
	scoreThreshold = inputs.scoreThreshold;
	const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);
	const boxesVals = boxesAndScores[0];
	const scoresVals = boxesAndScores[1];
	const { selectedIndices } = nonMaxSuppressionV3Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold);
	if ($boxes !== boxes) $boxes.dispose();
	if ($scores !== scores) $scores.dispose();
	return tensor1d(selectedIndices, "int32");
}
const nonMaxSuppressionAsync = nonMaxSuppressionAsync_;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score.js
/**
* Performs non maximum suppression of bounding boxes based on
* iou (intersection over union).
*
* This op also supports a Soft-NMS mode (c.f.
* Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score
* of other overlapping boxes, therefore favoring different regions of the image
* with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`
* parameter to be larger than 0.
*
* @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
*     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
*     the bounding box.
* @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
* @param maxOutputSize The maximum number of boxes to be selected.
* @param iouThreshold A float representing the threshold for deciding whether
*     boxes overlap too much with respect to IOU. Must be between [0, 1].
*     Defaults to 0.5 (50% box overlap).
* @param scoreThreshold A threshold for deciding when to remove boxes based
*     on score. Defaults to -inf, which means any score is accepted.
* @param softNmsSigma A float representing the sigma parameter for Soft NMS.
*     When sigma is 0, it falls back to nonMaxSuppression.
* @return A map with the following properties:
*     - selectedIndices: A 1D tensor with the selected box indices.
*     - selectedScores: A 1D tensor with the corresponding scores for each
*       selected box.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function nonMaxSuppressionWithScore_(boxes, scores, maxOutputSize, iouThreshold = .5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0) {
	const $boxes = convertToTensor(boxes, "boxes", "nonMaxSuppression");
	const $scores = convertToTensor(scores, "scores", "nonMaxSuppression");
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);
	maxOutputSize = params.maxOutputSize;
	iouThreshold = params.iouThreshold;
	scoreThreshold = params.scoreThreshold;
	softNmsSigma = params.softNmsSigma;
	const inputs = {
		boxes: $boxes,
		scores: $scores
	};
	const attrs = {
		maxOutputSize,
		iouThreshold,
		scoreThreshold,
		softNmsSigma
	};
	const result = ENGINE.runKernel(NonMaxSuppressionV5, inputs, attrs);
	return {
		selectedIndices: result[0],
		selectedScores: result[1]
	};
}
const nonMaxSuppressionWithScore = op({ nonMaxSuppressionWithScore_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_with_score_async.js
/**
* Asynchronously performs non maximum suppression of bounding boxes based on
* iou (intersection over union).
*
* This op also supports a Soft-NMS mode (c.f.
* Bodla et al, https://arxiv.org/abs/1704.04503) where boxes reduce the score
* of other overlapping boxes, therefore favoring different regions of the image
* with high scores. To enable this Soft-NMS mode, set the `softNmsSigma`
* parameter to be larger than 0.
*
* @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
*     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
*     the bounding box.
* @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
* @param maxOutputSize The maximum number of boxes to be selected.
* @param iouThreshold A float representing the threshold for deciding whether
*     boxes overlap too much with respect to IOU. Must be between [0, 1].
*     Defaults to 0.5 (50% box overlap).
* @param scoreThreshold A threshold for deciding when to remove boxes based
*     on score. Defaults to -inf, which means any score is accepted.
* @param softNmsSigma A float representing the sigma parameter for Soft NMS.
*     When sigma is 0, it falls back to nonMaxSuppression.
* @return A map with the following properties:
*     - selectedIndices: A 1D tensor with the selected box indices.
*     - selectedScores: A 1D tensor with the corresponding scores for each
*       selected box.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
async function nonMaxSuppressionWithScoreAsync_(boxes, scores, maxOutputSize, iouThreshold = .5, scoreThreshold = Number.NEGATIVE_INFINITY, softNmsSigma = 0) {
	const $boxes = convertToTensor(boxes, "boxes", "nonMaxSuppressionAsync");
	const $scores = convertToTensor(scores, "scores", "nonMaxSuppressionAsync");
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);
	maxOutputSize = params.maxOutputSize;
	iouThreshold = params.iouThreshold;
	scoreThreshold = params.scoreThreshold;
	softNmsSigma = params.softNmsSigma;
	const boxesAndScores = await Promise.all([$boxes.data(), $scores.data()]);
	const boxesVals = boxesAndScores[0];
	const scoresVals = boxesAndScores[1];
	const { selectedIndices, selectedScores } = nonMaxSuppressionV5Impl(boxesVals, scoresVals, maxOutputSize, iouThreshold, scoreThreshold, softNmsSigma);
	if ($boxes !== boxes) $boxes.dispose();
	if ($scores !== scores) $scores.dispose();
	return {
		selectedIndices: tensor1d(selectedIndices, "int32"),
		selectedScores: tensor1d(selectedScores)
	};
}
const nonMaxSuppressionWithScoreAsync = nonMaxSuppressionWithScoreAsync_;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded.js
/**
* Asynchronously performs non maximum suppression of bounding boxes based on
* iou (intersection over union), with an option to pad results.
*
* @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
*     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
*     the bounding box.
* @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
* @param maxOutputSize The maximum number of boxes to be selected.
* @param iouThreshold A float representing the threshold for deciding whether
*     boxes overlap too much with respect to IOU. Must be between [0, 1].
*     Defaults to 0.5 (50% box overlap).
* @param scoreThreshold A threshold for deciding when to remove boxes based
*     on score. Defaults to -inf, which means any score is accepted.
* @param padToMaxOutputSize Defalts to false. If true, size of output
*     `selectedIndices` is padded to maxOutputSize.
* @return A map with the following properties:
*     - selectedIndices: A 1D tensor with the selected box indices.
*     - validOutputs: A scalar denoting how many elements in `selectedIndices`
*       are valid. Valid elements occur first, then padding.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function nonMaxSuppressionPadded_(boxes, scores, maxOutputSize, iouThreshold = .5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {
	const $boxes = convertToTensor(boxes, "boxes", "nonMaxSuppression");
	const $scores = convertToTensor(scores, "scores", "nonMaxSuppression");
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null);
	const $maxOutputSize = params.maxOutputSize;
	const $iouThreshold = params.iouThreshold;
	const $scoreThreshold = params.scoreThreshold;
	const inputs = {
		boxes: $boxes,
		scores: $scores
	};
	const attrs = {
		maxOutputSize: $maxOutputSize,
		iouThreshold: $iouThreshold,
		scoreThreshold: $scoreThreshold,
		padToMaxOutputSize
	};
	const result = ENGINE.runKernel(NonMaxSuppressionV4, inputs, attrs);
	return {
		selectedIndices: result[0],
		validOutputs: result[1]
	};
}
const nonMaxSuppressionPadded = op({ nonMaxSuppressionPadded_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/non_max_suppression_padded_async.js
/**
* Asynchronously performs non maximum suppression of bounding boxes based on
* iou (intersection over union), with an option to pad results.
*
* @param boxes a 2d tensor of shape `[numBoxes, 4]`. Each entry is
*     `[y1, x1, y2, x2]`, where `(y1, x1)` and `(y2, x2)` are the corners of
*     the bounding box.
* @param scores a 1d tensor providing the box scores of shape `[numBoxes]`.
* @param maxOutputSize The maximum number of boxes to be selected.
* @param iouThreshold A float representing the threshold for deciding whether
*     boxes overlap too much with respect to IOU. Must be between [0, 1].
*     Defaults to 0.5 (50% box overlap).
* @param scoreThreshold A threshold for deciding when to remove boxes based
*     on score. Defaults to -inf, which means any score is accepted.
* @param padToMaxOutputSize Defalts to false. If true, size of output
*     `selectedIndices` is padded to maxOutputSize.
* @return A map with the following properties:
*     - selectedIndices: A 1D tensor with the selected box indices.
*     - validOutputs: A scalar denoting how many elements in `selectedIndices`
*       are valid. Valid elements occur first, then padding.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
async function nonMaxSuppressionPaddedAsync_(boxes, scores, maxOutputSize, iouThreshold = .5, scoreThreshold = Number.NEGATIVE_INFINITY, padToMaxOutputSize = false) {
	const $boxes = convertToTensor(boxes, "boxes", "nonMaxSuppressionAsync");
	const $scores = convertToTensor(scores, "scores", "nonMaxSuppressionAsync");
	const params = nonMaxSuppSanityCheck($boxes, $scores, maxOutputSize, iouThreshold, scoreThreshold, null);
	const $maxOutputSize = params.maxOutputSize;
	const $iouThreshold = params.iouThreshold;
	const $scoreThreshold = params.scoreThreshold;
	const [boxesVals, scoresVals] = await Promise.all([$boxes.data(), $scores.data()]);
	const { selectedIndices, validOutputs } = nonMaxSuppressionV4Impl(boxesVals, scoresVals, $maxOutputSize, $iouThreshold, $scoreThreshold, padToMaxOutputSize);
	if ($boxes !== boxes) $boxes.dispose();
	if ($scores !== scores) $scores.dispose();
	return {
		selectedIndices: tensor1d(selectedIndices, "int32"),
		validOutputs: scalar(validOutputs, "int32")
	};
}
const nonMaxSuppressionPaddedAsync = nonMaxSuppressionPaddedAsync_;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_bilinear.js
/**
* Bilinear resize a single 3D image or a batch of 3D images to a new shape.
*
* @param images The images, of rank 4 or rank 3, of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
* @param size The new shape `[newHeight, newWidth]` to resize the
*     images to. Each channel is resized individually.
* @param alignCorners Defaults to `false`. If true, rescale
*     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4
*     corners of images and resized images. If false, rescale by
*     `new_height / height`. Treat similarly the width dimension.
* @param halfPixelCenters Defaults to `false`. Whether to assume pixel centers
*     are at 0.5, which would make the floating point coordinates of the top
*     left pixel 0.5, 0.5.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function resizeBilinear_(images, size, alignCorners = false, halfPixelCenters = false) {
	const $images = convertToTensor(images, "images", "resizeBilinear");
	assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeBilinear: x must be rank 3 or 4, but got rank ${$images.rank}.`);
	assert(size.length === 2, () => `Error in resizeBilinear: new shape must 2D, but got shape ${size}.`);
	assert(halfPixelCenters === false || alignCorners === false, () => "Error in resizeBilinear: If halfPixelCenters is true, alignCorners must be false.");
	let batchImages = $images;
	let reshapedTo4D = false;
	if ($images.rank === 3) {
		reshapedTo4D = true;
		batchImages = reshape($images, [
			1,
			$images.shape[0],
			$images.shape[1],
			$images.shape[2]
		]);
	}
	const [] = size;
	const inputs = { images: batchImages };
	const attrs = {
		alignCorners,
		halfPixelCenters,
		size
	};
	const res = ENGINE.runKernel(ResizeBilinear, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const resizeBilinear = op({ resizeBilinear_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/resize_nearest_neighbor.js
/**
* NearestNeighbor resize a batch of 3D images to a new shape.
*
* @param images The images, of rank 4 or rank 3, of shape
*     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is assumed.
* @param size The new shape `[newHeight, newWidth]` to resize the
*     images to. Each channel is resized individually.
* @param alignCorners Defaults to False. If true, rescale
*     input by `(new_height - 1) / (height - 1)`, which exactly aligns the 4
*     corners of images and resized images. If false, rescale by
*     `new_height / height`. Treat similarly the width dimension.
* @param halfPixelCenters Defaults to `false`. Whether to assumes pixels are of
*      half the actual dimensions, and yields more accurate resizes. This flag
*      would also make the floating point coordinates of the top left pixel
*      0.5, 0.5.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function resizeNearestNeighbor_(images, size, alignCorners = false, halfPixelCenters = false) {
	const $images = convertToTensor(images, "images", "resizeNearestNeighbor");
	assert($images.rank === 3 || $images.rank === 4, () => `Error in resizeNearestNeighbor: x must be rank 3 or 4, but got rank ${$images.rank}.`);
	assert(size.length === 2, () => `Error in resizeNearestNeighbor: new shape must 2D, but got shape ${size}.`);
	assert($images.dtype === "float32" || $images.dtype === "int32", () => "`images` must have `int32` or `float32` as dtype");
	assert(halfPixelCenters === false || alignCorners === false, () => "Error in resizeNearestNeighbor: If halfPixelCenters is true, alignCorners must be false.");
	let batchImages = $images;
	let reshapedTo4D = false;
	if ($images.rank === 3) {
		reshapedTo4D = true;
		batchImages = reshape($images, [
			1,
			$images.shape[0],
			$images.shape[1],
			$images.shape[2]
		]);
	}
	const [] = size;
	const inputs = { images: batchImages };
	const attrs = {
		alignCorners,
		halfPixelCenters,
		size
	};
	const res = ENGINE.runKernel(ResizeNearestNeighbor, inputs, attrs);
	if (reshapedTo4D) return reshape(res, [
		res.shape[1],
		res.shape[2],
		res.shape[3]
	]);
	return res;
}
const resizeNearestNeighbor = op({ resizeNearestNeighbor_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/threshold.js
/**
* Performs image binarization with corresponding threshold
* (depends on the method)value, which creates a binary image from a grayscale.
* @param image 3d tensor of shape [imageHeight,imageWidth, depth],
* where imageHeight and imageWidth must be positive.The image color
* range should be [0, 255].
* @param method Optional string from `'binary' | 'otsu'`
* which specifies the method for thresholding. Defaults to 'binary'.
* @param inverted Optional boolean whichspecifies
* if colours should be inverted. Defaults to false.
* @param threshValue Optional number which defines threshold value from 0 to 1.
* Defaults to 0.5.
* @return A 3d tensor of shape [imageHeight,imageWidth, depth], which
* contains binarized image.
*/
function threshold_(image$1, method = "binary", inverted = false, threshValue = .5) {
	const $image = convertToTensor(image$1, "image", "threshold");
	const RED_INTENCITY_COEF = .2989;
	const GREEN_INTENCITY_COEF = .587;
	const BLUE_INTENCITY_COEF = .114;
	const totalPixelsInImage = $image.shape[0] * $image.shape[1];
	let $threshold = mul(tensor1d([threshValue]), 255);
	let r, g, b, grayscale;
	assert($image.rank === 3, () => `Error in threshold: image must be rank 3,but got rank ${$image.rank}.`);
	assert($image.shape[2] === 3 || $image.shape[2] === 1, () => `Error in threshold: image color channel must be equal to 3 or 1but got ${$image.shape[2]}.`);
	assert($image.dtype === "int32" || $image.dtype === "float32", () => `Error in dtype: image dtype must be int32 or float32,but got dtype ${$image.dtype}.`);
	assert(method === "otsu" || method === "binary", () => `Method must be binary or otsu, but was ${method}`);
	if ($image.shape[2] === 3) {
		[r, g, b] = split($image, [
			1,
			1,
			1
		], -1);
		const $r = mul(r, RED_INTENCITY_COEF);
		const $g = mul(g, GREEN_INTENCITY_COEF);
		const $b = mul(b, BLUE_INTENCITY_COEF);
		grayscale = add(add($r, $g), $b);
	} else grayscale = image$1;
	if (method === "otsu") {
		const $histogram = bincount(cast(round(grayscale), "int32"), tensor([]), 256);
		$threshold = otsu($histogram, totalPixelsInImage);
	}
	const invCondition = inverted ? lessEqual(grayscale, $threshold) : greater(grayscale, $threshold);
	return cast(mul(invCondition, 255), "int32");
}
function otsu(histogram, total) {
	let bestThresh = tensor1d([-1]);
	let bestInBetVar = tensor1d([0]);
	let cInBetVar = tensor1d([0]);
	let classFirst, classSecond, meanFirst, meanSec, weightForeground, weightBack;
	for (let index = 0; index < histogram.size - 1; index++) {
		classFirst = slice(histogram, 0, index + 1);
		classSecond = slice(histogram, index + 1);
		weightForeground = div(sum(classFirst), total);
		weightBack = div(sum(classSecond), total);
		const meanFirstDivA = sum(mul(classFirst, range(0, classFirst.size)));
		meanFirst = div(meanFirstDivA, sum(classFirst));
		const meanSecFill = fill(classSecond.shape, classFirst.size);
		const meanSecAdd = add(range(0, classSecond.size), meanSecFill);
		const meanSecMul = mul(classSecond, meanSecAdd);
		meanSec = div(sum(meanSecMul), sum(classSecond));
		const cInBetVarSubA = sub(meanFirst, meanSec);
		const cInBetVarSubB = sub(meanFirst, meanSec);
		const cInBetVarMul = mul(weightForeground, weightBack);
		cInBetVar = mul(mul(cInBetVarMul, cInBetVarSubA), cInBetVarSubB);
		const condition = greater(cInBetVar, bestInBetVar);
		bestInBetVar = where(condition, cInBetVar, bestInBetVar);
		bestThresh = where(condition, tensor1d([index]), bestThresh);
	}
	return bestThresh;
}
const threshold = op({ threshold_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/image/transform.js
/**
* Applies the given transform(s) to the image(s).
*
* @param image 4d tensor of shape `[batch, imageHeight, imageWidth, depth]`.
* @param transforms Projective transform matrix/matrices. A tensor1d of length
*     8 or tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0
*     b1, b2, c0, c1], then it maps the output point (x, y) to a transformed
*     input point (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k),
*     where k = c0 x + c1 y + 1. The transforms are inverted compared to the
*     transform mapping input points to output points.
* @param interpolation Interpolation mode.
*     Supported values: 'nearest', 'bilinear'. Default to 'nearest'.
* @param fillMode Points outside the boundaries of the input are filled
*     according to the given mode, one of 'constant', 'reflect', 'wrap',
*     'nearest'. Default to 'constant'.
*     'reflect': (d c b a | a b c d | d c b a ) The input is extended by
*     reflecting about the edge of the last pixel.
*     'constant': (k k k k | a b c d | k k k k) The input is extended by
*     filling all values beyond the edge with the same constant value k.
*     'wrap': (a b c d | a b c d | a b c d) The input is extended by
*     wrapping around to the opposite edge.
*     'nearest': (a a a a | a b c d | d d d d) The input is extended by
*     the nearest pixel.
* @param fillValue A float represents the value to be filled outside the
*     boundaries when fillMode is 'constant'.
* @param Output dimension after the transform, [height, width]. If undefined,
*     output is the same size as input image.
*
* @doc {heading: 'Operations', subheading: 'Images', namespace: 'image'}
*/
function transform_(image$1, transforms, interpolation = "nearest", fillMode = "constant", fillValue = 0, outputShape) {
	const $image = convertToTensor(image$1, "image", "transform", "float32");
	const $transforms = convertToTensor(transforms, "transforms", "transform", "float32");
	assert($image.rank === 4, () => `Error in transform: image must be rank 4,but got rank ${$image.rank}.`);
	assert($transforms.rank === 2 && ($transforms.shape[0] === $image.shape[0] || $transforms.shape[0] === 1) && $transforms.shape[1] === 8, () => `Error in transform: Input transform should be batch x 8 or 1 x 8`);
	assert(outputShape == null || outputShape.length === 2, () => `Error in transform: outputShape must be [height, width] or null, but got ${outputShape}.`);
	const inputs = {
		image: $image,
		transforms: $transforms
	};
	const attrs = {
		interpolation,
		fillMode,
		fillValue,
		outputShape
	};
	return ENGINE.runKernel(Transform, inputs, attrs);
}
const transform = op({ transform_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/linalg/band_part.js
/**
* Copy a tensor setting everything outside a central band in each innermost
* matrix to zero.
*
* The band part is computed as follows: Assume input has `k` dimensions
* `[I, J, K, ..., M, N]`, then the output is a tensor with the same shape where
* `band[i, j, k, ..., m, n] = in_band(m, n) * input[i, j, k, ..., m, n]`.
* The indicator function
* `in_band(m, n) = (num_lower < 0 || (m-n) <= num_lower))`
* `&& (num_upper < 0 || (n-m) <= num_upper)`
*
* ```js
* const x = tf.tensor2d([[ 0,  1,  2, 3],
*                        [-1,  0,  1, 2],
*                        [-2, -1,  0, 1],
*                        [-3, -2, -1, 0]]);
* let y = tf.linalg.bandPart(x, 1, -1);
* y.print(); // [[ 0,  1,  2, 3],
*            //  [-1,  0,  1, 2],
*            //  [ 0, -1,  0, 1],
*            //  [ 0, 0 , -1, 0]]
* let z = tf.linalg.bandPart(x, 2, 1);
* z.print(); // [[ 0,  1,  0, 0],
*            //  [-1,  0,  1, 0],
*            //  [-2, -1,  0, 1],
*            //  [ 0, -2, -1, 0]]
* ```
*
* @param x Rank `k` tensor
* @param numLower Number of subdiagonals to keep.
*   If negative, keep entire lower triangle.
* @param numUpper Number of subdiagonals to keep.
*   If negative, keep entire upper triangle.
* @returns Rank `k` tensor of the same shape as input.
*   The extracted banded tensor.
*
* @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}
*/
function bandPart_(a, numLower, numUpper) {
	assert(numLower % 1 === 0, () => `bandPart(): numLower must be an integer, got ${numLower}.`);
	assert(numUpper % 1 === 0, () => `bandPart(): numUpper must be an integer, got ${numUpper}.`);
	const $a = convertToTensor(a, "a", "bandPart");
	assert($a.rank >= 2, () => `bandPart(): Rank must be at least 2, got ${$a.rank}.`);
	const shape = $a.shape;
	const [M, N] = $a.shape.slice(-2);
	if (!(numLower <= M)) throw new Error(`bandPart(): numLower (${numLower}) must not be greater than the number of rows (${M}).`);
	if (!(numUpper <= N)) throw new Error(`bandPart(): numUpper (${numUpper}) must not be greater than the number of columns (${N}).`);
	if (numLower < 0) numLower = M;
	if (numUpper < 0) numUpper = N;
	const i = reshape(range(0, M, 1, "int32"), [-1, 1]);
	const j = range(0, N, 1, "int32");
	const ij = sub(i, j);
	const inBand = logicalAnd(lessEqual(ij, scalar(+numLower, "int32")), greaterEqual(ij, scalar(-numUpper, "int32")));
	const zero = zeros([M, N], $a.dtype);
	return reshape(stack(unstack(reshape($a, [
		-1,
		M,
		N
	])).map((mat) => where(inBand, mat, zero))), shape);
}
const bandPart = op({ bandPart_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/linalg/gram_schmidt.js
/**
* Gram-Schmidt orthogonalization.
*
* ```js
* const x = tf.tensor2d([[1, 2], [3, 4]]);
* let y = tf.linalg.gramSchmidt(x);
* y.print();
* console.log('Othogonalized:');
* y.dot(y.transpose()).print();  // should be nearly the identity matrix.
* console.log('First row direction maintained:');
* const data = await y.array();
* console.log(data[0][1] / data[0][0]);  // should be nearly 2.
* ```
*
* @param xs The vectors to be orthogonalized, in one of the two following
*   formats:
*   - An Array of `tf.Tensor1D`.
*   - A `tf.Tensor2D`, i.e., a matrix, in which case the vectors are the rows
*     of `xs`.
*   In each case, all the vectors must have the same length and the length
*   must be greater than or equal to the number of vectors.
* @returns The orthogonalized and normalized vectors or matrix.
*   Orthogonalization means that the vectors or the rows of the matrix
*   are orthogonal (zero inner products). Normalization means that each
*   vector or each row of the matrix has an L2 norm that equals `1`.
*
* @doc {heading:'Operations', subheading:'Linear Algebra', namespace:'linalg'}
*/
function gramSchmidt_(xs) {
	let inputIsTensor2D;
	if (Array.isArray(xs)) {
		inputIsTensor2D = false;
		assert(xs != null && xs.length > 0, () => "Gram-Schmidt process: input must not be null, undefined, or empty");
		const dim = xs[0].shape[0];
		for (let i = 1; i < xs.length; ++i) assert(xs[i].shape[0] === dim, () => `Gram-Schmidt: Non-unique lengths found in the input vectors: (${xs[i].shape[0]} vs. ${dim})`);
	} else {
		inputIsTensor2D = true;
		xs = split(xs, xs.shape[0], 0).map((x) => squeeze(x, [0]));
	}
	assert(xs.length <= xs[0].shape[0], () => `Gram-Schmidt: Number of vectors (${xs.length}) exceeds number of dimensions (${xs[0].shape[0]}).`);
	const ys = [];
	const xs1d = xs;
	for (let i = 0; i < xs.length; ++i) ys.push(ENGINE.tidy(() => {
		let x = xs1d[i];
		if (i > 0) for (let j = 0; j < i; ++j) {
			const proj = mul(sum(mul(ys[j], x)), ys[j]);
			x = sub(x, proj);
		}
		return div(x, norm(x, "euclidean"));
	}));
	if (inputIsTensor2D) return stack(ys, 0);
	else return ys;
}
const gramSchmidt = op({ gramSchmidt_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/linalg/qr.js
/**
* Compute QR decomposition of m-by-n matrix using Householder transformation.
*
* Implementation based on
*   [http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf]
* (http://www.cs.cornell.edu/~bindel/class/cs6210-f09/lec18.pdf)
*
* ```js
* const a = tf.tensor2d([[1, 2], [3, 4]]);
* let [q, r] = tf.linalg.qr(a);
* console.log('Q');
* q.print();
* console.log('R');
* r.print();
* console.log('Orthogonalized');
* q.dot(q.transpose()).print()  // should be nearly the identity matrix.
* console.log('Reconstructed');
* q.dot(r).print(); // should be nearly [[1, 2], [3, 4]];
* ```
*
* @param x The `tf.Tensor` to be QR-decomposed. Must have rank >= 2. Suppose
*   it has the shape `[..., M, N]`.
* @param fullMatrices An optional boolean parameter. Defaults to `false`.
*   If `true`, compute full-sized `Q`. If `false` (the default),
*   compute only the leading N columns of `Q` and `R`.
* @returns An `Array` of two `tf.Tensor`s: `[Q, R]`. `Q` is a unitary matrix,
*   i.e., its columns all have unit norm and are mutually orthogonal.
*   If `M >= N`,
*     If `fullMatrices` is `false` (default),
*       - `Q` has a shape of `[..., M, N]`,
*       - `R` has a shape of `[..., N, N]`.
*     If `fullMatrices` is `true` (default),
*       - `Q` has a shape of `[..., M, M]`,
*       - `R` has a shape of `[..., M, N]`.
*   If `M < N`,
*     - `Q` has a shape of `[..., M, M]`,
*     - `R` has a shape of `[..., M, N]`.
* @throws If the rank of `x` is less than 2.
*
* @doc {heading:'Operations',
*       subheading:'Linear Algebra',
*       namespace:'linalg'}
*/
function qr_(x, fullMatrices = false) {
	assert(x.rank >= 2, () => `qr() requires input tensor to have a rank >= 2, but got rank ${x.rank}`);
	if (x.rank === 2) return qr2d(x, fullMatrices);
	else {
		const outerDimsProd = x.shape.slice(0, x.shape.length - 2).reduce((value, prev) => value * prev);
		const x2ds = unstack(reshape(x, [
			outerDimsProd,
			x.shape[x.shape.length - 2],
			x.shape[x.shape.length - 1]
		]), 0);
		const q2ds = [];
		const r2ds = [];
		x2ds.forEach((x2d) => {
			const [q2d, r2d] = qr2d(x2d, fullMatrices);
			q2ds.push(q2d);
			r2ds.push(r2d);
		});
		const q = reshape(stack(q2ds, 0), x.shape);
		const r = reshape(stack(r2ds, 0), x.shape);
		return [q, r];
	}
}
function qr2d(x, fullMatrices = false) {
	return ENGINE.tidy(() => {
		assert(x.shape.length === 2, () => `qr2d() requires a 2D Tensor, but got a ${x.shape.length}D Tensor.`);
		const m = x.shape[0];
		const n = x.shape[1];
		let q = eye(m);
		let r = clone(x);
		const one2D = tensor2d([[1]], [1, 1]);
		let w = clone(one2D);
		const iters = m >= n ? n : m;
		for (let j = 0; j < iters; ++j) {
			const rTemp = r;
			const wTemp = w;
			const qTemp = q;
			[w, r, q] = ENGINE.tidy(() => {
				const rjEnd1 = slice(r, [j, j], [m - j, 1]);
				const normX = norm(rjEnd1);
				const rjj = slice(r, [j, j], [1, 1]);
				const s = where(greater(rjj, 0), tensor2d([[-1]]), tensor2d([[1]]));
				const u1 = sub(rjj, mul(s, normX));
				const wPre = div(rjEnd1, u1);
				if (wPre.shape[0] === 1) w = clone(one2D);
				else w = concat([one2D, slice(wPre, [1, 0], [wPre.shape[0] - 1, wPre.shape[1]])], 0);
				const tau = neg(div(matMul(s, u1), normX));
				const rjEndAll = slice(r, [j, 0], [m - j, n]);
				const tauTimesW = mul(tau, w);
				const wT = transpose(w);
				if (j === 0) r = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));
				else {
					const rTimesTau = sub(rjEndAll, matMul(tauTimesW, matMul(wT, rjEndAll)));
					r = concat([slice(r, [0, 0], [j, n]), rTimesTau], 0);
				}
				const tawTimesWT = transpose(tauTimesW);
				const qAllJEnd = slice(q, [0, j], [m, q.shape[1] - j]);
				if (j === 0) q = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));
				else {
					const qTimesTau = sub(qAllJEnd, matMul(matMul(qAllJEnd, w), tawTimesWT));
					q = concat([slice(q, [0, 0], [m, j]), qTimesTau], 1);
				}
				return [
					w,
					r,
					q
				];
			});
			dispose([
				rTemp,
				wTemp,
				qTemp
			]);
		}
		if (!fullMatrices && m > n) {
			q = slice(q, [0, 0], [m, n]);
			r = slice(r, [0, 0], [n, n]);
		}
		return [q, r];
	});
}
const qr = op({ qr_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/loss_ops_utils.js
/**
* @license
* Copyright 2020 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
var Reduction;
(function(Reduction$1) {
	Reduction$1[Reduction$1["NONE"] = 0] = "NONE";
	Reduction$1[Reduction$1["MEAN"] = 1] = "MEAN";
	Reduction$1[Reduction$1["SUM"] = 2] = "SUM";
	Reduction$1[Reduction$1["SUM_BY_NONZERO_WEIGHTS"] = 3] = "SUM_BY_NONZERO_WEIGHTS";
})(Reduction || (Reduction = {}));

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/compute_weighted_loss.js
/**
* Computes the weighted loss between two tensors.
*
* @param losses Tensor of shape `[batch_size, d1, ... dN]`.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `losses`, and must be broadcastable to `losses` (i.e., all
*    dimensions must be either `1`, or the same as the corresponding
*    `losses` dimension).
*
* @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
*/
function computeWeightedLoss_(losses$1, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $losses = convertToTensor(losses$1, "losses", "computeWeightedLoss");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "computeWeightedLoss");
	const weightedLoss = $weights == null ? $losses : mul($losses, $weights);
	if (reduction === Reduction.NONE) return weightedLoss;
	if (reduction === Reduction.SUM) return sum(weightedLoss);
	if (reduction === Reduction.MEAN) if ($weights == null) return mean(weightedLoss);
	else {
		const broadcastFactor = $losses.size / $weights.size;
		const result = div(sum(weightedLoss), sum($weights));
		return broadcastFactor > 1 ? div(result, scalar(broadcastFactor)) : result;
	}
	if (reduction === Reduction.SUM_BY_NONZERO_WEIGHTS) if ($weights == null) return div(sum(weightedLoss), scalar($losses.size));
	else {
		const broadcastedWeights = mul($weights, ones($losses.shape));
		const numNonZeros = cast(sum(notEqual(broadcastedWeights, scalar(0))), "float32");
		return div(sum(weightedLoss), numNonZeros);
	}
	throw Error(`Unknown reduction: ${reduction}`);
}
const computeWeightedLoss = op({ computeWeightedLoss_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/absolute_difference.js
/**
* Computes the absolute difference loss between two tensors.
*
* @param labels The ground truth output tensor, same dimensions as
*    'predictions'.
* @param predictions The predicted outputs.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
*    must be either `1`, or the same as the corresponding `losses`
*    dimension).
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`
*
* @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
*/
function absoluteDifference_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, "labels", "absoluteDifference");
	const $predictions = convertToTensor(predictions, "predictions", "absoluteDifference");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "absoluteDifference");
	assertShapesMatch($labels.shape, $predictions.shape, "Error in absoluteDifference: ");
	const losses$1 = abs(sub($labels, $predictions));
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const absoluteDifference = op({ absoluteDifference_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/cosine_distance.js
/**
* Computes the cosine distance loss between two tensors.
*
* @param labels The ground truth output tensor, same dimensions as
*    'predictions'.
* @param predictions The predicted outputs.
* @param axis The dimension along which the cosine distance is computed.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
*    must be either `1`, or the same as the corresponding `losses`
*    dimension).
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`
*
* @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
*/
function cosineDistance_(labels, predictions, axis, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, "labels", "cosineDistance");
	const $predictions = convertToTensor(predictions, "predictions", "cosineDistance");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "cosineDistance");
	assertShapesMatch($labels.shape, $predictions.shape, "Error in cosineDistance: ");
	const one = scalar(1);
	const losses$1 = sub(one, sum(mul($labels, $predictions), axis, true));
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const cosineDistance = op({ cosineDistance_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/hinge_loss.js
/**
* Computes the Hinge loss between two tensors.
*
* @param labels The ground truth output tensor, same dimensions as
*    'predictions'.
* @param predictions The predicted outputs.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
*    must be either `1`, or the same as the corresponding `losses`
*    dimension).
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`
*
* @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
*/
function hingeLoss_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	let $labels = convertToTensor(labels, "labels", "hingeLoss");
	const $predictions = convertToTensor(predictions, "predictions", "hingeLoss");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "hingeLoss");
	assertShapesMatch($labels.shape, $predictions.shape, "Error in hingeLoss: ");
	const one = scalar(1);
	$labels = sub(mul(scalar(2), $labels), one);
	const losses$1 = relu(sub(one, mul($labels, $predictions)));
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const hingeLoss = op({ hingeLoss_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/huber_loss.js
/**
* Computes the huber loss between two tensors.
*
* @param labels The ground truth output tensor, same dimensions as
*    'predictions'.
* @param predictions The predicted outputs.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
*    must be either `1`, or the same as the corresponding `losses`
*    dimension).
* @param delta Point where huber loss changes from quadratic to linear.
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`.
*
* @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
*/
function huberLoss_(labels, predictions, weights, delta = 1, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, "labels", "huberLoss");
	const $predictions = convertToTensor(predictions, "predictions", "huberLoss");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "huberLoss");
	assertShapesMatch($labels.shape, $predictions.shape, "Error in huberLoss: ");
	const deltaScalar = scalar(delta);
	const error = abs(sub($predictions, $labels));
	const quadratic = minimum(error, deltaScalar);
	const linear = sub(error, quadratic);
	const losses$1 = add(mul(scalar(.5), square(quadratic)), mul(deltaScalar, linear));
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const huberLoss = op({ huberLoss_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/log_loss.js
/**
* Computes the log loss between two tensors.
*
* @param labels The ground truth output tensor, same dimensions as
*    'predictions'.
* @param predictions The predicted outputs.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
*    must be either `1`, or the same as the corresponding `losses`
*    dimension).
* @param epsilon A small increment to avoid taking log of zero
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`
*
* @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
*/
function logLoss_(labels, predictions, weights, epsilon = 1e-7, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, "labels", "logLoss");
	const $predictions = convertToTensor(predictions, "predictions", "logLoss");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "logLoss");
	assertShapesMatch($labels.shape, $predictions.shape, "Error in logLoss: ");
	const one = scalar(1);
	const epsilonScalar = scalar(epsilon);
	const l1 = neg(mul($labels, log(add($predictions, epsilonScalar))));
	const l2 = mul(sub(one, $labels), log(add(sub(one, $predictions), epsilonScalar)));
	const losses$1 = sub(l1, l2);
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const logLoss = op({ logLoss_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/mean_squared_error.js
/**
* Computes the mean squared error between two tensors.
*
* @param labels The ground truth output tensor, same dimensions as
*    'predictions'.
* @param predictions The predicted outputs.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
*    must be either `1`, or the same as the corresponding `losses`
*    dimension).
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`
*
* @doc {heading: 'Training', subheading: 'Losses', namespace: 'losses'}
*/
function meanSquaredError_(labels, predictions, weights, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	const $labels = convertToTensor(labels, "labels", "meanSquaredError");
	const $predictions = convertToTensor(predictions, "predictions", "meanSquaredError");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "meanSquaredError");
	assertShapesMatch($labels.shape, $predictions.shape, "Error in meanSquaredError: ");
	const losses$1 = squaredDifference($labels, $predictions);
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const meanSquaredError = op({ meanSquaredError_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/sigmoid_cross_entropy.js
function sigmoidCrossEntropyWithLogits_(labels, logits) {
	const $labels = convertToTensor(labels, "labels", "sigmoidCrossEntropyWithLogits");
	const $logits = convertToTensor(logits, "logits", "sigmoidCrossEntropyWithLogits");
	assertShapesMatch($labels.shape, $logits.shape, "Error in sigmoidCrossEntropyWithLogits: ");
	/**
	* Implementation Details:
	*
	* For brevity, let `x = logits`, `z = labels`.  The logistic loss is
	*     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
	*   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
	*   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
	*   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
	*   = (1 - z) * x + log(1 + exp(-x))
	*   = x - x * z + log(1 + exp(-x))
	*
	*   For x < 0, to avoid overflow in exp(-x), we reformulate the above
	*     x - x * z + log(1 + exp(-x))
	*   = log(exp(x)) - x * z + log(1 + exp(-x))
	*   = - x * z + log(1 + exp(x))
	*
	* Hence, to ensure stability and avoid overflow, the implementation uses
	* this equivalent formulation:
	*     max(x, 0) - x * z + log(1 + exp(-abs(x)))
	*/
	const maxOutput = relu($logits);
	const outputXTarget = mul($logits, $labels);
	const sigmoidOutput = log1p(exp(neg(abs($logits))));
	return add(sub(maxOutput, outputXTarget), sigmoidOutput);
}
/**
* Computes the sigmoid cross entropy loss between two tensors.
*
* If labelSmoothing is nonzero, smooth the labels towards 1/2:
*
*   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)
*                         + 0.5 * labelSmoothing
*
* @param multiClassLabels The ground truth output tensor of shape
* [batch_size, num_classes], same dimensions as 'predictions'.
* @param logits The predicted outputs.
* @param weights Tensor whose rank is either 0, or the same rank as
*    `labels`, and must be broadcastable to `labels` (i.e., all dimensions
*    must be either `1`, or the same as the corresponding `losses`
*    dimension).
* @param labelSmoothing If greater than 0, then smooth the labels.
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`
*
* @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }
*/
function sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	let $multiClassLabels = convertToTensor(multiClassLabels, "multiClassLabels", "sigmoidCrossEntropy");
	const $logits = convertToTensor(logits, "logits", "sigmoidCrossEntropy");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "sigmoidCrossEntropy");
	assertShapesMatch($multiClassLabels.shape, $logits.shape, "Error in sigmoidCrossEntropy: ");
	if (labelSmoothing > 0) {
		const labelSmoothingScalar = scalar(labelSmoothing);
		const one = scalar(1);
		const half = scalar(.5);
		$multiClassLabels = add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));
	}
	const losses$1 = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const sigmoidCrossEntropy = op({ sigmoidCrossEntropy_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/losses/softmax_cross_entropy.js
/**
* Computes softmax cross entropy between logits and labels.
*
* Measures the probability error in discrete classification tasks in which
* the classes are mutually exclusive (each entry is in exactly one class).
* For example, each CIFAR-10 image is labeled with one and only one label: an
* image can be a dog or a truck, but not both.
*
* `NOTE`: While the classes are mutually exclusive, their probabilities need
* not be. All that is required is that each row of labels is a valid
* probability distribution. If they are not, the computation of the gradient
* will be incorrect.
*
* `WARNING`: This op expects unscaled logits, since it performs a softmax on
* logits internally for efficiency. Do not call this op with the output of
* softmax, as it will produce incorrect results.
*
* logits and labels must have the same shape, e.g. [batch_size, num_classes]
* and the same dtype.
* @param labels The labels array.
* @param logits The logits array.
* @param dim The dimension softmax would be performed on. Defaults to `-1`
*     which indicates the last dimension.
*/
function softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {
	if (dim === -1) dim = logits.rank - 1;
	if (dim !== logits.rank - 1) throw Error(`Softmax cross entropy along a non-last dimension is not yet supported. Labels / logits was rank ${logits.rank} and dim was ${dim}`);
	return customGrad((labels$1, logits$1, save) => {
		const lse = logSumExp(logits$1, [dim], true);
		const logResult = sub(cast(logits$1, "float32"), lse);
		save([labels$1, logResult]);
		const costVector = neg(mul(logResult, labels$1));
		const value = sum(costVector, [dim]);
		const gradFunc = (dy, saved) => {
			const [labels$2, logResult$1] = saved;
			const dyShape = expandShapeToKeepDim(dy.shape, [dim]);
			return [mul(reshape(dy, dyShape), sub(cast(labels$2, "float32"), exp(logResult$1))), mul(reshape(dy, dyShape), sub(exp(logResult$1), cast(labels$2, "float32")))];
		};
		return {
			value,
			gradFunc
		};
	})(labels, logits);
}
/**
* Computes the softmax cross entropy loss between two tensors.
*
* If labelSmoothing is nonzero, smooth the labels towards 1/2:
*
*   newOnehotLabels = onehotLabels * (1 - labelSmoothing)
*                         + labelSmoothing / numClasses
*
* @param onehotLabels One hot encoded labels
*    [batch_size, num_classes], same dimensions as 'predictions'.
* @param logits The predicted outputs.
* @param weights Tensor whose rank is either 0, or 1, and must be
*    broadcastable to `loss`  of shape [batch_size]
* @param labelSmoothing If greater than 0, then smooth the labels.
* @param reduction Type of reduction to apply to loss. Should be of type
*    `Reduction`
*
* @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }
*/
function softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {
	let $onehotLabels = convertToTensor(onehotLabels, "onehotLabels", "softmaxCrossEntropy");
	const $logits = convertToTensor(logits, "logits", "softmaxCrossEntropy");
	let $weights = null;
	if (weights != null) $weights = convertToTensor(weights, "weights", "softmaxCrossEntropy");
	assertShapesMatch($onehotLabels.shape, $logits.shape, "Error in softmaxCrossEntropy: ");
	if (labelSmoothing > 0) {
		const labelSmoothingScalar = scalar(labelSmoothing);
		const one = scalar(1);
		const numClasses = scalar($onehotLabels.shape[1]);
		$onehotLabels = add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));
	}
	const losses$1 = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);
	return computeWeightedLoss(losses$1, $weights, reduction);
}
const softmaxCrossEntropy = op({ softmaxCrossEntropy_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sparse/sparse_fill_empty_rows.js
/**
* The input SparseTensor is represented via the map of inputs {`indices`,
* `values`, `denseShape`}. The output SparseTensor has the same `denseShape`
* but with indices `outputIndices` and values `outputValues`. This op inserts a
* single entry for every row that doesn't have any values. The index is created
* as `[row, 0, ..., 0]` and the inserted value is `defaultValue`.
*
* For example, suppose `spInput` has shape [5, 6] and non-empty values:
* [0, 1]: a
* [0, 3]: b
* [2, 0]: c
* [3, 1]: d
*
* Rows 1 and 4 are empty, so the output will be of shape [5, 6] with values:
* [0, 1]: a
* [0, 3]: b
* [1, 0]: `defaultValue`
* [2, 0]: c
* [3, 1]: d
* [4, 0]: `defaultValue`
*
* The output SparseTensor will be in row-major order and will have the same
* shape as the input.
*
* This op also returns an indicator vector shaped [dense_shape[0]] such that
* emptyRowIndicator[i] = True iff row i was an empty row.
*
* And a reverse index map vector shaped [indices.shape[0]] that is used during
* backpropagation, reverseIndexMap[i] = outi s.t. indices[i, j] ==
* outputIndices[outi, j] for all j
*
* ```js
* const result = tf.sparse.sparseFillEmptyRows(
*   [[0, 0], [1, 0], [1, 3], [1, 4], [3, 2], [3, 3]],
*   [0, 10, 13, 14, 32, 33], [5, 6], -1);
* console.log(result);
* result['outputIndices'].print(); // [[0, 0], [1, 0], [1, 3], [1, 4],
*                                  //  [2, 0], [3, 2], [3, 3], [4, 0]]
* result['outputValues'].print(); // [0, 10, 13, 14,-1, 32, 33, -1]
* result['emptyRowIndicator'].print(); // [false, false, true, false, true]
* result['reverseIndexMap'].print(); // [0, 1, 2, 3, 5, 6]
* ```
* @param indices: 2-D. the indices of the sparse tensor.
* @param values: 1-D. the values of the sparse tensor.
* @param denseShape: 1-D. the shape of the sparse tensor.
* @param defaultValue: 0-D. default value to insert into location [row, 0, ...,
*     0] for rows missing from the input sparse tensor.
* @return A map with the following properties:
*     - outputIndices
*     - outputValues: 1-D. the values of the filled sparse tensor.
*     - emptyRowIndicator: 1-D. whether the dense row was missing in the input
* sparse tensor.
*     - reverseIndexMap: 1-D. a map from the input indices to the output
* indices.
* @doc {heading: 'Operations', subheading: 'Sparse'}
*/
function sparseFillEmptyRows_(indices, values, denseShape, defaultValue) {
	const $indices = convertToTensor(indices, "indices", "sparseFillEmptyRows");
	const $values = convertToTensor(values, "values", "sparseFillEmptyRows");
	const $denseShape = convertToTensor(denseShape, "denseShape", "sparseFillEmptyRows");
	const $defaultValue = convertToTensor(defaultValue, "defaultValue", "sparseFillEmptyRows", $values.dtype);
	if ($indices.rank !== 2) throw new Error(`Indices should be Tensor2D but received shape
        ${$indices.shape}`);
	if ($values.rank !== 1) throw new Error(`Values should be Tensor1D but received shape ${$values.shape}`);
	if ($denseShape.rank !== 1) throw new Error(`Dense shape should be Tensor1D but received shape ${$denseShape.shape}`);
	if ($defaultValue.rank !== 0) throw new Error(`Default value should be a scalar but received shape ${$defaultValue.shape}`);
	const inputs = {
		indices: $indices,
		values: $values,
		denseShape: $denseShape,
		defaultValue: $defaultValue
	};
	const result = ENGINE.runKernel(SparseFillEmptyRows, inputs);
	return {
		outputIndices: result[0],
		outputValues: result[1],
		emptyRowIndicator: result[2],
		reverseIndexMap: result[3]
	};
}
const sparseFillEmptyRows = op({ sparseFillEmptyRows_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sparse/sparse_reshape.js
/**
* This operation has the same semantics as reshape on the represented dense
* tensor. The `inputIndices` are recomputed based on the requested `newShape`.
* If one component of `newShape` is the special value -1, the size of that
* dimension is computed so that the total dense size remains constant. At most
* one component of `newShape` can be -1. The number of dense elements implied
* by `newShape` must be the same as the number of dense elements originally
* implied by `inputShape`. Reshaping does not affect the order of values in the
* SparseTensor. If the input tensor has rank R_in and N non-empty values, and
* `newShape` has length R_out, then `inputIndices` has shape [N, R_in],
* `inputShape` has length R_in, `outputIndices` has shape [N, R_out], and
* `outputShape` has length R_out.
*
* ```js
* const result = tf.sparse.sparseReshape(
*   [[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 2, 3]],
*   [2, 3, 6], [9, -1]);
* console.log(result);
* result['outputIndices'].print(); //[[0, 0], [0, 1], [1, 2], [4, 2], [8, 1]]
* result['outputShape'].print(); // [9, 4]
* ```
* @param inputIndices: 2-D. N x R_in matrix with the indices of non-empty
* values in a SparseTensor.
* @param inputShape: 1-D. R_in Tensor1D with the input SparseTensor's dense
* shape.
* @param newShape: 1-D. R_out Tensor1D with the requested new dense shape.
* @return A map with the following properties:
*     - outputIndices: 2-D. N x R_out matrix with the updated indices of
*       non-empty values in the output SparseTensor.
*     - outputShape: 1-D. R_out vector with the full dense shape of the output
*       SparseTensor. This is the same as newShape but with any -1 dimensions
*        filled in.
* @doc {heading: 'Operations', subheading: 'Sparse'}
*/
function sparseReshape_(inputIndices, inputShape, newShape) {
	const $inputIndices = convertToTensor(inputIndices, "inputIndices", "sparseReshape");
	const $inputShape = convertToTensor(inputShape, "inputShape", "sparseReshape");
	const $newShape = convertToTensor(newShape, "newShape", "sparseReshape");
	if ($inputIndices.rank !== 2) throw new Error(`Input indices should be Tensor2D but received shape
        ${$inputIndices.shape}`);
	if ($inputShape.rank !== 1) throw new Error(`Input shape should be Tensor1D but received shape ${$inputShape.shape}`);
	if ($newShape.rank !== 1) throw new Error(`New shape should be Tensor1D but received shape ${$newShape.shape}`);
	const inputs = {
		inputIndices: $inputIndices,
		inputShape: $inputShape,
		newShape: $newShape
	};
	const result = ENGINE.runKernel(SparseReshape, inputs);
	return {
		outputIndices: result[0],
		outputShape: result[1]
	};
}
const sparseReshape = op({ sparseReshape_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sparse/sparse_segment_mean.js
/**
* Computes the mean along sparse segments of a tensor.
*
* ```js
* const c = tf.tensor2d([[1,2,3,4], [-1,-2,-3,-4], [6,7,8,9]]);
* // Select two rows, one segment.
* const result1 = tf.sparse.sparseSegmentMean(c,
*                                           tf.tensor1d([0, 1], 'int32'),
*                                           tf.tensor1d([0, 0], 'int32'));
* result1.print(); // [[0, 0, 0, 0]]
*
* // Select two rows, two segments.
* const result2 = tf.sparse.sparseSegmentMean(c,
*                                             tf.tensor1d([0, 1], 'int32'),
*                                             tf.tensor1d([0, 1], 'int32'));
* result2.print(); // [[1, 2, 3, 4], [-1, -2, -3, -4]]
*
* // Select all rows, two segments.
* const result3 = tf.sparse.sparseSegmentMean(c,
*                                             tf.tensor1d([0, 1, 2], 'int32'),
*                                             tf.tensor1d([0, 1, 1], 'int32'));
* result3.print(); // [[1.0, 2.0, 3.0, 4.0], [2.5, 2.5, 2.5, 2.5]]
* ```
* @param data: A Tensor of at least one dimension with data that will be
*     assembled in the output.
* @param indices: A 1-D Tensor with indices into data. Has same rank as
*     segmentIds.
* @param segmentIds: A 1-D Tensor with indices into the output Tensor. Values
*     should be sorted and can be repeated.
* @return Has same shape as data, except for dimension 0 which has equal to
*         the number of segments.
*
* @doc {heading: 'Operations', subheading: 'Sparse'}
*/
function sparseSegmentMean_(data, indices, segmentIds) {
	const $data = convertToTensor(data, "data", "sparseSegmentMean");
	const $indices = convertToTensor(indices, "indices", "sparseSegmentMean");
	const $segmentIds = convertToTensor(segmentIds, "segmentIds", "sparseSegmentMean");
	if ($data.rank < 1) throw new Error(`Data should be at least 1 dimensional but received scalar`);
	if ($indices.rank !== 1) throw new Error(`Indices should be Tensor1D but received shape
          ${$indices.shape}`);
	if ($segmentIds.rank !== 1) throw new Error(`Segment ids should be Tensor1D but received shape
          ${$segmentIds.shape}`);
	const inputs = {
		data: $data,
		indices: $indices,
		segmentIds: $segmentIds
	};
	return ENGINE.runKernel(SparseSegmentMean, inputs);
}
const sparseSegmentMean = op({ sparseSegmentMean_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/sparse/sparse_segment_sum.js
/**
* Computes the sum along sparse segments of a tensor.
*
* ```js
* const c = tf.tensor2d([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]]);
* // Select two rows, one segment.
* const result1 = tf.sparse.sparseSegmentSum(c,
*                                           tf.tensor1d([0, 1], 'int32'),
*                                           tf.tensor1d([0, 0], 'int32'));
* result1.print(); // [[0, 0, 0, 0]]
*
* // Select two rows, two segment.
* const result2 = tf.sparse.sparseSegmentSum(c,
*                                           tf.tensor1d([0, 1], 'int32'),
*                                           tf.tensor1d([0, 1], 'int32'));
* result2.print(); // [[1, 2, 3, 4], [-1, -2, -3, -4]]
*
* // Select all rows, two segments.
* const result3 = tf.sparse.sparseSegmentSum(c,
*                                           tf.tensor1d([0, 1, 2], 'int32'),
*                                           tf.tensor1d([0, 0, 1], 'int32'));
* result3.print(); // [[0, 0, 0, 0], [5, 6, 7, 8]]
* ```
* @param data: A Tensor of at least one dimension with data that will be
*     assembled in the output.
* @param indices: A 1-D Tensor with indices into data. Has same rank as
*     segmentIds.
* @param segmentIds: A 1-D Tensor with indices into the output Tensor. Values
*     should be sorted and can be repeated.
* @return Has same shape as data, except for dimension 0 which has equal to
*         the number of segments.
*
* @doc {heading: 'Operations', subheading: 'Sparse'}
*/
function sparseSegmentSum_(data, indices, segmentIds) {
	const $data = convertToTensor(data, "data", "sparseSegmentSum");
	const $indices = convertToTensor(indices, "indices", "sparseSegmentSum");
	const $segmentIds = convertToTensor(segmentIds, "segmentIds", "sparseSegmentSum");
	if ($data.rank < 1) throw new Error(`Data should be at least 1 dimensional but received scalar`);
	if ($indices.rank !== 1) throw new Error(`Indices should be Tensor1D but received shape
         ${$indices.shape}`);
	if ($segmentIds.rank !== 1) throw new Error(`Segment ids should be Tensor1D but received shape
         ${$segmentIds.shape}`);
	const inputs = {
		data: $data,
		indices: $indices,
		segmentIds: $segmentIds
	};
	return ENGINE.runKernel(SparseSegmentSum, inputs);
}
const sparseSegmentSum = op({ sparseSegmentSum_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/string/string_n_grams.js
/**
* Creates ngrams from ragged string data.
*
* This op accepts a ragged tensor with 1 ragged dimension containing only
* strings and outputs a ragged tensor with 1 ragged dimension containing ngrams
* of that string, joined along the innermost axis.
*
* ```js
* const result = tf.string.stringNGrams(
*   ['a', 'b', 'c', 'd'], tf.tensor1d([0, 2, 4], 'int32'),
*   '|', [1, 2], 'LP', 'RP', -1, false);
* result['nGrams'].print(); // ['a', 'b', 'LP|a', 'a|b', 'b|RP',
*                           //  'c', 'd', 'LP|c', 'c|d', 'd|RP']
* result['nGramsSplits'].print(); // [0, 5, 10]
* ```
* @param data: The values tensor of the ragged string tensor to make ngrams out
*     of. Must be a 1D string tensor.
* @param dataSplits: The splits tensor of the ragged string tensor to make
*     ngrams out of.
* @param separator: The string to append between elements of the token. Use ""
*     for no separator.
* @param nGramWidths: The sizes of the ngrams to create.
* @param leftPad: The string to use to pad the left side of the ngram sequence.
*     Only used if pad_width !== 0.
* @param rightPad: The string to use to pad the right side of the ngram
*     sequence. Only used if pad_width !== 0.
* @param padWidth: The number of padding elements to add to each side of each
*     sequence. Note that padding will never be greater than `nGramWidths`-1
*     regardless of this value. If `padWidth`=-1 , then add max(`nGramWidths)-1
*     elements.
* @param preserveShortSequences: If true, then ensure that at least one ngram
*     is generated for each input sequence. In particular, if an input sequence
*     is shorter than min(ngramWidth) + 2*padWidth, then generate a single
*     ngram containing the entire sequence. If false, then no ngrams are
*     generated for these short input sequences.
* @return A map with the following properties:
*     - nGrams: The values tensor of the output ngrams ragged tensor.
*     - nGramsSplits: The splits tensor of the output ngrams ragged tensor.
*
* @doc {heading: 'Operations', subheading: 'String'}
*/
function stringNGrams_(data, dataSplits, separator, nGramWidths, leftPad, rightPad$1, padWidth, preserveShortSequences) {
	const $data = convertToTensor(data, "data", "stringNGrams", "string");
	if ($data.dtype !== "string") throw new Error("Data must be of datatype string");
	if ($data.shape.length !== 1) throw new Error(`Data must be a vector, saw: ${$data.shape}`);
	const $dataSplits = convertToTensor(dataSplits, "dataSplits", "stringNGrams");
	if ($dataSplits.dtype !== "int32") throw new Error("Data splits must be of datatype int32");
	const attrs = {
		separator,
		nGramWidths,
		leftPad,
		rightPad: rightPad$1,
		padWidth,
		preserveShortSequences
	};
	const inputs = {
		data: $data,
		dataSplits: $dataSplits
	};
	const result = ENGINE.runKernel(StringNGrams, inputs, attrs);
	return {
		nGrams: result[0],
		nGramsSplits: result[1]
	};
}
const stringNGrams = op({ stringNGrams_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/string/string_split.js
/**
* Split elements of `input` based on `delimiter` into a SparseTensor .
*
* Let N be the size of source (typically N will be the batch size). Split each
* element of `input` based on `delimiter` and return a SparseTensor containing
* the splitted tokens. Empty tokens are ignored if `skipEmpty` is set to True.
*
* `delimiter` can be empty, or a string of split characters. If `delimiter` is
* an empty string, each element of `input` is split into individual
* character strings. Otherwise every character of `delimiter` is a potential
* split point.
*
* ```js
* const result = tf.string.stringSplit(['hello world',  'a b c'], ' ');
* result['indices'].print(); // [[0, 0], [0, 1], [1, 0], [1, 1], [1, 2]]
* result['values'].print(); // ['hello', 'world', 'a', 'b', 'c']
* result['shape'].print(); // [2, 3]
* ```
* @param input: 1-D. Strings to split.
* @param delimiter: 0-D. Delimiter characters, or empty string.
* @param skipEmpty: Optional. If true, skip the empty strings from the result.
*     Defaults to true.
* @return A map with the following properties:
*     - indices: A dense matrix of int32 representing the indices of the sparse
*       tensor.
*     - values: A vector of strings corresponding to the splited values.
*     - shape: a length-2 vector of int32 representing the shape of the sparse
* tensor, where the first value is N and the second value is the maximum number
* of tokens in a single input entry.
*
* @doc {heading: 'Operations', subheading: 'String'}
*/
function stringSplit_(input, delimiter, skipEmpty = true) {
	const $input = convertToTensor(input, "input", "stringSplit", "string");
	const $delimiter = convertToTensor(delimiter, "delimiter", "stringSplit", "string");
	if ($input.rank !== 1) throw new Error(`Input should be Tensor1D but received shape ${$input.shape}`);
	if ($delimiter.rank !== 0) throw new Error(`Delimiter should be a scalar but received shape ${$delimiter.shape}`);
	const attrs = { skipEmpty };
	const inputs = {
		input: $input,
		delimiter: $delimiter
	};
	const result = ENGINE.runKernel(StringSplit, inputs, attrs);
	return {
		indices: result[0],
		values: result[1],
		shape: result[2]
	};
}
const stringSplit = op({ stringSplit_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/string/string_to_hash_bucket_fast.js
/**
* Converts each string in the input Tensor to its hash mod by a number of
* buckets.
*
* The hash function is deterministic on the content of the string within the
* process and will never change. However, it is not suitable for cryptography.
* This function may be used when CPU time is scarce and inputs are trusted or
* unimportant. There is a risk of adversaries constructing inputs that all hash
* to the same bucket.
*
* ```js
* const result = tf.string.stringToHashBucketFast(
*   ['Hello', 'TensorFlow', '2.x'], 3);
* result.print(); // [0, 2, 2]
* ```
* @param input: The strings to assign a hash bucket.
* @param numBuckets: The number of buckets.
* @return A Tensor of the same shape as the input tensor.
*
* @doc {heading: 'Operations', subheading: 'String'}
*/
function stringToHashBucketFast_(input, numBuckets) {
	const $input = convertToTensor(input, "input", "stringToHashBucketFast", "string");
	const attrs = { numBuckets };
	if (numBuckets <= 0) throw new Error(`Number of buckets must be at least 1`);
	const inputs = { input: $input };
	return ENGINE.runKernel(StringToHashBucketFast, inputs, attrs);
}
const stringToHashBucketFast = op({ stringToHashBucketFast_ });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/ops.js
var spectral = {
	fft,
	ifft,
	rfft,
	irfft
};
var signal = {
	hammingWindow,
	hannWindow,
	frame,
	stft
};
var image = {
	flipLeftRight,
	grayscaleToRGB,
	resizeNearestNeighbor,
	resizeBilinear,
	rotateWithOffset,
	cropAndResize,
	nonMaxSuppression,
	nonMaxSuppressionAsync,
	nonMaxSuppressionWithScore,
	nonMaxSuppressionWithScoreAsync,
	nonMaxSuppressionPadded,
	nonMaxSuppressionPaddedAsync,
	threshold,
	transform
};
var linalg = {
	bandPart,
	gramSchmidt,
	qr
};
var losses = {
	absoluteDifference,
	computeWeightedLoss,
	cosineDistance,
	hingeLoss,
	huberLoss,
	logLoss,
	meanSquaredError,
	sigmoidCrossEntropy,
	softmaxCrossEntropy
};
var sparse = {
	sparseFillEmptyRows,
	sparseReshape,
	sparseSegmentMean,
	sparseSegmentSum
};
var string = {
	stringNGrams,
	stringSplit,
	stringToHashBucketFast
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer.js
/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */
var Optimizer = class extends Serializable {
	/**
	* Executes `f()` and minimizes the scalar output of `f()` by computing
	* gradients of y with respect to the list of trainable variables provided by
	* `varList`. If no list is provided, it defaults to all trainable variables.
	*
	* @param f The function to execute and whose output to minimize.
	* @param returnCost Whether to return the scalar cost value produced by
	* executing `f()`.
	* @param varList An optional list of variables to update. If specified, only
	* the trainable variables in varList will be updated by minimize. Defaults to
	* all trainable variables.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers'}
	*/
	minimize(f, returnCost = false, varList) {
		const { value, grads: grads$1 } = this.computeGradients(f, varList);
		if (varList != null) {
			const gradArray = varList.map((v) => ({
				name: v.name,
				tensor: grads$1[v.name]
			}));
			this.applyGradients(gradArray);
		} else this.applyGradients(grads$1);
		dispose(grads$1);
		if (returnCost) return value;
		else {
			value.dispose();
			return null;
		}
	}
	/**
	* The number of iterations that this optimizer instance has been invoked for.
	*/
	get iterations() {
		if (this.iterations_ == null) this.iterations_ = 0;
		return this.iterations_;
	}
	incrementIterations() {
		this.iterations_ = this.iterations + 1;
	}
	/**
	* Executes f() and computes the gradient of the scalar output of f() with
	* respect to the list of trainable variables provided by `varList`. If no
	* list is provided, it defaults to all trainable variables.
	*
	* @param f The function to execute and whose output to use for computing
	* gradients with respect to variables.
	* @param varList An optional list of variables to compute gradients with
	* respect to. If specified, only the trainable variables in varList will have
	* gradients computed with respect to. Defaults to all trainable variables.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers'}
	*/
	computeGradients(f, varList) {
		return variableGrads(f, varList);
	}
	/**
	* Dispose the variables (if any) owned by this optimizer instance.
	*/
	dispose() {
		if (this.iterations_ != null) dispose(this.iterations_);
	}
	async saveIterations() {
		if (this.iterations_ == null) this.iterations_ = 0;
		return {
			name: "iter",
			tensor: scalar(this.iterations_, "int32")
		};
	}
	async getWeights() {
		throw new Error("getWeights() is not implemented for this optimizer yet.");
	}
	async setWeights(weightValues) {
		throw new Error(`setWeights() is not implemented for this optimizer class ${this.getClassName()}`);
	}
	/**
	* Extract the first element of the weight values and set it
	* as the iterations counter variable of this instance of optimizer.
	*
	* @param weightValues
	* @returns Weight values with the first element consumed and excluded.
	*/
	async extractIterations(weightValues) {
		this.iterations_ = (await weightValues[0].tensor.data())[0];
		return weightValues.slice(1);
	}
};
Object.defineProperty(Optimizer, Symbol.hasInstance, { value: (instance) => {
	return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;
} });

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/adadelta_optimizer.js
/** @doclink Optimizer */
var AdadeltaOptimizer = class extends Optimizer {
	constructor(learningRate, rho, epsilon = null) {
		super();
		this.learningRate = learningRate;
		this.rho = rho;
		this.epsilon = epsilon;
		this.accumulatedGrads = [];
		this.accumulatedUpdates = [];
		if (epsilon == null) this.epsilon = ENGINE.backend.epsilon();
	}
	applyGradients(variableGradients) {
		(Array.isArray(variableGradients) ? variableGradients.map((item) => item.name) : Object.keys(variableGradients)).forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			const trainable = false;
			if (this.accumulatedGrads[i] == null) this.accumulatedGrads[i] = {
				originalName: `${name}/accum_grad`,
				variable: tidy(() => zerosLike(value).variable(trainable))
			};
			if (this.accumulatedUpdates[i] == null) this.accumulatedUpdates[i] = {
				originalName: `${name}/accum_var`,
				variable: tidy(() => zerosLike(value).variable(trainable))
			};
			const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];
			if (gradient == null) return;
			const accumulatedGrad = this.accumulatedGrads[i].variable;
			const accumulatedUpdate = this.accumulatedUpdates[i].variable;
			tidy(() => {
				const newAccumulatedGrad = add(mul(accumulatedGrad, this.rho), mul(square(gradient), 1 - this.rho));
				const updates = mul(div(sqrt(add(accumulatedUpdate, this.epsilon)), sqrt(add(accumulatedGrad, this.epsilon))), gradient);
				const newAccumulatedUpdate = add(mul(accumulatedUpdate, this.rho), mul(square(updates), 1 - this.rho));
				accumulatedGrad.assign(newAccumulatedGrad);
				accumulatedUpdate.assign(newAccumulatedUpdate);
				const newValue = add(mul(updates, -this.learningRate), value);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}
	dispose() {
		if (this.accumulatedUpdates != null) {
			dispose(this.accumulatedGrads.map((v) => v.variable));
			dispose(this.accumulatedUpdates.map((v) => v.variable));
		}
	}
	async getWeights() {
		const variables = [...this.accumulatedGrads, ...this.accumulatedUpdates];
		return [await this.saveIterations()].concat(variables.map((v) => ({
			name: v.originalName,
			tensor: v.variable
		})));
	}
	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const variableCount = weightValues.length / 2;
		const trainable = false;
		this.accumulatedGrads = weightValues.slice(0, variableCount).map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
		this.accumulatedUpdates = weightValues.slice(variableCount, variableCount * 2).map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
	}
	getConfig() {
		return {
			"learningRate": this.learningRate,
			"rho": this.rho,
			"epsilon": this.epsilon
		};
	}
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config["learningRate"], config["rho"], config["epsilon"]);
	}
};
/** @nocollapse */
AdadeltaOptimizer.className = "Adadelta";
registerClass(AdadeltaOptimizer);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/adagrad_optimizer.js
/** @doclink Optimizer */
var AdagradOptimizer = class extends Optimizer {
	constructor(learningRate, initialAccumulatorValue = .1) {
		super();
		this.learningRate = learningRate;
		this.initialAccumulatorValue = initialAccumulatorValue;
		this.accumulatedGrads = [];
	}
	applyGradients(variableGradients) {
		(Array.isArray(variableGradients) ? variableGradients.map((item) => item.name) : Object.keys(variableGradients)).forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			if (this.accumulatedGrads[i] == null) {
				const trainable = false;
				this.accumulatedGrads[i] = {
					originalName: `${name}/accumulator`,
					variable: tidy(() => fill(value.shape, this.initialAccumulatorValue).variable(trainable))
				};
			}
			const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];
			if (gradient == null) return;
			const accumulatedGrad = this.accumulatedGrads[i].variable;
			tidy(() => {
				const newAccumulatedGrad = add(accumulatedGrad, square(gradient));
				accumulatedGrad.assign(newAccumulatedGrad);
				const newValue = add(mul(div(gradient, sqrt(add(newAccumulatedGrad, ENGINE.backend.epsilon()))), -this.learningRate), value);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}
	dispose() {
		if (this.accumulatedGrads != null) dispose(this.accumulatedGrads.map((v) => v.variable));
	}
	async getWeights() {
		return [await this.saveIterations()].concat(this.accumulatedGrads.map((v) => ({
			name: v.originalName,
			tensor: v.variable
		})));
	}
	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const trainable = false;
		this.accumulatedGrads = weightValues.map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
	}
	getConfig() {
		return {
			"learningRate": this.learningRate,
			"initialAccumulatorValue": this.initialAccumulatorValue
		};
	}
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config["learningRate"], config["initialAccumulatorValue"]);
	}
};
/** @nocollapse */
AdagradOptimizer.className = "Adagrad";
registerClass(AdagradOptimizer);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/adam_optimizer.js
var AdamOptimizer = class extends Optimizer {
	constructor(learningRate, beta1, beta2, epsilon = null) {
		super();
		this.learningRate = learningRate;
		this.beta1 = beta1;
		this.beta2 = beta2;
		this.epsilon = epsilon;
		this.accumulatedFirstMoment = [];
		this.accumulatedSecondMoment = [];
		tidy(() => {
			this.accBeta1 = scalar(beta1).variable();
			this.accBeta2 = scalar(beta2).variable();
		});
		if (epsilon == null) this.epsilon = ENGINE.backend.epsilon();
	}
	applyGradients(variableGradients) {
		const varNames = Array.isArray(variableGradients) ? variableGradients.map((v) => v.name) : Object.keys(variableGradients);
		tidy(() => {
			const oneMinusAccBeta1 = sub(1, this.accBeta1);
			const oneMinusAccBeta2 = sub(1, this.accBeta2);
			varNames.forEach((name, i) => {
				const value = ENGINE.registeredVariables[name];
				const trainable = false;
				if (this.accumulatedFirstMoment[i] == null) this.accumulatedFirstMoment[i] = {
					originalName: `${name}/m`,
					variable: tidy(() => zerosLike(value).variable(trainable))
				};
				if (this.accumulatedSecondMoment[i] == null) this.accumulatedSecondMoment[i] = {
					originalName: `${name}/v`,
					variable: tidy(() => zerosLike(value).variable(trainable))
				};
				const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];
				if (gradient == null) return;
				const firstMoment = this.accumulatedFirstMoment[i].variable;
				const secondMoment = this.accumulatedSecondMoment[i].variable;
				const newFirstMoment = add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));
				const newSecondMoment = add(mul(secondMoment, this.beta2), mul(square(gradient), 1 - this.beta2));
				const biasCorrectedFirstMoment = div(newFirstMoment, oneMinusAccBeta1);
				const biasCorrectedSecondMoment = div(newSecondMoment, oneMinusAccBeta2);
				firstMoment.assign(newFirstMoment);
				secondMoment.assign(newSecondMoment);
				const newValue = add(mul(div(biasCorrectedFirstMoment, add(sqrt(biasCorrectedSecondMoment), this.epsilon)), -this.learningRate), value);
				value.assign(newValue);
			});
			this.accBeta1.assign(mul(this.accBeta1, this.beta1));
			this.accBeta2.assign(mul(this.accBeta2, this.beta2));
		});
		this.incrementIterations();
	}
	dispose() {
		this.accBeta1.dispose();
		this.accBeta2.dispose();
		if (this.accumulatedFirstMoment != null) dispose(this.accumulatedFirstMoment.map((v) => v.variable));
		if (this.accumulatedSecondMoment != null) dispose(this.accumulatedSecondMoment.map((v) => v.variable));
	}
	async getWeights() {
		const variables = [...this.accumulatedFirstMoment, ...this.accumulatedSecondMoment];
		return [await this.saveIterations()].concat(variables.map((v) => ({
			name: v.originalName,
			tensor: v.variable
		})));
	}
	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		tidy(() => {
			this.accBeta1.assign(pow(this.beta1, this.iterations_ + 1));
			this.accBeta2.assign(pow(this.beta2, this.iterations_ + 1));
		});
		const variableCount = weightValues.length / 2;
		const trainable = false;
		this.accumulatedFirstMoment = weightValues.slice(0, variableCount).map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
		this.accumulatedSecondMoment = weightValues.slice(variableCount, variableCount * 2).map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
	}
	getConfig() {
		return {
			"learningRate": this.learningRate,
			"beta1": this.beta1,
			"beta2": this.beta2,
			"epsilon": this.epsilon
		};
	}
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config["learningRate"], config["beta1"], config["beta2"], config["epsilon"]);
	}
};
/** @nocollapse */
AdamOptimizer.className = "Adam";
registerClass(AdamOptimizer);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/adamax_optimizer.js
var AdamaxOptimizer = class extends Optimizer {
	constructor(learningRate, beta1, beta2, epsilon = null, decay = 0) {
		super();
		this.learningRate = learningRate;
		this.beta1 = beta1;
		this.beta2 = beta2;
		this.epsilon = epsilon;
		this.decay = decay;
		this.accumulatedFirstMoment = [];
		this.accumulatedWeightedInfNorm = [];
		tidy(() => {
			this.iteration = scalar(0).variable();
			this.accBeta1 = scalar(beta1).variable();
		});
		if (epsilon == null) this.epsilon = ENGINE.backend.epsilon();
	}
	applyGradients(variableGradients) {
		const variableNames = Array.isArray(variableGradients) ? variableGradients.map((item) => item.name) : Object.keys(variableGradients);
		tidy(() => {
			const oneMinusAccBeta1 = sub(1, this.accBeta1);
			const lr = div(-this.learningRate, add(mul(this.iteration, this.decay), 1));
			variableNames.forEach((name, i) => {
				const value = ENGINE.registeredVariables[name];
				const trainable = false;
				if (this.accumulatedFirstMoment[i] == null) this.accumulatedFirstMoment[i] = {
					originalName: `${name}/m`,
					variable: zerosLike(value).variable(trainable)
				};
				if (this.accumulatedWeightedInfNorm[i] == null) this.accumulatedWeightedInfNorm[i] = {
					originalName: `${name}/v`,
					variable: zerosLike(value).variable(trainable)
				};
				const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];
				if (gradient == null) return;
				const firstMoment = this.accumulatedFirstMoment[i].variable;
				const weightedInfNorm = this.accumulatedWeightedInfNorm[i].variable;
				const newFirstMoment = add(mul(firstMoment, this.beta1), mul(gradient, 1 - this.beta1));
				const ut0 = mul(weightedInfNorm, this.beta2);
				const ut1 = abs(gradient);
				const newWeightedInfNorm = maximum(ut0, ut1);
				firstMoment.assign(newFirstMoment);
				weightedInfNorm.assign(newWeightedInfNorm);
				const newValue = add(mul(div(lr, oneMinusAccBeta1), div(newFirstMoment, add(newWeightedInfNorm, this.epsilon))), value);
				value.assign(newValue);
			});
			this.iteration.assign(add(this.iteration, 1));
			this.accBeta1.assign(mul(this.accBeta1, this.beta1));
		});
		this.incrementIterations();
	}
	dispose() {
		this.accBeta1.dispose();
		this.iteration.dispose();
		if (this.accumulatedFirstMoment != null) dispose(this.accumulatedFirstMoment.map((v) => v.variable));
		if (this.accumulatedWeightedInfNorm != null) dispose(this.accumulatedWeightedInfNorm.map((v) => v.variable));
	}
	async getWeights() {
		throw new Error("getWeights() is not implemented for Adamax yet.");
	}
	async setWeights(weightValues) {
		throw new Error("setWeights() is not implemented for Adamax yet.");
	}
	getConfig() {
		return {
			"learningRate": this.learningRate,
			"beta1": this.beta1,
			"beta2": this.beta2,
			"epsilon": this.epsilon,
			"decay": this.decay
		};
	}
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config["learningRate"], config["beta1"], config["beta2"], config["epsilon"], config["decay"]);
	}
};
/** @nocollapse */
AdamaxOptimizer.className = "Adamax";
registerClass(AdamaxOptimizer);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/sgd_optimizer.js
/** @doclink Optimizer */
var SGDOptimizer = class extends Optimizer {
	constructor(learningRate) {
		super();
		this.learningRate = learningRate;
		this.setLearningRate(learningRate);
	}
	applyGradients(variableGradients) {
		(Array.isArray(variableGradients) ? variableGradients.map((v) => v.name) : Object.keys(variableGradients)).forEach((name, i) => {
			const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];
			if (gradient == null) return;
			const value = ENGINE.registeredVariables[name];
			tidy(() => {
				const newValue = add(mul(this.c, gradient), value);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}
	/**
	* Sets the learning rate of the optimizer.
	*/
	setLearningRate(learningRate) {
		this.learningRate = learningRate;
		if (this.c != null) this.c.dispose();
		this.c = keep(scalar(-learningRate));
	}
	dispose() {
		this.c.dispose();
	}
	async getWeights() {
		return [await this.saveIterations()];
	}
	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		if (weightValues.length !== 0) throw new Error("SGD optimizer does not have settable weights.");
	}
	getConfig() {
		return { "learningRate": this.learningRate };
	}
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config["learningRate"]);
	}
};
/** @nocollapse */
SGDOptimizer.className = "SGD";
registerClass(SGDOptimizer);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/momentum_optimizer.js
/** @doclink Optimizer */
var MomentumOptimizer = class extends SGDOptimizer {
	constructor(learningRate, momentum, useNesterov = false) {
		super(learningRate);
		this.learningRate = learningRate;
		this.momentum = momentum;
		this.useNesterov = useNesterov;
		this.accumulations = [];
		this.m = scalar(this.momentum);
	}
	applyGradients(variableGradients) {
		(Array.isArray(variableGradients) ? variableGradients.map((item) => item.name) : Object.keys(variableGradients)).forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			if (this.accumulations[i] == null) {
				const trainable = false;
				this.accumulations[i] = {
					originalName: `${name}/momentum`,
					variable: tidy(() => zerosLike(value).variable(trainable))
				};
			}
			const accumulation = this.accumulations[i].variable;
			const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];
			if (gradient == null) return;
			tidy(() => {
				let newValue;
				const newAccumulation = add(mul(this.m, accumulation), gradient);
				if (this.useNesterov) newValue = add(mul(this.c, add(gradient, mul(newAccumulation, this.m))), value);
				else newValue = add(mul(this.c, newAccumulation), value);
				accumulation.assign(newAccumulation);
				value.assign(newValue);
			});
		});
		this.incrementIterations();
	}
	dispose() {
		this.m.dispose();
		if (this.accumulations != null) dispose(this.accumulations.map((v) => v.variable));
	}
	/**
	* Sets the momentum of the optimizer.
	*
	* @param momentum
	*/
	setMomentum(momentum) {
		this.momentum = momentum;
	}
	async getWeights() {
		return [await this.saveIterations()].concat(this.accumulations.map((v) => ({
			name: v.originalName,
			tensor: v.variable
		})));
	}
	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const trainable = false;
		this.accumulations = weightValues.map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
	}
	getConfig() {
		return {
			"learningRate": this.learningRate,
			"momentum": this.momentum,
			"useNesterov": this.useNesterov
		};
	}
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config["learningRate"], config["momentum"], config["useNesterov"]);
	}
};
/** @nocollapse */
MomentumOptimizer.className = "Momentum";
registerClass(MomentumOptimizer);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/rmsprop_optimizer.js
/** @doclink Optimizer */
var RMSPropOptimizer = class extends Optimizer {
	constructor(learningRate, decay = .9, momentum = 0, epsilon = null, centered = false) {
		super();
		this.learningRate = learningRate;
		this.decay = decay;
		this.momentum = momentum;
		this.epsilon = epsilon;
		this.accumulatedMeanSquares = [];
		this.accumulatedMoments = [];
		this.accumulatedMeanGrads = [];
		this.centered = centered;
		if (epsilon == null) this.epsilon = ENGINE.backend.epsilon();
		if (learningRate == null) throw new Error(`learningRate for RMSPropOptimizer must be defined.`);
	}
	applyGradients(variableGradients) {
		(Array.isArray(variableGradients) ? variableGradients.map((item) => item.name) : Object.keys(variableGradients)).forEach((name, i) => {
			const value = ENGINE.registeredVariables[name];
			const trainable = false;
			if (this.accumulatedMeanSquares[i] == null) this.accumulatedMeanSquares[i] = {
				originalName: `${name}/rms`,
				variable: tidy(() => zerosLike(value).variable(trainable))
			};
			if (this.accumulatedMoments[i] == null) this.accumulatedMoments[i] = {
				originalName: `${name}/momentum`,
				variable: tidy(() => zerosLike(value).variable(trainable))
			};
			if (this.accumulatedMeanGrads[i] == null && this.centered) this.accumulatedMeanGrads[i] = {
				originalName: `${name}/mg`,
				variable: tidy(() => zerosLike(value).variable(trainable))
			};
			const gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];
			if (gradient == null) return;
			const accumulatedMeanSquare = this.accumulatedMeanSquares[i].variable;
			const accumulatedMoments = this.accumulatedMoments[i].variable;
			tidy(() => {
				const newAccumulatedMeanSquare = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));
				if (this.centered) {
					const accumulatedMeanGrad = this.accumulatedMeanGrads[i].variable;
					const newAccumulatedMeanGrad = add(mul(accumulatedMeanGrad, this.decay), mul(gradient, 1 - this.decay));
					const gradContribution = div(mul(gradient, this.learningRate), sqrt(sub(newAccumulatedMeanSquare, add(square(newAccumulatedMeanGrad), this.epsilon))));
					const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), gradContribution);
					accumulatedMeanSquare.assign(newAccumulatedMeanSquare);
					accumulatedMeanGrad.assign(newAccumulatedMeanGrad);
					accumulatedMoments.assign(newAccumulatedMoments);
					const newValue = sub(value, newAccumulatedMoments);
					value.assign(newValue);
				} else {
					const newAccumulatedMeanSquare$1 = add(mul(accumulatedMeanSquare, this.decay), mul(square(gradient), 1 - this.decay));
					const newAccumulatedMoments = add(mul(accumulatedMoments, this.momentum), div(mul(gradient, this.learningRate), sqrt(add(newAccumulatedMeanSquare$1, this.epsilon))));
					accumulatedMeanSquare.assign(newAccumulatedMeanSquare$1);
					accumulatedMoments.assign(newAccumulatedMoments);
					const newValue = sub(value, newAccumulatedMoments);
					value.assign(newValue);
				}
			});
		});
		this.incrementIterations();
	}
	dispose() {
		if (this.accumulatedMeanSquares != null) dispose(this.accumulatedMeanSquares.map((v) => v.variable));
		if (this.accumulatedMeanGrads != null && this.centered) dispose(this.accumulatedMeanGrads.map((v) => v.variable));
		if (this.accumulatedMoments != null) dispose(this.accumulatedMoments.map((v) => v.variable));
	}
	async getWeights() {
		const variables = [...this.accumulatedMeanSquares, ...this.accumulatedMoments];
		if (this.centered) variables.push(...this.accumulatedMeanGrads);
		return [await this.saveIterations()].concat(variables.map((v) => ({
			name: v.originalName,
			tensor: v.variable
		})));
	}
	async setWeights(weightValues) {
		weightValues = await this.extractIterations(weightValues);
		const variableCount = this.centered ? weightValues.length / 3 : weightValues.length / 2;
		const trainable = false;
		this.accumulatedMeanSquares = weightValues.slice(0, variableCount).map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
		this.accumulatedMoments = weightValues.slice(variableCount, variableCount * 2).map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
		if (this.centered) this.accumulatedMeanGrads = weightValues.slice(variableCount * 2, variableCount * 3).map((v) => ({
			originalName: v.name,
			variable: v.tensor.variable(trainable)
		}));
	}
	getConfig() {
		return {
			"learningRate": this.learningRate,
			"decay": this.decay,
			"momentum": this.momentum,
			"epsilon": this.epsilon,
			"centered": this.centered
		};
	}
	/** @nocollapse */
	static fromConfig(cls, config) {
		return new cls(config["learningRate"], config["decay"], config["momentum"], config["epsilon"], config["centered"]);
	}
};
/** @nocollapse */
RMSPropOptimizer.className = "RMSProp";
registerClass(RMSPropOptimizer);

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/optimizers/optimizer_constructors.js
var OptimizerConstructors = class {
	/**
	* Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.
	*
	* ```js
	* // Fit a quadratic function by learning the coefficients a, b, c.
	* const xs = tf.tensor1d([0, 1, 2, 3]);
	* const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);
	*
	* const a = tf.scalar(Math.random()).variable();
	* const b = tf.scalar(Math.random()).variable();
	* const c = tf.scalar(Math.random()).variable();
	*
	* // y = a * x^2 + b * x + c.
	* const f = x => a.mul(x.square()).add(b.mul(x)).add(c);
	* const loss = (pred, label) => pred.sub(label).square().mean();
	*
	* const learningRate = 0.01;
	* const optimizer = tf.train.sgd(learningRate);
	*
	* // Train the model.
	* for (let i = 0; i < 10; i++) {
	*   optimizer.minimize(() => loss(f(xs), ys));
	* }
	*
	* // Make predictions.
	* console.log(
	*     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);
	* const preds = f(xs).dataSync();
	* preds.forEach((pred, i) => {
	*   console.log(`x: ${i}, pred: ${pred}`);
	* });
	* ```
	*
	* @param learningRate The learning rate to use for the SGD algorithm.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	*/
	static sgd(learningRate) {
		return new SGDOptimizer(learningRate);
	}
	/**
	* Constructs a `tf.MomentumOptimizer` that uses momentum gradient
	* descent.
	*
	* See
	* [http://proceedings.mlr.press/v28/sutskever13.pdf](
	* http://proceedings.mlr.press/v28/sutskever13.pdf)
	*
	* @param learningRate The learning rate to use for the Momentum gradient
	* descent algorithm.
	* @param momentum The momentum to use for the momentum gradient descent
	* algorithm.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	*/
	static momentum(learningRate, momentum, useNesterov = false) {
		return new MomentumOptimizer(learningRate, momentum, useNesterov);
	}
	/**
	* Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient
	* descent. This implementation uses plain momentum and is not centered
	* version of RMSProp.
	*
	* See
	* [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](
	* http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
	*
	* @param learningRate The learning rate to use for the RMSProp gradient
	* descent algorithm.
	* @param decay The discounting factor for the history/coming gradient.
	* @param momentum The momentum to use for the RMSProp gradient descent
	* algorithm.
	* @param epsilon Small value to avoid zero denominator.
	* @param centered If true, gradients are normalized by the estimated
	* variance of the gradient.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	*/
	static rmsprop(learningRate, decay = .9, momentum = 0, epsilon = null, centered = false) {
		return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);
	}
	/**
	* Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.
	* See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
	*
	* @param learningRate The learning rate to use for the Adam gradient
	* descent algorithm.
	* @param beta1 The exponential decay rate for the 1st moment estimates.
	* @param beta2 The exponential decay rate for the 2nd moment estimates.
	* @param epsilon A small constant for numerical stability.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	*/
	static adam(learningRate = .001, beta1 = .9, beta2 = .999, epsilon = null) {
		return new AdamOptimizer(learningRate, beta1, beta2, epsilon);
	}
	/**
	* Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.
	* See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)
	*
	* @param learningRate The learning rate to use for the Adadelta gradient
	* descent algorithm.
	* @param rho The learning rate decay over each update.
	* @param epsilon A constant epsilon used to better condition the grad
	* update.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	*/
	static adadelta(learningRate = .001, rho = .95, epsilon = null) {
		return new AdadeltaOptimizer(learningRate, rho, epsilon);
	}
	/**
	* Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.
	* See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)
	*
	* @param learningRate The learning rate to use for the Adamax gradient
	* descent algorithm.
	* @param beta1 The exponential decay rate for the 1st moment estimates.
	* @param beta2 The exponential decay rate for the 2nd moment estimates.
	* @param epsilon A small constant for numerical stability.
	* @param decay The learning rate decay over each update.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	*/
	static adamax(learningRate = .002, beta1 = .9, beta2 = .999, epsilon = null, decay = 0) {
		return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);
	}
	/**
	* Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.
	* See
	* [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](
	* http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
	* or
	* [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](
	* http://ruder.io/optimizing-gradient-descent/index.html#adagrad)
	*
	* @param learningRate The learning rate to use for the Adagrad gradient
	* descent algorithm.
	* @param initialAccumulatorValue Starting value for the accumulators, must be
	* positive.
	*
	* @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}
	*/
	static adagrad(learningRate, initialAccumulatorValue = .1) {
		return new AdagradOptimizer(learningRate, initialAccumulatorValue);
	}
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/train.js
const train = {
	sgd: OptimizerConstructors.sgd,
	momentum: OptimizerConstructors.momentum,
	adadelta: OptimizerConstructors.adadelta,
	adagrad: OptimizerConstructors.adagrad,
	rmsprop: OptimizerConstructors.rmsprop,
	adamax: OptimizerConstructors.adamax,
	adam: OptimizerConstructors.adam
};

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/browser_util.js
/**
* @license
* Copyright 2017 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
var delayCallback = (() => {
	if (typeof requestAnimationFrame !== "undefined") return requestAnimationFrame;
	else if (typeof setImmediate !== "undefined") return setImmediate;
	return (f) => f();
})();
/**
* Returns a promise that resolve when a requestAnimationFrame has completed.
*
* On Node.js this uses setImmediate instead of requestAnimationFrame.
*
* This is simply a sugar method so that users can do the following:
* `await tf.nextFrame();`
*
* @doc {heading: 'Performance', subheading: 'Timing'}
*/
function nextFrame() {
	return new Promise((resolve) => delayCallback(() => resolve()));
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/concat_util.js
function assertParamsConsistent(shapes, axis) {
	const rank = shapes[0].length;
	shapes.forEach((shape, i) => {
		assert(shape.length === rank, () => `Error in concat${rank}D: rank of tensors[${i}] must be the same as the rank of the rest (${rank})`);
	});
	assert(axis >= 0 && axis < rank, () => `Error in concat${rank}D: axis must be between 0 and ${rank - 1}.`);
	const firstShape = shapes[0];
	shapes.forEach((shape, i) => {
		for (let r = 0; r < rank; r++) assert(r === axis || shape[r] === firstShape[r], () => `Error in concat${rank}D: Shape of tensors[${i}] (${shape}) does not match the shape of the rest (${firstShape}) along the non-concatenated axis ${i}.`);
	});
}
function computeOutShape$1(shapes, axis) {
	const outputShape = shapes[0].slice();
	for (let i = 1; i < shapes.length; i++) outputShape[axis] += shapes[i][axis];
	return outputShape;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/reduce_util.js
const PARALLELIZE_THRESHOLD = 30;
function computeOptimalWindowSize(inSize) {
	if (inSize <= PARALLELIZE_THRESHOLD) return inSize;
	return nearestDivisor(inSize, Math.floor(Math.sqrt(inSize)));
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/rotate_util.js
/**
* @license
* Copyright 2020 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
function getImageCenter(center, imageHeight, imageWidth) {
	const centerX = imageWidth * (typeof center === "number" ? center : center[0]);
	const centerY = imageHeight * (typeof center === "number" ? center : center[1]);
	return [centerX, centerY];
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/array_ops_util.js
/**
* @license
* Copyright 2018 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
* Gets the new shape of the input Tensor after it's been reshaped
* to:
* [blockShape[0], ..., blockShape[M-1], batch / prod(blockShape),
* inputShape[1], ..., inputShape[N-1]]
*
* See step 1: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
*/
function getReshaped(inputShape, blockShape, prod$1, batchToSpace = true) {
	let reshaped = [];
	if (batchToSpace) {
		reshaped = reshaped.concat(blockShape.slice(0));
		reshaped.push(inputShape[0] / prod$1);
		reshaped = reshaped.concat(inputShape.slice(1));
	} else {
		reshaped = reshaped.concat(inputShape[0]);
		const spatialLength = blockShape.length;
		for (let i = 0; i < spatialLength; ++i) reshaped = reshaped.concat([inputShape[i + 1] / blockShape[i], blockShape[i]]);
		reshaped = reshaped.concat(inputShape.slice(spatialLength + 1));
	}
	return reshaped;
}
/**
* Gets the permutation that will transpose the dimensions of the
* reshaped tensor to shape:
*
* [batch / prod(block_shape),inputShape[1], blockShape[0], ...,
* inputShape[M], blockShape[M-1],inputShape[M+1], ..., inputShape[N-1]]
*
* see step 2: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
*/
function getPermuted(reshapedRank, blockShapeRank, batchToSpace = true) {
	const permuted = [];
	if (batchToSpace) {
		permuted.push(blockShapeRank);
		for (let i = blockShapeRank + 1; i < reshapedRank; ++i) if (i <= 2 * blockShapeRank) {
			permuted.push(i);
			permuted.push(i - (blockShapeRank + 1));
		} else permuted.push(i);
	} else {
		const permutedBeforeBatch = [];
		const permutedAfterBatch = [];
		for (let i = 1; i < reshapedRank; ++i) if (i >= blockShapeRank * 2 + 1 || i % 2 === 1) permutedAfterBatch.push(i);
		else permutedBeforeBatch.push(i);
		permuted.push(...permutedBeforeBatch);
		permuted.push(0);
		permuted.push(...permutedAfterBatch);
	}
	return permuted;
}
/**
* Gets the shape of the reshaped and permuted input Tensor before any cropping
* is applied.  The new shape will be:
*
* [batch / prod(blockShape),inputShape[1] * blockShape[0], ...,
* inputShape[M] * blockShape[M-1],inputShape[M+1], ..., inputShape[N-1]]
*
* See step 3: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
*/
function getReshapedPermuted(inputShape, blockShape, prod$1, batchToSpace = true) {
	const reshapedPermuted = [];
	if (batchToSpace) reshapedPermuted.push(inputShape[0] / prod$1);
	else reshapedPermuted.push(inputShape[0] * prod$1);
	for (let i = 1; i < inputShape.length; ++i) if (i <= blockShape.length) if (batchToSpace) reshapedPermuted.push(blockShape[i - 1] * inputShape[i]);
	else reshapedPermuted.push(inputShape[i] / blockShape[i - 1]);
	else reshapedPermuted.push(inputShape[i]);
	return reshapedPermuted;
}
/**
* Converts the crops argument into the beginning coordinates of a slice
* operation.
*/
function getSliceBeginCoords(crops, blockShape) {
	const sliceBeginCoords = [0];
	for (let i = 0; i < blockShape; ++i) sliceBeginCoords.push(crops[i][0]);
	return sliceBeginCoords;
}
/**
* Converts the crops argument into the size of a slice operation.  When
* combined with getSliceBeginCoords this function allows the reshaped and
* permuted Tensor to be cropped to its final output shape of:
*
* inputShape[1] * blockShape[0] - crops[0,0] - crops[0,1], ...,
* inputShape[M] * blockShape[M-1] -crops[M-1,0] -
* crops[M-1,1],inputShape[M+1], ..., inputShape[N-1]]
*
* See step 4: https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
*/
function getSliceSize(uncroppedShape, crops, blockShape) {
	const sliceSize = uncroppedShape.slice(0, 1);
	for (let i = 0; i < blockShape; ++i) sliceSize.push(uncroppedShape[i + 1] - crops[i][0] - crops[i][1]);
	return sliceSize;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/selu_util.js
/**
* @license
* Copyright 2018 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
const SELU_SCALEALPHA = 1.7580993408473768;
const SELU_SCALE = 1.0507009873554805;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/erf_util.js
/**
* @license
* Copyright 2018 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
const ERF_P = .3275911;
const ERF_A1 = .254829592;
const ERF_A2 = -.284496736;
const ERF_A3 = 1.421413741;
const ERF_A4 = -1.453152027;
const ERF_A5 = 1.061405429;

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/backends/complex_util.js
/**
* @license
* Copyright 2018 Google LLC. All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*
* http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
* =============================================================================
*/
/**
* Merges real and imaginary Float32Arrays into a single complex Float32Array.
*
* The memory layout is interleaved as follows:
* real: [r0, r1, r2]
* imag: [i0, i1, i2]
* complex: [r0, i0, r1, i1, r2, i2]
*
* This is the inverse of splitRealAndImagArrays.
*
* @param real The real values of the complex tensor values.
* @param imag The imag values of the complex tensor values.
* @returns A complex tensor as a Float32Array with merged values.
*/
function mergeRealAndImagArrays(real$1, imag$1) {
	if (real$1.length !== imag$1.length) throw new Error(`Cannot merge real and imag arrays of different lengths. real:${real$1.length}, imag: ${imag$1.length}.`);
	const result = new Float32Array(real$1.length * 2);
	for (let i = 0; i < result.length; i += 2) {
		result[i] = real$1[i / 2];
		result[i + 1] = imag$1[i / 2];
	}
	return result;
}
/**
* Splits a complex Float32Array into real and imag parts.
*
* The memory layout is interleaved as follows:
* complex: [r0, i0, r1, i1, r2, i2]
* real: [r0, r1, r2]
* imag: [i0, i1, i2]
*
* This is the inverse of mergeRealAndImagArrays.
*
* @param complex The complex tensor values.
* @returns An object with real and imag Float32Array components of the complex
*     tensor.
*/
function splitRealAndImagArrays(complex$1) {
	const real$1 = new Float32Array(complex$1.length / 2);
	const imag$1 = new Float32Array(complex$1.length / 2);
	for (let i = 0; i < complex$1.length; i += 2) {
		real$1[i / 2] = complex$1[i];
		imag$1[i / 2] = complex$1[i + 1];
	}
	return {
		real: real$1,
		imag: imag$1
	};
}
/**
* Extracts even indexed complex values in the given array.
* @param complex The complex tensor values
*/
function complexWithEvenIndex(complex$1) {
	const len = Math.ceil(complex$1.length / 4);
	const real$1 = new Float32Array(len);
	const imag$1 = new Float32Array(len);
	for (let i = 0; i < complex$1.length; i += 4) {
		real$1[Math.floor(i / 4)] = complex$1[i];
		imag$1[Math.floor(i / 4)] = complex$1[i + 1];
	}
	return {
		real: real$1,
		imag: imag$1
	};
}
/**
* Extracts odd indexed comple values in the given array.
* @param complex The complex tensor values
*/
function complexWithOddIndex(complex$1) {
	const len = Math.floor(complex$1.length / 4);
	const real$1 = new Float32Array(len);
	const imag$1 = new Float32Array(len);
	for (let i = 2; i < complex$1.length; i += 4) {
		real$1[Math.floor(i / 4)] = complex$1[i];
		imag$1[Math.floor(i / 4)] = complex$1[i + 1];
	}
	return {
		real: real$1,
		imag: imag$1
	};
}
/**
* Get the map representing a complex value in the given array.
* @param complex The complex tensor values.
* @param index An index of the target complex value.
*/
function getComplexWithIndex(complex$1, index) {
	const real$1 = complex$1[index * 2];
	const imag$1 = complex$1[index * 2 + 1];
	return {
		real: real$1,
		imag: imag$1
	};
}
/**
* Insert a given complex value into the TypedArray.
* @param data The array in which the complex value is inserted.
* @param c The complex value to be inserted.
* @param index An index of the target complex value.
*/
function assignToTypedArray(data, real$1, imag$1, index) {
	data[index * 2] = real$1;
	data[index * 2 + 1] = imag$1;
}
/**
* Make the list of exponent terms used by FFT.
*/
function exponents(n, inverse) {
	const real$1 = new Float32Array(n / 2);
	const imag$1 = new Float32Array(n / 2);
	for (let i = 0; i < Math.ceil(n / 2); i++) {
		const x = (inverse ? 2 : -2) * Math.PI * (i / n);
		real$1[i] = Math.cos(x);
		imag$1[i] = Math.sin(x);
	}
	return {
		real: real$1,
		imag: imag$1
	};
}
/**
* Make the exponent term used by FFT.
*/
function exponent(k, n, inverse) {
	const x = (inverse ? 2 : -2) * Math.PI * (k / n);
	return {
		real: Math.cos(x),
		imag: Math.sin(x)
	};
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/backends/einsum_util.js
var ARROW = "->";
var ARROW_REGEX = /->/g;
var COMMA = ",";
var ELLIPSIS = "...";
/**
* Parse an equation for einsum.
*
* @param equation The einsum equation (e.g., "ij,jk->ik").
* @param numTensors Number of tensors provided along with `equation`. Used to
*   check matching number of input tensors.
* @returns An object consisting of the following fields:
*   - allDims: all dimension names as strings.
*   - summedDims: a list of all dimensions being summed over, as indices to
*     the elements of `allDims`.
*   - idDims: indices of the dimensions in each input tensor, as indices to
*     the elements of `allDims.
*/
function decodeEinsumEquation(equation, numTensors) {
	equation = equation.replace(/\s/g, "");
	const numArrows = (equation.length - equation.replace(ARROW_REGEX, "").length) / 2;
	if (numArrows < 1) throw new Error("Equations without an arrow are not supported.");
	else if (numArrows > 1) throw new Error(`Equation must contain exactly one arrow ("${ARROW}").`);
	const [inputString, outputString] = equation.split(ARROW);
	assert(inputString.indexOf(ELLIPSIS) === -1, () => `The ellipsis notation ("${ELLIPSIS}") is not supported yet.`);
	const inputTerms = inputString.split(COMMA);
	const numInputs = inputTerms.length;
	if (numTensors !== numInputs) throw new Error(`Expected ${numInputs} input tensors, received ${numTensors}`);
	if (numInputs > 2) throw new Error("Support for more than 2 input tensors is not implemented yet.");
	const allDims = [];
	for (let i = 0; i < outputString.length; ++i) {
		const dimName = outputString[i];
		if (!inputTerms.some((inputTerm) => inputTerm.indexOf(dimName) !== -1)) throw new Error(`Output subscripts contain the label ${dimName} not present in the input subscripts.`);
		if (allDims.indexOf(dimName) === -1) allDims.push(dimName);
	}
	for (let i = 0; i < inputString.length; ++i) {
		const dimName = inputString[i];
		if (allDims.indexOf(dimName) === -1 && dimName !== COMMA) allDims.push(dimName);
	}
	const idDims = new Array(inputTerms.length);
	for (let i = 0; i < numInputs; ++i) {
		if (new Set(inputTerms[i].split("")).size !== inputTerms[i].length) throw new Error(`Found duplicate axes in input component ${inputTerms[i]}. Support for duplicate axes in input is not implemented yet.`);
		idDims[i] = [];
		for (let j = 0; j < inputTerms[i].length; ++j) idDims[i].push(allDims.indexOf(inputTerms[i][j]));
	}
	const numDims = allDims.length;
	const numOutDims = outputString.length;
	const summedDims = [];
	for (let i = numOutDims; i < numDims; ++i) summedDims.push(i);
	return {
		allDims,
		summedDims,
		idDims
	};
}
/**
* Get the permutation for a given input tensor.
*
* @param nDims Total number of dimension of all tensors involved in the einsum
*   operation.
* @param idDims Dimension indices involve in the tensor in question.
* @returns An object consisting of the following fields:
*   - permutationIndices: Indices to permute the axes of the tensor with.
*   - expandDims: Indices to the dimension that need to be expanded from the
*     tensor after permutation.
*/
function getEinsumPermutation(nDims, idDims) {
	let permutationIndices = new Array(nDims);
	permutationIndices.fill(-1);
	for (let i = 0; i < idDims.length; ++i) permutationIndices[idDims[i]] = i;
	const expandDims$1 = [];
	for (let i = 0; i < nDims; ++i) if (permutationIndices[i] === -1) expandDims$1.push(i);
	permutationIndices = permutationIndices.filter((d) => d !== -1);
	return {
		permutationIndices,
		expandDims: expandDims$1
	};
}
/**
* Checks that the dimension sizes from different input tensors match the
* equation.
*/
function checkEinsumDimSizes(nDims, idDims, tensors) {
	const dimSizes = new Array(nDims);
	for (let i = 0; i < tensors.length; ++i) {
		const shape = tensors[i].shape;
		for (let j = 0; j < idDims[i].length; ++j) if (dimSizes[idDims[i][j]] === void 0) dimSizes[idDims[i][j]] = shape[j];
		else assert(dimSizes[idDims[i][j]] === shape[j], () => `Expected dimension ${dimSizes[idDims[i][j]]} at axis ${j} of input shaped ${JSON.stringify(shape)}, but got dimension ${shape[j]}`);
	}
}
/**
* Gets path of computation for einsum.
*
* @param summedDims indices to the dimensions being summed over.
* @param idDims A look up table for the dimensions present in each input
*     tensor. Each consituent array contains indices for the dimensions in the
*     corresponding input tensor.
*
* @return A map with two fields:
*   - path: The path of computation, with each element indicating the dimension
*     being summed over after the element-wise multiplication in that step.
*   - steps: With the same length as `path`. Each element contains the indices
*     to the input tensors being used for element-wise multiplication in the
*     corresponding step.
*/
function getEinsumComputePath(summedDims, idDims) {
	const path = summedDims;
	const steps = [];
	let nSteps = 0;
	if (summedDims.length === 0) path.push(-1);
	nSteps = summedDims.length + 1;
	for (let i = 0; i < nSteps; ++i) steps.push([]);
	const computedTermIndices = [];
	for (let i = 0; i < path.length; ++i) {
		const summedDim = path[i];
		const termIndices = findTermsWithDim(idDims, summedDim);
		for (const termIndex of termIndices) if (computedTermIndices.indexOf(termIndex) === -1) {
			steps[i].push(termIndex);
			computedTermIndices.push(termIndex);
		}
	}
	return {
		path,
		steps
	};
}
/** Determines if an axes permutation is the identity permutation. */
function isIdentityPermutation(perm) {
	return perm.every((dim, index) => dim === index);
}
function findTermsWithDim(idDims, dim) {
	const termIndices = [];
	for (let i = 0; i < idDims.length; ++i) if (idDims[i].length === 0 || idDims[i].indexOf(dim) !== -1 || dim === -1) termIndices.push(i);
	return termIndices;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/split_util.js
/**
* Prepare the split size array. When the input is a number, the axis is evenly
* divided among the split size. When the input contains the negative value, the
* rest of the axis is allocated toward that.
*/
function prepareSplitSize(x, numOrSizeSplits, axis = 0) {
	let splitSizes = [];
	if (typeof numOrSizeSplits === "number") {
		assert(x.shape[axis] % numOrSizeSplits === 0, () => "Number of splits must evenly divide the axis.");
		splitSizes = new Array(numOrSizeSplits).fill(x.shape[axis] / numOrSizeSplits);
	} else {
		const numOfNegs = numOrSizeSplits.reduce((count, value) => {
			if (value === -1) count += 1;
			return count;
		}, 0);
		assert(numOfNegs <= 1, () => "There should be only one negative value in split array.");
		const negIndex = numOrSizeSplits.indexOf(-1);
		if (negIndex !== -1) {
			const total = numOrSizeSplits.reduce((a, b) => b > 0 ? a + b : a);
			numOrSizeSplits[negIndex] = x.shape[axis] - total;
		}
		assert(x.shape[axis] === numOrSizeSplits.reduce((a, b) => a + b), () => "The sum of sizes must match the size of the axis dimension.");
		splitSizes = numOrSizeSplits;
	}
	return splitSizes;
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/ops/segment_util.js
var segment_util_exports = /* @__PURE__ */ __export({
	collectGatherOpShapeInfo: () => collectGatherOpShapeInfo,
	computeOutShape: () => computeOutShape,
	segOpComputeOptimalWindowSize: () => segOpComputeOptimalWindowSize
});
function segOpComputeOptimalWindowSize(inSize, numSegments) {
	let done = false;
	let res;
	if (inSize <= PARALLELIZE_THRESHOLD) {
		res = inSize;
		done = true;
	} else res = nearestDivisor(inSize, Math.floor(Math.sqrt(inSize)));
	while (!done) if (res > numSegments || res === inSize) done = true;
	else res = nearestDivisor(inSize, res + 1);
	return res;
}
function computeOutShape(aShape, axis, numSegments) {
	const outShape = [];
	const rank = aShape.length;
	for (let dim = 0; dim < rank; dim++) if (dim !== axis) outShape.push(aShape[dim]);
	else outShape.push(numSegments);
	return outShape;
}
function collectGatherOpShapeInfo(x, indices, axis, batchDims) {
	const indicesRank = indices.shape.length;
	const xRank = x.shape.length;
	if (batchDims !== 0) {
		if (batchDims < -indicesRank || batchDims > indicesRank) throw new Error(`Expect batchDims in the range of [-${indicesRank}, ${indicesRank}], but got ${batchDims}`);
	}
	if (batchDims < 0) batchDims += indicesRank;
	if (batchDims > xRank) throw new Error(`batchDims (${batchDims}) must be less than rank(x) (
    ${xRank}).`);
	if (axis < batchDims) throw new Error(`batchDims (${batchDims}) must be less than or equal to axis (${axis}).`);
	for (let i = 0; i < batchDims; ++i) if (x.shape[i] !== indices.shape[i]) throw new Error(`x.shape[${i}]: ${x.shape[i]} should be equal to indices.shape[${i}]: ${indices.shape[i]}.`);
	const dimSize = x.shape[axis];
	const outputShape = [];
	let batchSize = 1;
	let outerSize = 1;
	let sliceSize = 1;
	for (let i = 0; i < batchDims; ++i) {
		outputShape.push(x.shape[i]);
		batchSize *= x.shape[i];
	}
	for (let i = batchDims; i < axis; i++) {
		outputShape.push(x.shape[i]);
		outerSize *= x.shape[i];
	}
	for (let i = batchDims; i < indicesRank; i++) outputShape.push(indices.shape[i]);
	for (let i = axis + 1; i < xRank; i++) {
		outputShape.push(x.shape[i]);
		sliceSize *= x.shape[i];
	}
	return {
		batchSize,
		sliceSize,
		outerSize,
		dimSize,
		outputShape
	};
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/backends/backend_util.js
var backend_util_exports = /* @__PURE__ */ __export({
	ERF_A1: () => ERF_A1,
	ERF_A2: () => ERF_A2,
	ERF_A3: () => ERF_A3,
	ERF_A4: () => ERF_A4,
	ERF_A5: () => ERF_A5,
	ERF_P: () => ERF_P,
	PARALLELIZE_THRESHOLD: () => PARALLELIZE_THRESHOLD,
	SELU_SCALE: () => SELU_SCALE,
	SELU_SCALEALPHA: () => SELU_SCALEALPHA,
	applyActivation: () => applyActivation,
	assertAndGetBroadcastShape: () => assertAndGetBroadcastShape,
	assertAxesAreInnerMostDims: () => assertAxesAreInnerMostDims,
	assertParamsConsistent: () => assertParamsConsistent,
	assignToTypedArray: () => assignToTypedArray,
	axesAreInnerMostDims: () => axesAreInnerMostDims,
	calculateShapes: () => calculateShapes,
	checkEinsumDimSizes: () => checkEinsumDimSizes,
	combineLocations: () => combineLocations,
	complexWithEvenIndex: () => complexWithEvenIndex,
	complexWithOddIndex: () => complexWithOddIndex,
	computeConv2DInfo: () => computeConv2DInfo,
	computeConv3DInfo: () => computeConv3DInfo,
	computeDefaultPad: () => computeDefaultPad,
	computeDilation2DInfo: () => computeDilation2DInfo,
	computeOptimalWindowSize: () => computeOptimalWindowSize,
	computeOutAndReduceShapes: () => computeOutAndReduceShapes,
	computeOutShape: () => computeOutShape$1,
	computePool2DInfo: () => computePool2DInfo,
	computePool3DInfo: () => computePool3DInfo,
	convertConv2DDataFormat: () => convertConv2DDataFormat,
	decodeEinsumEquation: () => decodeEinsumEquation,
	eitherStridesOrDilationsAreOne: () => eitherStridesOrDilationsAreOne,
	expandShapeToKeepDim: () => expandShapeToKeepDim,
	exponent: () => exponent,
	exponents: () => exponents,
	fromStringArrayToUint8: () => fromStringArrayToUint8,
	fromUint8ToStringArray: () => fromUint8ToStringArray,
	getAxesPermutation: () => getAxesPermutation,
	getBroadcastDims: () => getBroadcastDims,
	getComplexWithIndex: () => getComplexWithIndex,
	getEinsumComputePath: () => getEinsumComputePath,
	getEinsumPermutation: () => getEinsumPermutation,
	getFusedBiasGradient: () => getFusedBiasGradient,
	getFusedDyActivation: () => getFusedDyActivation,
	getImageCenter: () => getImageCenter,
	getInnerMostAxes: () => getInnerMostAxes,
	getPermuted: () => getPermuted,
	getReductionAxes: () => getReductionAxes,
	getReshaped: () => getReshaped,
	getReshapedPermuted: () => getReshapedPermuted,
	getSliceBeginCoords: () => getSliceBeginCoords,
	getSliceSize: () => getSliceSize,
	getUndoAxesPermutation: () => getUndoAxesPermutation,
	isIdentityPermutation: () => isIdentityPermutation,
	log: () => log$1,
	mergeRealAndImagArrays: () => mergeRealAndImagArrays,
	prepareAndValidate: () => prepareAndValidate,
	prepareSplitSize: () => prepareSplitSize,
	segment_util: () => segment_util_exports,
	shouldFuse: () => shouldFuse,
	slice_util: () => slice_util_exports,
	splitRealAndImagArrays: () => splitRealAndImagArrays,
	tupleValuesAreOne: () => tupleValuesAreOne,
	upcastType: () => upcastType,
	validateInput: () => validateInput$1,
	validateUpdateShape: () => validateUpdateShape,
	warn: () => warn
});
function fromUint8ToStringArray(vals) {
	try {
		return vals.map((val) => decodeString(val));
	} catch (err) {
		throw new Error(`Failed to decode encoded string bytes into utf-8, error: ${err}`);
	}
}
function fromStringArrayToUint8(strings) {
	return strings.map((s) => encodeString(s));
}

//#endregion
//#region node_modules/@tensorflow/tfjs-core/dist/backends/kernel_impls.js
var kernel_impls_exports = /* @__PURE__ */ __export({
	nonMaxSuppressionV3Impl: () => nonMaxSuppressionV3Impl,
	nonMaxSuppressionV4Impl: () => nonMaxSuppressionV4Impl,
	nonMaxSuppressionV5Impl: () => nonMaxSuppressionV5Impl,
	whereImpl: () => whereImpl
});

//#endregion
export { Abs, Acos, Acosh, AdadeltaOptimizer, AdagradOptimizer, AdamOptimizer, AdamaxOptimizer, Add, AddN, All, Any, ArgMax, ArgMin, Asin, Asinh, Atan, Atan2, Atanh, AvgPool, AvgPool3D, AvgPool3DGrad, AvgPoolGrad, BatchMatMul, BatchToSpaceND, Bincount, BroadcastArgs, BroadcastTo, Cast, Ceil, ClipByValue, Complex, ComplexAbs, Concat, Conv2D, Conv2DBackpropFilter, Conv2DBackpropInput, Conv3D, Conv3DBackpropFilterV2, Conv3DBackpropInputV2, Cos, Cosh, CropAndResize, Cumsum, DataStorage, DenseBincount, DepthToSpace, DepthwiseConv2dNative, DepthwiseConv2dNativeBackpropFilter, DepthwiseConv2dNativeBackpropInput, Diag, Dilation2D, Dilation2DBackpropFilter, Dilation2DBackpropInput, ENV$1 as ENV, Einsum, Elu, EluGrad, Environment, Equal, Erf, Exp, ExpandDims, Expm1, FFT, Fill, FlipLeftRight, Floor, FloorDiv, FromPixels, FusedBatchNorm, FusedConv2D, FusedDepthwiseConv2D, GatherNd, GatherV2, Greater, GreaterEqual, IFFT, Identity, Imag, IsFinite, IsInf, IsNan, KernelBackend, LRN, LRNGrad, LeakyRelu, Less, LessEqual, LinSpace, Log, Log1p, LogSoftmax, LogicalAnd, LogicalNot, LogicalOr, Max, MaxPool, MaxPool3D, MaxPool3DGrad, MaxPoolGrad, MaxPoolWithArgmax, Maximum, Mean, Min, Minimum, MirrorPad, Mod, MomentumOptimizer, Multinomial, Multiply, Neg, NonMaxSuppressionV3, NonMaxSuppressionV4, NonMaxSuppressionV5, NotEqual, OP_SCOPE_SUFFIX, OneHot, OnesLike, Optimizer, OptimizerConstructors, Pack, PadV2, Pool, Pow, Prelu, Prod, RMSPropOptimizer, Range, Rank, Real, RealDiv, Reciprocal, Reduction, Relu, Relu6, Reshape, ResizeBilinear, ResizeBilinearGrad, ResizeNearestNeighbor, ResizeNearestNeighborGrad, Reverse, RotateWithOffset, Round, Rsqrt, SGDOptimizer, ScatterNd, Select, Selu, Sigmoid, Sign, Sin, Sinh, Slice, Softmax, Softplus, SpaceToBatchND, SparseFillEmptyRows, SparseReshape, SparseSegmentMean, SparseSegmentSum, SparseToDense, SplitV, Sqrt, Square, SquaredDifference, Step, StridedSlice, StringNGrams, StringSplit, StringToHashBucketFast, Sub, Sum, Tan, Tanh, Tensor, TensorBuffer, Tile, TopK, Transform, Transpose, Unique, Unpack, UnsortedSegmentSum, Variable, ZerosLike, _FusedMatMul, abs, acos, acosh, add, addN, all, any, argMax, argMin, asin, asinh, atan, atan2, atanh, avgPool, avgPool3d, backend, backend_util_exports, basicLSTMCell, batchNorm, batchNorm2d, batchNorm3d, batchNorm4d, batchToSpaceND, bincount, booleanMaskAsync, broadcastArgs, broadcastTo, browser_exports, buffer, cast, ceil, clipByValue, clone, complex, concat, concat1d, concat2d, concat3d, concat4d, conv1d, conv2d, conv2dTranspose, conv3d, conv3dTranspose, copyRegisteredKernels, cos, cosh, cosineWindow, cumsum, customGrad, denseBincount, deprecationWarn, depthToSpace, depthwiseConv2d, device_util_exports, diag, dilation2d, disableDeprecationWarnings, dispose, disposeVariables, div, divNoNan, dot, dropout, einsum, elu, enableDebugMode, enableProdMode, enclosingPowerOfTwo, engine, env, equal, erf, exp, expandDims, expm1, eye, fft, fill, findBackend, findBackendFactory, floor, floorDiv, fused_ops_exports, gather, gatherND, gather_nd_util_exports, getBackend, getGradient, getKernel, getKernelsForBackend, grad, grads, greater, greaterEqual, ifft, imag, image, inTopKAsync, io_exports, irfft, isFinite$1 as isFinite, isInf, isNaN$1 as isNaN, keep, kernel_impls_exports, leakyRelu, less, lessEqual, linalg, linspace, localResponseNormalization, log, log1p, logSigmoid, logSoftmax, logSumExp, logicalAnd, logicalNot, logicalOr, logicalXor, losses, matMul, math_exports, max, maxPool, maxPool3d, maxPoolWithArgmax, maximum, mean, memory, meshgrid, min, minimum, mirrorPad, mod, moments, movingAverage, mul, multiRNNCell, multinomial, neg, nextFrame, norm, notEqual, oneHot, ones, onesLike, op, outerProduct, pad, pad1d, pad2d, pad3d, pad4d, pool, pow, prelu, print, prod, profile, rand, randomGamma, randomNormal, randomUniform, range, ready, real, reciprocal, registerBackend, registerGradient, registerKernel, relu, relu6, removeBackend, reshape, reverse, reverse1d, reverse2d, reverse3d, reverse4d, rfft, round, rsqrt, scalar, scatterND, scatter_nd_util_exports, selu, separableConv2d, serialization_exports, setBackend, setPlatform, setdiff1dAsync, sigmoid, sign, signal, sin, sinh, slice, slice1d, slice2d, slice3d, slice4d, slice_util_exports, softmax, softplus, spaceToBatchND, sparse, sparseToDense, spectral, split, sqrt, square, squaredDifference, squeeze, stack, step, stridedSlice, string, sub, sum, sumOutType, tan, tanh, tensor, tensor1d, tensor2d, tensor3d, tensor4d, tensor5d, tensor6d, tensor_util_exports, test_util_exports, tidy, tile, time, topk, train, transpose, truncatedNormal, unique, unregisterGradient, unregisterKernel, unsortedSegmentSum, unstack, upcastType, util_exports, valueAndGrad, valueAndGrads, variable, variableGrads, version, where, whereAsync, zeros, zerosLike };
//# sourceMappingURL=dist-X0kg-ZjU.js.map